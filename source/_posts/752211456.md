---
layout: post
title: "Code Llamaï¼šLlama 2 å­¦ä¼šå†™ä»£ç äº†ï¼"
date: "2023-09-08T00:56:25.021Z"
---
Code Llamaï¼šLlama 2 å­¦ä¼šå†™ä»£ç äº†ï¼
==========================

å¼•è¨€
--

Code Llama æ˜¯ä¸ºä»£ç ç±»ä»»åŠ¡è€Œç”Ÿçš„ä¸€ç»„æœ€å…ˆè¿›çš„ã€å¼€æ”¾çš„ [Llama 2](https://huggingface.co/blog/zh/llama2) æ¨¡å‹ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´èƒ½å°†å…¶é›†æˆå…¥ Hugging Face ç”Ÿæ€ç³»ç»Ÿï¼Code Llama ä½¿ç”¨ä¸ Llama 2 ç›¸åŒçš„ç¤¾åŒºè®¸å¯è¯ï¼Œä¸”å¯å•†ç”¨ã€‚

ä»Šå¤©ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´èƒ½å‘å¸ƒ Hugging Face å¯¹ Code Llama çš„å…¨é¢æ”¯æŒ , åŒ…æ‹¬:

*   Hub ä¸Šçš„æ¨¡å‹æ”¯æŒï¼ŒåŒ…æ‹¬æ¨¡å‹å¡åŠè®¸å¯è¯
*   Transformers å·²é›†æˆ Code Llama
*   TGI å·²é›†æˆ Code Llamaï¼Œä»¥æ”¯æŒå¯¹å…¶è¿›è¡Œå¿«é€Ÿé«˜æ•ˆçš„äº§å“çº§æ¨ç†
*   æ¨ç†ç»ˆç«¯ (Inference Endpoints) å·²é›†æˆ Code Llama
*   å¯¹ Code Llama çš„ä»£ç åŸºå‡†æµ‹è¯•ç»“æœå·²å‘å¸ƒ

ä»£ç å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•å¯¹äºè½¯ä»¶å·¥ç¨‹å¸ˆæ¥è¯´æ— ç–‘æ˜¯æŒ¯å¥‹äººå¿ƒçš„ï¼Œå› ä¸ºè¿™æ„å‘³ç€ä»–ä»¬å¯ä»¥é€šè¿‡ IDE ä¸­çš„ä»£ç è¡¥å…¨åŠŸèƒ½æ¥æé«˜ç”Ÿäº§åŠ›ï¼Œå¹¶åˆ©ç”¨å…¶æ¥å¤„ç†é‡å¤æˆ–çƒ¦äººçš„ä»»åŠ¡ï¼Œä¾‹å¦‚ä¸ºä»£ç ç¼–å†™æ–‡æ¡£å­—ç¬¦ä¸²æˆ–åˆ›å»ºå•å…ƒæµ‹è¯•ã€‚

ç›®å½•
--

*   [å¼•è¨€](#%E5%BC%95%E8%A8%80)
*   [ç›®å½•](#%E7%9B%AE%E5%BD%95)
*   [Code Llama ç®€ä»‹](#code-llama-%E7%AE%80%E4%BB%8B)
*   [å¦‚ä½•ä½¿ç”¨ Code Llama?](#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-code-llama)
    *   [æ¼”ç¤º](#%E6%BC%94%E7%A4%BA)
    *   [Transformers](#transformers)
        *   [ä»£ç è¡¥å…¨](#%E4%BB%A3%E7%A0%81%E8%A1%A5%E5%85%A8)
        *   [ä»£ç å¡«å……](#%E4%BB%A3%E7%A0%81%E5%A1%AB%E5%85%85)
        *   [å¯¹è¯å¼æŒ‡ä»¤](#%E5%AF%B9%E8%AF%9D%E5%BC%8F%E6%8C%87%E4%BB%A4)
        *   [4 æ¯”ç‰¹åŠ è½½](#4-%E6%AF%94%E7%89%B9%E5%8A%A0%E8%BD%BD)
    *   [ä½¿ç”¨ TGI å’Œæ¨ç†ç»ˆç«¯](#%E4%BD%BF%E7%94%A8-tgi-%E5%92%8C%E6%8E%A8%E7%90%86%E7%BB%88%E7%AB%AF)
*   [è¯„ä¼°](#%E8%AF%84%E4%BC%B0)
*   [å…¶ä»–èµ„æº](#%E5%85%B6%E4%BB%96%E8%B5%84%E6%BA%90)

Code Llama ç®€ä»‹
-------------

Code Llama åŒ…å« 3 ä¸ªä¸åŒå‚æ•°é‡çš„ç‰ˆæœ¬ï¼Œåˆ†åˆ«ä¸º: 7 äº¿å‚æ•°ç‰ˆã€13 äº¿å‚æ•°ç‰ˆ ä»¥åŠ 340 äº¿å‚æ•°ç‰ˆã€‚åœ¨è®­ç»ƒåŸºç¡€æ¨¡å‹æ—¶ï¼Œå…ˆç”¨åŒç­‰å‚æ•°é‡çš„ Llama 2 æ¨¡å‹åˆå§‹åŒ–æƒé‡ï¼Œç„¶ååœ¨ 5000 äº¿è¯å…ƒçš„ä»£ç æ•°æ®é›†ä¸Šè®­ç»ƒã€‚ Meta è¿˜å¯¹è®­å¾—çš„åŸºç¡€æ¨¡å‹è¿›è¡Œäº†ä¸¤ç§ä¸åŒé£æ ¼çš„å¾®è°ƒï¼Œåˆ†åˆ«ä¸º: Python ä¸“å®¶ç‰ˆ (å†åŠ  1000 äº¿ä¸ªé¢å¤–è¯å…ƒ) ; ä»¥åŠæŒ‡ä»¤å¾®è°ƒç‰ˆï¼Œå…¶å¯ä»¥ç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚

è¿™äº›æ¨¡å‹åœ¨ Pythonã€C++ã€Javaã€PHPã€C#ã€TypeScript å’Œ Bash ä¸­éƒ½å±•ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚7B å’Œ 13B åŸºç¡€ç‰ˆå’ŒæŒ‡ä»¤ç‰ˆæ”¯æŒå®Œå½¢å¡«ç©ºï¼Œå› æ­¤éå¸¸é€‚åˆç”¨ä½œä»£ç åŠ©æ‰‹ã€‚

Code Llama åŸºäº 16k ä¸Šä¸‹æ–‡çª—å£è®­ç»ƒã€‚æ­¤å¤–ï¼Œè¿™ä¸‰ä¸ªå°ºå¯¸çš„æ¨¡å‹è¿˜è¿›è¡Œäº†é¢å¤–çš„é•¿ä¸Šä¸‹æ–‡å¾®è°ƒï¼Œä½¿å…¶ä¸Šä¸‹æ–‡çª—å£æœ€å¤šå¯æ‰©å±•è‡³ 10 ä¸‡è¯å…ƒã€‚

å—ç›Šäº RoPE æ‰©å±•æ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œå°† Llama 2 çš„ 4k ä¸Šä¸‹æ–‡çª—å£å¢åŠ åˆ° Code Llama çš„ 16k (ç”šè‡³å¯ä»¥å¤–æ’è‡³ 100k) æˆä¸ºå¯èƒ½ã€‚ç¤¾åŒºå‘ç°å¯ä»¥å¯¹ Llama çš„ä½ç½®åµŒå…¥è¿›è¡Œçº¿æ€§æ’å€¼æˆ–é¢‘åŸŸæ’å€¼ï¼Œè¿™ä½¿å¾—é€šè¿‡å¾®è°ƒè®©åŸºç¡€æ¨¡å‹è½»æ¾æ‰©å±•åˆ°æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£æˆä¸ºå¯èƒ½ã€‚åœ¨ Code Llama ä¸­ï¼Œä»–ä»¬æŠŠé¢‘åŸŸç¼©æ”¾å’Œæ¾å¼›æŠ€æœ¯äºŒè€…ç»“åˆèµ·æ¥: å¾®è°ƒé•¿åº¦æ˜¯ç¼©æ”¾åçš„é¢„è®­ç»ƒé•¿åº¦çš„ä¸€å°éƒ¨åˆ†ã€‚è¿™ä¸ªåšæ³•èµ‹äºˆäº†æ¨¡å‹å¼ºå¤§çš„å¤–æ¨èƒ½åŠ›ã€‚

![è®­ç»ƒè¿‡ç¨‹](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202309071727904.jpeg)

ç¬¬ä¸€æ­¥æ˜¯åœ¨ 5000 äº¿è¯å…ƒçš„å…¬å¼€ä»£ç æ•°æ®é›†ä¸Šè®­ç»ƒå‡ºä¸€ä¸ªæ¨¡å‹ã€‚è¯¥æ•°æ®é›†ä¸­é™¤äº†æœ‰ä»£ç æ•°æ®é›†å¤–ï¼Œè¿˜åŒ…å«ä¸€äº›è‡ªç„¶è¯­è¨€æ•°æ®é›†ï¼Œä¾‹å¦‚æœ‰å…³ä»£ç å’Œä»£ç ç‰‡æ®µçš„è®¨è®ºï¼Œä¸”æœ€ç»ˆæ•°æ®é›†æ˜¯ä½¿ç”¨è¿‘ä¼¼å»é‡æ³•å»è¿‡é‡çš„ã€‚ä¸å¹¸çš„æ˜¯ï¼ŒMeta æ²¡æœ‰æŠ«éœ²æœ‰å…³è¯¥æ•°æ®é›†çš„æ›´å¤šä¿¡æ¯ã€‚

åœ¨å¯¹æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒæ—¶ï¼Œä½¿ç”¨äº†ä¸¤ä¸ªæ•°æ®é›†: ä¸º Llama 2 Chat æ”¶é›†çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†å’Œè‡ªæŒ‡ä»¤æ•°æ®é›†ã€‚è‡ªæŒ‡ä»¤æ•°æ®é›†æ”¶é›†äº† Llama 2 ç¼–åˆ¶å‡ºçš„ç¼–ç¨‹é¢è¯•é—®é¢˜ï¼Œç„¶åä½¿ç”¨ Code Llama ç”Ÿæˆå•å…ƒæµ‹è¯•å’Œè§£ç­”ï¼Œæœ€åé€šè¿‡æ‰§è¡Œæµ‹è¯•æ¥è¯„ä¼°è§£ç­”ã€‚

å¦‚ä½•ä½¿ç”¨ Code Llama?
----------------

`Transformers` ä» 4.33 ç‰ˆå¼€å§‹æ”¯æŒ Code Llamaã€‚åœ¨æ­¤ä¹‹å‰ï¼Œéœ€è¦ä»ä¸»åˆ†æ”¯è¿›è¡Œæºä»£ç å®‰è£…æ‰è¡Œã€‚

### æ¼”ç¤º

æˆ‘ä»¬å‡†å¤‡äº† **[è¿™ä¸ª Space](https://huggingface.co/spaces/codellama/codellama-playground)** æˆ–ä¸‹é¢çš„ Playground ä»¥ä¾›å¤§å®¶å°è¯• Code Llama æ¨¡å‹ (130 äº¿å‚æ•°ï¼):

![](https://devrel.andfun.cn/devrel/posts/2023/09/07/2Cz3eu.png)

è¿™ä¸ªæ¼”ç¤ºèƒŒåä½¿ç”¨äº† Hugging Face [TGI](https://github.com/huggingface/text-generation-inference)ï¼Œ[HuggingChat](https://huggingface.co/chat) ä¹Ÿç”¨äº†ç›¸åŒçš„æŠ€æœ¯ï¼Œå…·ä½“å†…å®¹è§ä¸‹æ–‡ã€‚

ä½ è¿˜å¯ä»¥ç©ç© [è¿™ä¸ªèŠå¤©æœºå™¨äºº](https://huggingface.co/spaces/codellama/codellama-13b-chat)ï¼Œæˆ–è€…å¤åˆ¶ä¸€ä»½åˆ°è‡ªå·±çš„è´¦å·ä¸‹ä»¥ä¾›ä½ ä½¿ç”¨ â€“ å®ƒæ˜¯è‡ªå«çš„ï¼Œå› æ­¤ä½ å¯ä»¥éšå¿ƒæ‰€æ¬²åœ°ä¿®æ”¹ä»£ç ï¼

### Transformers

ä»æœ€æ–°å‘å¸ƒçš„ `transformers` 4.33 å¼€å§‹ï¼Œä½ å¯ä»¥åœ¨ Code Llama ä¸Šåº”ç”¨ HF ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ‰€æœ‰å·¥å…·ï¼Œä¾‹å¦‚:

*   è®­ç»ƒå’Œæ¨ç†è„šæœ¬å’Œç¤ºä¾‹
*   å®‰å…¨çš„æ–‡ä»¶æ ¼å¼ (`safetensors` )
*   ä¸ `bitsandbytes` (4 æ¯”ç‰¹é‡åŒ–) å’Œ PEFT ç­‰å·¥å…·ç»“åˆä½¿ç”¨
*   è¿è¡Œæ¨¡å‹ç”Ÿæˆæ‰€éœ€çš„å·¥å…·åŠè¾…åŠ©ä»£ç 
*   å¯¼å‡ºæ¨¡å‹ä»¥è¿›è¡Œéƒ¨ç½²çš„æœºåˆ¶

åœ¨ `transformers` 4.33 å‘å¸ƒä¹‹å‰ï¼Œç”¨æˆ·éœ€è¦ä»ä¸»åˆ†æ”¯æºç å®‰è£… `transformers` ã€‚

    !pip install git+https://github.com/huggingface/transformers.git@main accelerate
    

#### ä»£ç è¡¥å…¨

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ 7B å’Œ 13B æ¨¡å‹è¿›è¡Œæ–‡æœ¬/ä»£ç è¡¥å…¨æˆ–å¡«å……ã€‚ä¸‹è¿°ä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `pipeline` æ¥å£æ¥è¿›è¡Œæ–‡æœ¬è¡¥å…¨ã€‚è¿è¡Œæ—¶ï¼Œåªéœ€é€‰æ‹© GPU å³å¯åœ¨ Colab çš„å…è´¹ GPU ä¸Šè¿è¡Œã€‚

    from transformers import AutoTokenizer
    import transformers
    import torch
    
    tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-hf")
    pipeline = transformers.pipeline(
        "text-generation",
        model="codellama/CodeLlama-7b-hf",
        torch_dtype=torch.float16,
        device_map="auto",
    )
    
    sequences = pipeline(
        'def fibonacci(',
        do_sample=True,
        temperature=0.2,
        top_p=0.9,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        max_length=100,
    )
    for seq in sequences:
        print(f"Result: {seq['generated_text']}")
    

å…¶è¾“å‡ºå¦‚ä¸‹:

    Result: def fibonacci(n):
        if n == 0:
            return 0
        elif n == 1:
            return 1
        else:
            return fibonacci(n-1) + fibonacci(n-2)
    
    def fibonacci_memo(n, memo={}):
        if n == 0:
            return 0
        elif n == 1:
            return
    

Code Llama è™½ç„¶ä¸“ç²¾äºä»£ç ç†è§£ï¼Œä½†å…¶ä»æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ã€‚ä½ ä»ç„¶å¯ä»¥ä½¿ç”¨ç›¸åŒçš„ç”Ÿæˆç­–ç•¥æ¥è‡ªåŠ¨å®Œæˆæ³¨é‡Šæˆ–è‡ªç„¶è¯­è¨€æ–‡æœ¬ã€‚

#### ä»£ç å¡«å……

è¿™æ˜¯ä»£ç æ¨¡å‹æ‰èƒ½å®Œæˆçš„ä¸“é—¨ä»»åŠ¡ã€‚è¯¥æ¨¡å‹ç»è¿‡è®­ç»ƒåï¼Œå¯ä»¥ç”Ÿæˆä¸ç»™å®šä¸Šä¸‹æ–‡æœ€åŒ¹é…çš„ä»£ç  (åŒ…æ‹¬æ³¨é‡Š)ã€‚è¿™æ˜¯ä»£ç åŠ©ç†çš„å…¸å‹ä½¿ç”¨åœºæ™¯: è¦æ±‚å®ƒä»¬æ ¹æ®ä¸Šä¸‹æ–‡å¡«å……å½“å‰å…‰æ ‡å¤„çš„ä»£ç ã€‚

æ­¤ä»»åŠ¡éœ€è¦ä½¿ç”¨ 7B å’Œ 13B çš„ **åŸºç¡€** æˆ– **æŒ‡ä»¤** æ¨¡å‹ã€‚ä»»ä½• 34B æˆ– Python ç‰ˆæ¨¡å‹ä¸èƒ½ç”¨äºæ­¤ä»»åŠ¡ã€‚

å¡«å……ç±»ä»»åŠ¡éœ€è¦åœ¨ç”Ÿæˆæ—¶ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒæ ¼å¼çš„è¾“å…¥æ–‡æœ¬ï¼Œå› ä¸ºè®­ç»ƒæ—¶ä¼šä½¿ç”¨ç‰¹æ®Šçš„åˆ†éš”ç¬¦æ¥åŒºåˆ†æç¤ºçš„ä¸åŒéƒ¨åˆ†ã€‚å¹¸è¿çš„æ˜¯ï¼Œ `transformers` çš„ `CodeLlamaTokenizer` å·²ç»å¸®ä½ æŠŠè¿™äº‹åšäº†ï¼Œå¦‚ä¸‹æ‰€ç¤º:

    from transformers import AutoTokenizer, AutoModelForCausalLM
    import transformers
    import torch
    
    model_id = "codellama/CodeLlama-7b-hf"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16
    ).to("cuda")
    
    prompt = '''def remove_non_ascii(s: str) -> str:
        """ <FILL_ME>
        return result
    '''
    
    input_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to("cuda")
    output = model.generate(
        input_ids,
        max_new_tokens=200,
    )
    output = output[0].to("cpu")
    
    filling = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)
    print(prompt.replace("<FILL_ME>", filling))
    

è¾“å‡ºå¦‚ä¸‹:

    def remove_non_ascii(s: str) -> str:
        """ Remove non-ASCII characters from a string.
    
        Args:
            s: The string to remove non-ASCII characters from.
    
        Returns:
            The string with non-ASCII characters removed.
        """
        result = ""
        for c in s:
            if ord(c) < 128:
                result += c
        return result
    

åœ¨åº•å±‚ï¼Œåˆ†è¯å™¨ä¼š [è‡ªåŠ¨æŒ‰ `<fill_me>` åˆ†å‰²](https://huggingface.co/docs/transformers/main/model_doc/code_llama#transformers.CodeLlamaTokenizer.fill_token) å¹¶ç”Ÿæˆä¸€ä¸ªæ ¼å¼åŒ–çš„è¾“å…¥å­—ç¬¦ä¸²ï¼Œå…¶æ ¼å¼ä¸ [è®­ç»ƒæ—¶çš„æ ¼å¼](https://github.com/facebookresearch/codellama/blob/cb51c14ec761370ba2e2bc351374a79265d0465e/llama/generation.py#L402) ç›¸åŒã€‚è¿™æ ·åšæ—¢é¿å…äº†ç”¨æˆ·è‡ªå·±æ ¼å¼åŒ–çš„å¾ˆå¤šéº»çƒ¦ï¼Œä¹Ÿé¿å…äº†ä¸€äº›å¾ˆéš¾è°ƒè¯•çš„é™·é˜±ï¼Œä¾‹å¦‚è¯å…ƒç²˜åˆ (token glueing)ã€‚

#### å¯¹è¯å¼æŒ‡ä»¤

å¦‚ä¸Šæ‰€è¿°ï¼ŒåŸºç¡€æ¨¡å‹å¯ç”¨äºè¡¥å…¨å’Œå¡«å……ã€‚Code Llama è¿˜åŒ…å«ä¸€ä¸ªé€‚ç”¨äºå¯¹è¯åœºæ™¯çš„æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ã€‚

ä¸ºæ­¤ç±»ä»»åŠ¡å‡†å¤‡è¾“å…¥æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæç¤ºæ¨¡æ¿ã€‚ä¸€ä¸ªä¾‹å­æ˜¯æˆ‘ä»¬åœ¨ [Llama 2 åšæ–‡](https://huggingface.co/blog/zh/llama2#%E5%A6%82%E4%BD%95%E6%8F%90%E7%A4%BA-Llama-2) ä¸­æè¿°çš„æ¨¡æ¿ï¼Œå¦‚ä¸‹:

    <s>[INST] <<SYS>>
    {{ system_prompt }}
    <</SYS>>
    
    {{ user_msg_1 }} [/INST]{{ model_answer_1 }} </s><s>[INST]{{ user_msg_2 }} [/INST]
    

è¯·æ³¨æ„ï¼Œç³»ç»Ÿæç¤º ( `system prompt` ) æ˜¯å¯é€‰çš„ - æ²¡æœ‰å®ƒæ¨¡å‹ä¹Ÿèƒ½å·¥ä½œï¼Œä½†ä½ å¯ä»¥ç”¨å®ƒæ¥è¿›ä¸€æ­¥æŒ‡å®šæ¨¡å‹çš„è¡Œä¸ºæˆ–é£æ ¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ å¸Œæœ›è·å¾— JavaScript çš„ç­”æ¡ˆï¼Œå³å¯åœ¨æ­¤å£°æ˜ã€‚åœ¨ç³»ç»Ÿæç¤ºä¹‹åï¼Œä½ éœ€è¦æä¾›å¯¹è¯äº¤äº’å†å²: ç”¨æˆ·é—®äº†ä»€ä¹ˆä»¥åŠæ¨¡å‹å›ç­”äº†ä»€ä¹ˆã€‚ä¸å¡«å……åœºæ™¯ä¸€æ ·ï¼Œä½ éœ€è¦æ³¨æ„åˆ†éš”ç¬¦çš„ä½¿ç”¨ã€‚è¾“å…¥çš„æœ€åå¿…é¡»æ˜¯æ–°çš„ç”¨æˆ·æŒ‡ä»¤ï¼Œè¿™å¯¹æ¨¡å‹è€Œè¨€æ˜¯è®©å…¶æä¾›ç­”æ¡ˆçš„ä¿¡å·ã€‚

ä»¥ä¸‹ä»£ç ç‰‡æ®µæ¼”ç¤ºäº†å¦‚ä½•åœ¨å®é™…å·¥ä½œä¸­ä½¿ç”¨è¯¥æ¨¡æ¿ã€‚

1.  **é¦–æ¬¡ç”¨æˆ·è¾“å…¥ï¼Œæ— ç³»ç»Ÿæç¤º**

    user = 'In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month?'
    
    prompt = f"<s>[INST]{user.strip()} [/INST]"
    inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=False).to("cuda")
    

2.  **é¦–æ¬¡ç”¨æˆ·æŸ¥è¯¢ï¼Œæœ‰ç³»ç»Ÿæç¤º**

    system = "Provide answers in JavaScript"
    user = "Write a function that computes the set of sums of all contiguous sublists of a given list."
    
    prompt = f"<s><<SYS>>\\n{system}\\n<</SYS>>\\n\\n{user}"
    inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=False).to("cuda")
    

3.  **å«å¯¹è¯å†å²çš„å¤šè½®å¯¹è¯**

è¯¥è¿‡ç¨‹ä¸ [Llama 2](https://huggingface.co/blog/zh/llama2#%E5%A6%82%E4%BD%95%E6%8F%90%E7%A4%BA-Llama-2) ä¸­çš„è¿‡ç¨‹ç›¸åŒã€‚ä¸ºäº†æœ€æ¸…æ¥šèµ·è§ï¼Œæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨å¾ªç¯æˆ–æ³›åŒ–æ­¤ç¤ºä¾‹ä»£ç :

    system = "System prompt"
    user_1 = "user_prompt_1"
    answer_1 = "answer_1"
    user_2 = "user_prompt_2"
    answer_2 = "answer_2"
    user_3 = "user_prompt_3"
    
    prompt = f"<<SYS>>\\n{system}\\n<</SYS>>\\n\\n{user_1}"
    prompt = f"<s>[INST]{prompt.strip()} [/INST]{answer_1.strip()} </s>"
    prompt += f"<s>[INST]{user_2.strip()} [/INST]{answer_2.strip()} </s>"
    prompt += f"<s>[INST]{user_3.strip()} [/INST]"
    
    inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=False).to("cuda")
    

#### 4 æ¯”ç‰¹åŠ è½½

å°† Code Llama é›†æˆåˆ° Transformers ä¸­æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ç«‹å³è·å¾— 4 æ¯”ç‰¹åŠ è½½ç­‰é«˜çº§åŠŸèƒ½çš„æ”¯æŒã€‚è¿™ä½¿å¾—ç”¨æˆ·å¯ä»¥åœ¨è‹±ä¼Ÿè¾¾ 3090 å¡ç­‰æ¶ˆè´¹ç±» GPU ä¸Šè¿è¡Œå¤§å‹çš„ 32B å‚æ•°é‡æ¨¡å‹ï¼

ä»¥ä¸‹æ˜¯åœ¨ 4 æ¯”ç‰¹æ¨¡å¼ä¸‹è¿è¡Œæ¨ç†çš„æ–¹æ³•:

    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
    import torch
    
    model_id = "codellama/CodeLlama-34b-hf"
    quantization_config = BitsAndBytesConfig(
       load_in_4bit=True,
       bnb_4bit_compute_dtype=torch.float16
    )
    
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=quantization_config,
        device_map="auto",
    )
    
    prompt = 'def remove_non_ascii(s: str) -> str:\n """ '
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    output = model.generate(
        inputs["input_ids"],
        max_new_tokens=200,
        do_sample=True,
        top_p=0.9,
        temperature=0.1,
    )
    output = output[0].to("cpu")
    print(tokenizer.decode(output))
    

### ä½¿ç”¨ TGI å’Œæ¨ç†ç»ˆç«¯

[TGI](https://github.com/huggingface/text-generation-inference) æ˜¯ Hugging Face å¼€å‘çš„ç”Ÿäº§çº§æ¨ç†å®¹å™¨ï¼Œå¯ç”¨äºè½»æ¾éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹ã€‚å®ƒåŒ…å«è¿ç»­æ‰¹å¤„ç†ã€æµå¼è¾“å‡ºã€åŸºäºå¼ é‡å¹¶è¡Œçš„å¤š GPU å¿«é€Ÿæ¨ç†ä»¥åŠç”Ÿäº§çº§çš„æ—¥å¿—è®°å½•å’Œè·Ÿè¸ªç­‰åŠŸèƒ½ã€‚

ä½ å¯ä»¥åœ¨è‡ªå·±çš„åŸºç¡€è®¾æ–½ä¸Šä½¿ç”¨ TGIï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ Hugging Face çš„ [æ¨ç†ç»ˆç«¯](https://huggingface.co/inference-endpoints)ã€‚è¦éƒ¨ç½² Codellama 2 æ¨¡å‹ï¼Œè¯·ç™»é™†å…¶ [æ¨¡å‹é¡µé¢](https://huggingface.co/codellama)ï¼Œç„¶åå•å‡» [Deploy -> Inference Endpoints](https://huggingface.co/codellama/CodeLlama-7b-hf) æŒ‰é’®ã€‚

*   æ¨ç† 7B æ¨¡å‹ï¼Œæˆ‘ä»¬å»ºè®®é€‰æ‹©â€œGPU \[medium\] - 1x Nvidia A10Gâ€ã€‚
*   æ¨ç† 13B æ¨¡å‹ï¼Œæˆ‘ä»¬å»ºè®®é€‰æ‹©â€œGPU \[xlarge\] - 1x Nvidia A100â€ã€‚
*   æ¨ç† 34B æ¨¡å‹ï¼Œæˆ‘ä»¬å»ºè®®å¯ç”¨ `bitsandbytes` é‡åŒ–å¹¶é€‰æ‹©â€œGPU \[1xlarge\] - 1x Nvidia A100â€æˆ–â€œGPU \[2xlarge\] - 2x Nvidia A100â€

_æ³¨æ„: ä½ å¯èƒ½éœ€è¦å‘é‚®ä»¶ç»™ **[api-enterprise@huggingface.co](mailto:api-enterprise@huggingface.co)** ç”³è¯·é…é¢å‡çº§æ‰èƒ½è®¿é—® A100_

ä½ å¯ä»¥åœ¨æˆ‘ä»¬çš„åšæ–‡ä¸­è¯¦ç»†äº†è§£å¦‚ä½• [ä½¿ç”¨ Hugging Face æ¨ç†ç»ˆç«¯éƒ¨ç½² LLM](https://huggingface.co/blog/zh/inference-endpoints-llm)ï¼Œè¯¥ [åšæ–‡](https://huggingface.co/blog/zh/inference-endpoints-llm) è¿˜åŒ…å«äº†æœ‰å…³å…¶æ”¯æŒçš„è¶…å‚ä»¥åŠå¦‚ä½•ä½¿ç”¨ Python å’Œ Javascript API æµå¼ç”Ÿæˆæ–‡æœ¬çš„ç›¸å…³çŸ¥è¯†ã€‚

è¯„ä¼°
--

ä»£ç è¯­è¨€æ¨¡å‹é€šå¸¸åœ¨ HumanEval ç­‰æ•°æ®é›†ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå…¶åŒ…å«äº†ä¸€ç³»åˆ—ç¼–ç¨‹é¢˜ï¼Œæˆ‘ä»¬å°†å‡½æ•°ç­¾åå’Œæ–‡æ¡£å­—ç¬¦ä¸²è¾“å…¥ç»™æ¨¡å‹ï¼Œæ¨¡å‹éœ€è¦å®Œæˆå‡½æ•°ä½“ä»£ç çš„ç¼–å†™ã€‚æ¥ç€æ˜¯è¿è¡Œä¸€ç»„é¢„å®šä¹‰çš„å•å…ƒæµ‹è¯•æ¥éªŒè¯æ‰€æå‡ºçš„è§£ç­”ã€‚æœ€åæ˜¯æŠ¥å‘Šé€šè¿‡ç‡ï¼Œå³æœ‰å¤šå°‘è§£ç­”é€šè¿‡äº†æ‰€æœ‰æµ‹è¯•ã€‚pass@1 åº¦é‡äº†æ¨¡å‹ä¸€æ¬¡ç”Ÿæˆå³é€šè¿‡çš„é¢‘ç‡ï¼Œè€Œ pass@10 æè¿°äº†æ¨¡å‹ç”Ÿæˆ 10 ä¸ªå€™é€‰è§£ç­”å…¶ä¸­è‡³å°‘æœ‰ä¸€ä¸ªè§£ç­”é€šè¿‡çš„é¢‘ç‡ã€‚

è™½ç„¶ HumanEval æ˜¯ä¸€ä¸ª Python åŸºå‡†æµ‹è¯•ï¼Œä½†ç¤¾åŒºä»˜å‡ºäº†å·¨å¤§åŠªåŠ›å°†å…¶è½¬æˆæ›´å¤šç¼–ç¨‹è¯­è¨€ï¼Œä»è€Œå®ç°æ›´å…¨é¢çš„è¯„ä¼°ã€‚å…¶ä¸­ä¸€ç§æ–¹æ³•æ˜¯ [MultiPL-E](https://github.com/nuprl/MultiPL-E)ï¼Œå®ƒå°† HumanEval ç¿»è¯‘æˆåå¤šç§ç¼–ç¨‹è¯­è¨€ã€‚æˆ‘ä»¬æ­£åœ¨åŸºäºå…¶åˆ¶ä½œä¸€ä¸ª [å¤šè¯­è¨€ä»£ç æ’è¡Œæ¦œ](https://huggingface.co/spaces/bigcode/multilingual-code-evals)ï¼Œè¿™æ ·ç¤¾åŒºå°±å¯ä»¥ç”¨å®ƒæ¥æ¯”è¾ƒä¸åŒæ¨¡å‹åœ¨å„ç§ç¼–ç¨‹è¯­è¨€ä¸Šçš„è¡¨ç°ï¼Œä»¥è¯„ä¼°å“ªä¸ªæ¨¡å‹æœ€é€‚åˆä»–ä»¬çš„éœ€æ±‚ã€‚

æ¨¡å‹

è®¸å¯è¯

è®­ç»ƒæ•°æ®é›†æ˜¯å¦å·²çŸ¥

æ˜¯å¦å¯å•†ç”¨

é¢„è®­ç»ƒè¯å…ƒæ•°

Python

JavaScript

Leaderboard Avg Score

CodeLlaMa-34B

Llama 2 license

âŒ

âœ…

2,500B

45.11

41.66

33.89

CodeLlaMa-13B

Llama 2 license

âŒ

âœ…

2,500B

35.07

38.26

28.35

CodeLlaMa-7B

Llama 2 license

âŒ

âœ…

2,500B

29.98

31.8

24.36

CodeLlaMa-34B-Python

Llama 2 license

âŒ

âœ…

2,620B

53.29

44.72

33.87

CodeLlaMa-13B-Python

Llama 2 license

âŒ

âœ…

2,620B

42.89

40.66

28.67

CodeLlaMa-7B-Python

Llama 2 license

âŒ

âœ…

2,620B

40.48

36.34

23.5

CodeLlaMa-34B-Instruct

Llama 2 license

âŒ

âœ…

2,620B

50.79

45.85

35.09

CodeLlaMa-13B-Instruct

Llama 2 license

âŒ

âœ…

2,620B

50.6

40.91

31.29

CodeLlaMa-7B-Instruct

Llama 2 license

âŒ

âœ…

2,620B

45.65

33.11

26.45

StarCoder-15B

BigCode-OpenRail-M

âœ…

âœ…

1,035B

33.57

30.79

22.74

StarCoderBase-15B

BigCode-OpenRail-M

âœ…

âœ…

1,000B

30.35

31.7

22.4

WizardCoder-15B

BigCode-OpenRail-M

âŒ

âœ…

1,035B

58.12

41.91

32.07

OctoCoder-15B

BigCode-OpenRail-M

âœ…

âœ…

1,000B

45.3

32.8

24.01

CodeGeeX-2-6B

CodeGeeX License

âŒ

âŒ

2,000B

33.49

29.9

21.23

CodeGen-2.5-7B-Mono

Apache-2.0

âœ…

âœ…

1400B

45.65

23.22

12.1

CodeGen-2.5-7B-Multi

Apache-2.0

âœ…

âœ…

1400B

28.7

26.27

20.04

**æ³¨æ„:** ä¸Šè¡¨ä¸­çš„åˆ†æ•°æ¥è‡ªæˆ‘ä»¬çš„ä»£ç æ’è¡Œæ¦œï¼Œæ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨ç›¸åŒçš„è®¾ç½®ã€‚æ¬²äº†è§£æ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚é˜… [æ’è¡Œæ¦œ](https://huggingface.co/spaces/bigcode/multilingual-code-evals)ã€‚

å…¶ä»–èµ„æº
----

*   [Hub ä¸Šçš„æ¨¡å‹](https://huggingface.co/codellama)
*   [è®ºæ–‡](https://huggingface.co/papers/2308.12950)
*   [Meta å®˜å®£åšæ–‡](https://ai.meta.com/blog/code-llama-large-language-model-coding/)
*   [è´Ÿè´£ä»»ä½¿ç”¨æŒ‡å—](https://ai.meta.com/llama/responsible-use-guide/)
*   [æ¼”ç¤º (ä»£ç è¡¥å…¨ï¼Œæµå¼ç”Ÿæˆ)](https://huggingface.co/spaces/codellama/codellama-playground)
*   [æ¼”ç¤º (æŒ‡ä»¤å¾®è°ƒã€è‡ªå«ã€å¯å¤åˆ¶åˆ°è‡ªå·±çš„ç©ºé—´å¹¶ä¿®æ”¹)](https://huggingface.co/spaces/codellama/codellama-13b-chat)

> ğŸ¤— å®å­ä»¬å¯ä»¥æˆ³ **é˜…è¯»åŸæ–‡** æŸ¥çœ‹æ–‡ä¸­æ‰€æœ‰çš„å¤–éƒ¨é“¾æ¥å“Ÿï¼

* * *

> è‹±æ–‡åŸæ–‡: [https://hf.co/blog/codellama](https://hf.co/blog/codellama)
> 
> åŸæ–‡ä½œè€…: Philipp Schmidï¼ŒOmar Sansevieroï¼ŒPedro Cuencaï¼ŒLewis Tunstallï¼ŒLeandro von Werraï¼ŒLoubna Ben Allalï¼ŒArthur Zuckerï¼ŒJoao Gante
> 
> è¯‘è€…: Matrix Yao (å§šä¼Ÿå³°)ï¼Œè‹±ç‰¹å°”æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œå·¥ä½œæ–¹å‘ä¸º transformer-family æ¨¡å‹åœ¨å„æ¨¡æ€æ•°æ®ä¸Šçš„åº”ç”¨åŠå¤§è§„æ¨¡æ¨¡å‹çš„è®­ç»ƒæ¨ç†ã€‚
> 
> å®¡æ ¡/æ’ç‰ˆ: zhongdongy (é˜¿ä¸œ)