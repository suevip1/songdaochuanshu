---
layout: post
title: "æ®è¯´ï¼ŒTransformer ä¸èƒ½æœ‰æ•ˆåœ°è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ï¼Ÿ"
date: "2023-07-05T01:24:22.390Z"
---
æ®è¯´ï¼ŒTransformer ä¸èƒ½æœ‰æ•ˆåœ°è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ï¼Ÿ
=============================

ç®€ä»‹
--

å‡ ä¸ªæœˆå‰ï¼Œæˆ‘ä»¬ä»‹ç»äº† [Informer](https://huggingface.co/blog/informer) è¿™ä¸ªæ¨¡åž‹ï¼Œç›¸å…³è®ºæ–‡ ([Zhou, Haoyi, et al., 2021](https://arxiv.org/abs/2012.07436)) æ˜¯ä¸€ç¯‡èŽ·å¾—äº† AAAI 2021 æœ€ä½³è®ºæ–‡å¥–çš„æ—¶é—´åºåˆ—è®ºæ–‡ã€‚æˆ‘ä»¬ä¹Ÿå±•ç¤ºäº†ä¸€ä¸ªä½¿ç”¨ Informer è¿›è¡Œå¤šå˜é‡æ¦‚çŽ‡é¢„æµ‹çš„ä¾‹å­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºä»¥ä¸‹é—®é¢˜: [Transformer æ¨¡åž‹å¯¹æ—¶é—´åºåˆ—é¢„æµ‹çœŸçš„æœ‰æ•ˆå—ï¼Ÿ](https://arxiv.org/abs/2012.07436)æˆ‘ä»¬ç»™å‡ºçš„ç­”æ¡ˆæ˜¯ï¼Œå®ƒä»¬çœŸçš„æœ‰æ•ˆã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä¼šæä¾›ä¸€äº›å®žéªŒè¯æ®ï¼Œå±•ç¤ºå…¶çœŸæ­£çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å¯¹æ¯”å®žéªŒå°†è¡¨æ˜Žï¼Œ _DLinear_ è¿™ä¸ªç®€å•çº¿æ€§æ¨¡åž‹å¹¶æ²¡æœ‰åƒè¯´çš„é‚£æ ·æ¯” transformer å¥½ã€‚å½“æˆ‘ä»¬åœ¨åŒç­‰æ¨¡åž‹å¤§å°å’Œç›¸åŒè®¾å®šçš„æƒ…å†µä¸‹å¯¹æ¯”æ—¶ï¼Œæˆ‘ä»¬å‘çŽ°åŸºäºŽ transformer çš„æ¨¡åž‹åœ¨æˆ‘ä»¬å…³æ³¨çš„æµ‹è¯•æ ‡å‡†ä¸Šè¡¨çŽ°å¾—æ›´å¥½ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†ä¼šä»‹ç» _Autoformer_ æ¨¡åž‹ï¼Œç›¸å…³è®ºæ–‡ ([Wu, Haixu, et al., 2021](https://arxiv.org/abs/2106.13008)) åœ¨ Informer æ¨¡åž‹é—®ä¸–åŽå‘è¡¨åœ¨ NeurIPS 2021 ä¸Šã€‚Autoformer çš„æ¨¡åž‹çŽ°åœ¨å·²ç»å¯ä»¥åœ¨ ðŸ¤— Transformers ä¸­ [ä½¿ç”¨](https://huggingface.co/docs/transformers/main/en/model_doc/autoformer)ã€‚æœ€åŽï¼Œæˆ‘ä»¬è¿˜ä¼šè®¨è®º _DLinear_ æ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹æ˜¯ä¸€ä¸ªç®€å•çš„å‰å‘ç½‘ç»œï¼Œä½¿ç”¨äº† Autoformer ä¸­çš„åˆ†è§£å±‚ (decomposition layer)ã€‚DLinear æ¨¡åž‹æ˜¯åœ¨ [Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/abs/2205.13504) è¿™ç¯‡è®ºæ–‡ä¸­æå‡ºçš„ï¼Œæ–‡ä¸­å£°ç§°å…¶æ€§èƒ½åœ¨æ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸè¶…è¶Šäº† transformer ç³»åˆ—çš„ç®—æ³•ã€‚

ä¸‹é¢æˆ‘ä»¬å¼€å§‹ï¼

è¯„ä¼° Transformer ç³»åˆ—æ¨¡åž‹ å’Œ DLinear æ¨¡åž‹
--------------------------------

åœ¨ AAAI 2023 çš„è®ºæ–‡ [Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/abs/2205.13504) ä¸­ï¼Œä½œè€…å£°ç§° transformer ç³»åˆ—æ¨¡åž‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹æ–¹é¢å¹¶ä¸æœ‰æ•ˆã€‚ä»–ä»¬æ‹¿åŸºäºŽ transformer çš„æ¨¡åž‹ä¸Žä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡åž‹ _DLinear_ ä½œå¯¹æ¯”ã€‚DLinear ä½¿ç”¨äº† Autoformer ä¸­çš„ decomposition layer ç»“æž„ (ä¸‹æ–‡å°†ä¼šä»‹ç»)ï¼Œä½œè€…å£°ç§°å…¶æ€§èƒ½è¶…è¶Šäº†åŸºäºŽ transformer çš„æ¨¡åž‹ã€‚ä½†äº‹å®žçœŸçš„æ˜¯è¿™æ ·å—ï¼Ÿæˆ‘ä»¬æŽ¥ä¸‹æ¥çœ‹çœ‹ã€‚

Dataset

Autoformer (uni.) MASE

DLinear MASE

`Traffic`

0.910

0.965

`Exchange-Rate`

1.087

1.690

`Electricity`

0.751

0.831

ä¸Šè¡¨å±•ç¤ºäº† Autoformer å’Œ DLinear åœ¨ä¸‰ä¸ªè®ºæ–‡ä¸­ç”¨åˆ°çš„æ•°æ®é›†ä¸Šçš„è¡¨çŽ°ã€‚ç»“æžœè¯´æ˜Ž Autoformer åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¡¨çŽ°éƒ½è¶…è¶Šäº† DLinear æ¨¡åž‹ã€‚

æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä»‹ç» Autoformer å’Œ DLinear æ¨¡åž‹ï¼Œæ¼”ç¤ºæˆ‘ä»¬å¦‚ä½•åœ¨ä¸Šè¡¨ Traffic æ•°æ®é›†ä¸Šå¯¹æ¯”å®ƒä»¬çš„æ€§èƒ½ï¼Œå¹¶ä¸ºç»“æžœæä¾›ä¸€äº›å¯è§£é‡Šæ€§ã€‚

**å…ˆè¯´ç»“è®º:** ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡åž‹å¯èƒ½åœ¨æŸäº›ç‰¹å®šæƒ…å†µä¸‹æ›´æœ‰ä¼˜åŠ¿ï¼Œä½†å¯èƒ½æ— æ³•åƒ transformer ä¹‹ç±»çš„å¤æ‚æ¨¡åž‹é‚£æ ·å¤„ç†åæ–¹å·®ä¿¡æ¯ã€‚

Autoformer è¯¦ç»†ä»‹ç»
---------------

Autoformer åŸºäºŽä¼ ç»Ÿçš„æ—¶é—´åºåˆ—æ–¹æ³•: æŠŠæ—¶é—´åºåˆ—åˆ†è§£ä¸ºå­£èŠ‚æ€§ (seasonality) ä»¥åŠè¶‹åŠ¿ - å‘¨æœŸ (trend-cycle) è¿™äº›è¦ç´ ã€‚è¿™é€šè¿‡åŠ å…¥åˆ†è§£å±‚ ( _Decomposition Layer_ ) æ¥å®žçŽ°ï¼Œä»¥æ­¤æ¥å¢žå¼ºæ¨¡åž‹èŽ·å–è¿™äº›ä¿¡æ¯çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒAutoformer ä¸­è¿˜ç‹¬åˆ›äº†è‡ªç›¸å…³ (auto-correlation) æœºåˆ¶ï¼Œæ›¿æ¢æŽ‰äº†ä¼ ç»Ÿ transformer ä¸­çš„è‡ªæ³¨æ„åŠ› (self-attention)ã€‚è¯¥æœºåˆ¶ä½¿å¾—æ¨¡åž‹å¯ä»¥åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶ä¸­å‘¨æœŸæ€§çš„ä¾èµ–ï¼Œæå‡äº†æ€»ä½“æ€§èƒ½ã€‚

ä¸‹é¢ï¼Œæˆ‘ä»¬å°†æ·±å…¥æŽ¢è®¨ Autoformer çš„è¿™ä¸¤å¤§ä¸»è¦è´¡çŒ®: åˆ†è§£å±‚ ( _Decomposition Layer_ ) å’Œè‡ªç›¸å…³æœºåˆ¶ ( _Autocorrelation Mechanism_ )ã€‚ç›¸å…³ä»£ç ä¹Ÿä¼šæä¾›å‡ºæ¥ã€‚

### åˆ†è§£å±‚

åˆ†è§£æ˜¯ä¸€ä¸ªæ—¶é—´åºåˆ—é¢†åŸŸååˆ†å¸¸ç”¨çš„æ–¹æ³•ï¼Œä½†åœ¨ Autoformer ä»¥å‰éƒ½æ²¡æœ‰è¢«å¯†é›†é›†æˆå…¥æ·±åº¦å­¦ä¹ æ¨¡åž‹ä¸­ã€‚æˆ‘ä»¬å…ˆç®€å•ä»‹ç»è¿™ä¸€æ¦‚å¿µï¼ŒéšåŽä¼šä½¿ç”¨ PyTorch ä»£ç æ¼”ç¤ºè¿™ä¸€æ€è·¯æ˜¯å¦‚ä½•åº”ç”¨åˆ° Autoformer ä¸­çš„ã€‚

#### æ—¶é—´åºåˆ—åˆ†è§£

åœ¨æ—¶é—´åºåˆ—åˆ†æžä¸­ï¼Œåˆ†è§£ ([decomposition](https://en.wikipedia.org/wiki/Decomposition_of_time_series)) æ˜¯æŠŠä¸€ä¸ªæ—¶é—´åºåˆ—æ‹†åˆ†æˆä¸‰ä¸ªç³»ç»Ÿæ€§è¦ç´ çš„æ–¹æ³•: è¶‹åŠ¿å‘¨æœŸ (trend-cycle) ã€å­£èŠ‚æ€§å˜åŠ¨ (seasonal variation) å’Œéšæœºæ³¢åŠ¨ (random fluctuations)ã€‚è¶‹åŠ¿è¦ç´ ä»£è¡¨äº†æ—¶é—´åºåˆ—çš„é•¿æœŸèµ°åŠ¿æ–¹å‘; å­£èŠ‚è¦ç´ åæ˜ äº†ä¸€äº›åå¤å‡ºçŽ°çš„æ¨¡å¼ï¼Œä¾‹å¦‚ä»¥ä¸€å¹´æˆ–ä¸€å­£åº¦ä¸ºå‘¨æœŸå‡ºçŽ°çš„æ¨¡å¼; è€Œéšæœº (æ— è§„å¾‹) å› ç´ åˆ™åæ˜ äº†æ•°æ®ä¸­æ— æ³•è¢«ä¸Šè¿°ä¸¤ç§è¦ç´ è§£é‡Šçš„éšæœºå™ªå£°ã€‚

æœ‰ä¸¤ç§ä¸»æµçš„åˆ†è§£æ–¹æ³•: åŠ æ³•åˆ†è§£å’Œä¹˜æ³•åˆ†è§£ï¼Œè¿™åœ¨ [statsmodels](https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html) è¿™ä¸ªåº“é‡Œéƒ½æœ‰å®žçŽ°ã€‚é€šè¿‡åˆ†è§£æ—¶é—´åºåˆ—åˆ°è¿™ä¸‰ä¸ªè¦ç´ ï¼Œæˆ‘ä»¬èƒ½æ›´å¥½åœ°ç†è§£å’Œå»ºæ¨¡æ•°æ®ä¸­æ½œåœ¨çš„æ¨¡å¼ã€‚

ä½†æ€Žæ ·æŠŠåˆ†è§£é›†æˆè¿› transformer ç»“æž„å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥å‚è€ƒå‚è€ƒ Autoformer çš„åšæ³•ã€‚

#### Autoformer ä¸­çš„åˆ†è§£

![autoformer_architecture](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202307041742706.png)

Autoformer ç»“æž„ (æ¥è‡ª[è®ºæ–‡](https://arxiv.org/abs/2106.13008))

Autoformer æŠŠåˆ†è§£ä½œä¸ºä¸€ä¸ªå†…éƒ¨è®¡ç®—æ“ä½œé›†æˆåˆ°æ¨¡åž‹ä¸­ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºã€‚å¯ä»¥çœ‹åˆ°ï¼Œç¼–ç å™¨å’Œè§£ç å™¨éƒ½ä½¿ç”¨äº†åˆ†è§£æ¨¡å—æ¥é›†åˆ trend-cyclical ä¿¡æ¯ï¼Œå¹¶ä»Žåºåˆ—ä¸­æ¸è¿›åœ°æå– seasonal ä¿¡æ¯ã€‚è¿™ç§å†…éƒ¨åˆ†è§£çš„æ¦‚å¿µå·²ç»ä»Ž Autoformer ä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚æ‰€ä»¥å¾ˆå¤šå…¶å®ƒçš„æ—¶é—´åºåˆ—è®ºæ–‡ä¹Ÿå¼€å§‹é‡‡ç”¨è¿™ä¸€æ–¹æ³•ï¼Œä¾‹å¦‚ FEDformer ([Zhou, Tian, et al., ICML 2022](https://arxiv.org/abs/2201.12740)) å’Œ DLinear [(Zeng, Ailing, et al., AAAI 2023)](https://arxiv.org/abs/2205.13504)ï¼Œè¿™æ›´è¯´æ˜Žäº†å…¶åœ¨æ—¶é—´åºåˆ—å»ºæ¨¡ä¸­çš„æ„ä¹‰ã€‚

çŽ°åœ¨ï¼Œæˆ‘ä»¬æ­£å¼åœ°ç»™åˆ†è§£å±‚åšå‡ºå®šä¹‰:

å¯¹ä¸€ä¸ªé•¿åº¦ä¸º \\(L\\) çš„åºåˆ— \\(\\mathcal{X} \\in \\mathbb{R}^{L \\times d}\\)ï¼Œåˆ†è§£å±‚è¿”å›žçš„ \\(\\mathcal{X}\_\\textrm{trend} å’Œ \\mathcal{X}\_\\textrm{seasonal}\\) å®šä¹‰å¦‚ä¸‹:

\\\[\\mathcal{X}\_\\textrm{trend} = \\textrm{AvgPool(Padding(} \\mathcal{X} \\textrm{))} \\\\ \\mathcal{X}\_\\textrm{seasonal} = \\mathcal{X} - \\mathcal{X}\_\\textrm{trend} \\\]

å¯¹åº”çš„ PyTorch ä»£ç å®žçŽ°æ˜¯:

    import torch
    from torch import nn
    
    class DecompositionLayer(nn.Module):
        """
        Returns the trend and the seasonal parts of the time series.
        """
    
        def __init__(self, kernel_size):
            super().__init__()
            self.kernel_size = kernel_size
            self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=1, padding=0) # moving average
    
        def forward(self, x):
            """Input shape: Batch x Time x EMBED_DIM"""
            # padding on the both ends of time series
            num_of_pads = (self.kernel_size - 1) // 2
            front = x[:, 0:1, :].repeat(1, num_of_pads, 1)
            end = x[:, -1:, :].repeat(1, num_of_pads, 1)
            x_padded = torch.cat([front, x, end], dim=1)
    
            # calculate the trend and seasonal part of the series
            x_trend = self.avg(x_padded.permute(0, 2, 1)).permute(0, 2, 1)
            x_seasonal = x - x_trend
            return x_seasonal, x_trend
    

å¯è§ï¼Œä»£ç éžå¸¸ç®€å•ï¼Œå¯ä»¥å¾ˆæ–¹ä¾¿åœ°ç”¨åœ¨å…¶å®ƒæ¨¡åž‹ä¸­ï¼Œæ­£å¦‚ DLinear é‚£æ ·ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬è®²è§£ç¬¬äºŒä¸ªåˆ›æ–°ç‚¹: _æ³¨æ„åŠ› (è‡ªç›¸å…³) æœºåˆ¶_ ã€‚

### æ³¨æ„åŠ› (è‡ªç›¸å…³) æœºåˆ¶

![autoformer_autocorrelation_vs_full_attention](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/148_autoformer/autoformer_autocorrelation_vs_full_attention.png)

æœ€åŽŸå§‹çš„æ³¨æ„åŠ›æœºåˆ¶å’Œè‡ªç›¸å…³æœºåˆ¶ (å›¾ç‰‡æ¥è‡ª[è®ºæ–‡](https://arxiv.org/abs/2106.13008))

é™¤äº†åˆ†è§£å±‚ä¹‹å¤–ï¼ŒAutoformer è¿˜ä½¿ç”¨äº†ä¸€ä¸ªåŽŸåˆ›çš„è‡ªç›¸å…³ (autocorrelation) æœºåˆ¶ï¼Œå¯ä»¥å®Œç¾Žæ›¿æ¢è‡ªæ³¨æ„åŠ› (self-attention) æœºåˆ¶ã€‚åœ¨ [æœ€åŽŸå§‹çš„æ—¶é—´åºåˆ— transformer æ¨¡åž‹](https://huggingface.co/docs/transformers/model_doc/time_series_transformer) ä¸­ï¼Œæ³¨æ„åŠ›æƒé‡æ˜¯åœ¨æ—¶åŸŸè®¡ç®—å¹¶é€ç‚¹èšåˆçš„ã€‚è€Œä»Žä¸Šå›¾ä¸­å¯ä»¥çœ‹å‡ºï¼ŒAutoformer ä¸åŒçš„æ˜¯å®ƒåœ¨é¢‘åŸŸè®¡ç®—è¿™äº› (ä½¿ç”¨ [å¿«é€Ÿå‚…ç«‹å¶å˜æ¢](https://en.wikipedia.org/wiki/Fast_Fourier_transform))ï¼Œç„¶åŽé€šè¿‡æ—¶å»¶èšåˆå®ƒä»¬ã€‚

æŽ¥ä¸‹æ¥éƒ¨åˆ†ï¼Œæˆ‘ä»¬æ·±å…¥ç»†èŠ‚ï¼Œå¹¶ä½¿ç”¨ä»£ç ä½œå‡ºè®²è§£ã€‚

#### æ—¶åŸŸçš„æ³¨æ„åŠ›æœºåˆ¶

![autoformer_autocorrelation_only_attention](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202307041742450.png)

å€ŸåŠ© FFT åœ¨é¢‘åŸŸè®¡ç®—æ³¨æ„åŠ›æƒé‡ (å›¾ç‰‡æ¥è‡ª[è®ºæ–‡](https://arxiv.org/abs/2106.13008))

ç†è®ºä¸Šè®²ï¼Œç»™å®šä¸€ä¸ªæ—¶é—´å»¶è¿Ÿ \\(\\tau\\)ï¼Œä¸€ä¸ªç¦»æ•£å˜é‡çš„ _è‡ªç›¸å…³æ€§_ \\(y\\) å¯ä»¥ç”¨æ¥è¡¡é‡è¿™ä¸ªå˜é‡å½“å‰æ—¶åˆ» \\(t\\) çš„å€¼å’Œè¿‡åŽ»æ—¶åˆ» \\(t-\\tau\\) çš„å€¼ä¹‹é—´çš„â€œå…³ç³»â€(çš®å°”é€Šç›¸å…³æ€§ï¼Œpearson correlation):

\\\[\\textrm{Autocorrelation}(\\tau) = \\textrm{Corr}(y\_t, y\_{t-\\tau}) \\\]

ä½¿ç”¨è‡ªç›¸å…³æ€§ï¼ŒAutoformer æå–äº† query å’Œ key ä¹‹é—´åŸºäºŽé¢‘åŸŸçš„ç›¸äº’ä¾èµ–ï¼Œè€Œä¸æ˜¯åƒä¹‹å‰é‚£æ ·ä¸¤ä¸¤ä¹‹é—´çš„ç‚¹ä¹˜ã€‚å¯ä»¥æŠŠè¿™ä¸ªæ“ä½œçœ‹æˆæ˜¯è‡ªæ³¨æ„åŠ›ä¸­ \\(QK^T\\) çš„æ›¿æ¢ã€‚

å®žé™…æ“ä½œä¸­ï¼Œquery å’Œ key ä¹‹é—´çš„è‡ªç›¸å…³æ˜¯é€šè¿‡ FFT ä¸€æ¬¡æ€§é’ˆå¯¹ **æ‰€æœ‰æ—¶é—´å»¶è¿Ÿ** è®¡ç®—å‡ºæ¥çš„ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œè‡ªç›¸å…³æœºåˆ¶è¾¾åˆ°äº† \\(O(L \\log L)\\) çš„æ—¶é—´å¤æ‚åº¦ ( \\(L\\) æ˜¯è¾“å…¥æ—¶é—´é•¿åº¦)ï¼Œè¿™ä¸ªé€Ÿåº¦å’Œ [Informer çš„ ProbSparse attention](https://huggingface.co/blog/informer#probsparse-attention) æŽ¥è¿‘ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œä½¿ç”¨ FFT è®¡ç®—è‡ªç›¸å…³æ€§çš„ç†è®ºåŸºç¡€æ˜¯ [Wienerâ€“Khinchin theorem](https://en.wikipedia.org/wiki/Wiener%E2%80%93Khinchin_theorem)ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸ç»†è®²äº†ã€‚

çŽ°åœ¨ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ç›¸åº”çš„ PyTorch ä»£ç :

    import torch
    
    def autocorrelation(query_states, key_states):
        """
        Computes autocorrelation(Q,K) using `torch.fft`.
        Think about it as a replacement for the QK^T in the self-attention.
        
        Assumption: states are resized to same shape of [batch_size, time_length, embedding_dim].
        """
        query_states_fft = torch.fft.rfft(query_states, dim=1)
        key_states_fft = torch.fft.rfft(key_states, dim=1)
        attn_weights = query_states_fft * torch.conj(key_states_fft)
        attn_weights = torch.fft.irfft(attn_weights, dim=1)
        
        return attn_weights
    

ä»£ç éžå¸¸ç®€æ´ï¼ðŸ˜Ž è¯·æ³¨æ„è¿™åªæ˜¯ `autocorrelation(Q,K)` çš„éƒ¨åˆ†å®žçŽ°ï¼Œå®Œæ•´å®žçŽ°è¯·å‚è€ƒ ðŸ¤— Transformers ä¸­çš„ä»£ç ã€‚

æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨æ—¶å»¶å€¼èšåˆæˆ‘ä»¬çš„ `attn_weights` ï¼Œè¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸ºæ—¶å»¶èšåˆ ( _Time Delay Aggregation_ )ã€‚

#### æ—¶å»¶èšåˆ

![autoformer_autocorrelation_only_aggregation](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202307041742417.png)

é€šè¿‡æ—¶å»¶æ¥èšåˆï¼Œå›¾ç‰‡æ¥è‡ª [Autoformer è®ºæ–‡](https://arxiv.org/abs/2106.13008)

æˆ‘ä»¬ç”¨ \\(\\mathcal{R\_{Q,K}}\\) æ¥è¡¨ç¤ºè‡ªç›¸å…³ (å³ `attn_weights` )ã€‚é‚£ä¹ˆé—®é¢˜æ˜¯: æˆ‘ä»¬åº”è¯¥å¦‚ä½•èšåˆè¿™äº› \\(\\mathcal{R\_{Q,K}}(\\tau\_1), \\mathcal{R\_{Q,K}}(\\tau\_2), â€¦, \\mathcal{R\_{Q,K}}(\\tau\_k)\\) åˆ° \\(\\mathcal{V}\\) ä¸Šé¢ï¼Ÿåœ¨æ ‡å‡†çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œè¿™ç§èšåˆé€šè¿‡ç‚¹ä¹˜å®Œæˆã€‚ä½†åœ¨ Autoformer ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ç§ä¸åŒçš„æ–¹æ³•ã€‚é¦–å…ˆæˆ‘ä»¬åœ¨æ—¶å»¶ \\(\\tau\_1, \\tau\_2, â€¦ \\tau\_k\\) ä¸Šå¯¹é½ \\(\\mathcal{V}\\)ï¼Œè®¡ç®—åœ¨è¿™äº›æ—¶å»¶ä¸‹å®ƒå¯¹åº”çš„å€¼ï¼Œè¿™ä¸ªæ“ä½œå«ä½œ _Rolling_ ã€‚æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¯¹é½çš„ \\(\\mathcal{V}\\) å’Œè‡ªç›¸å…³çš„å€¼è¿›è¡Œé€ç‚¹çš„ä¹˜æ³•è¿ç®—ã€‚åœ¨ä¸Šå›¾ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°åœ¨å·¦è¾¹æ˜¯åŸºäºŽæ—¶å»¶å¯¹ \\(\\mathcal{V}\\) è¿›è¡Œçš„ Rolling æ“ä½œ; è€Œå³è¾¹å°±å±•ç¤ºäº†ä¸Žè‡ªç›¸å…³è¿›è¡Œçš„é€ç‚¹ä¹˜æ³•ã€‚

æ•´ä¸ªè¿‡ç¨‹å¯ä»¥ç”¨ä»¥ä¸‹å…¬å¼æ€»ç»“:

\\\[\\tau\_1, \\tau\_2, â€¦ \\tau\_k = \\textrm{arg Top-k}(\\mathcal{R\_{Q,K}}(\\tau)) \\ \\hat{\\mathcal{R}}\\mathcal{ \_{Q,K}}(\\tau \_1), \\hat{\\mathcal{R}}\\mathcal{\_ {Q,K}}(\\tau \_2), â€¦, \\hat{\\mathcal{R}}\\mathcal{\_ {Q,K}}(\\tau \_k) = \\textrm{Softmax}(\\mathcal{R\_ {Q,K}}(\\tau \_1), \\mathcal{R\_ {Q,K}}(\\tau\_2), â€¦, \\mathcal{R\_ {Q,K}}(\\tau\_k)) \\ \\textrm{Autocorrelation-Attention} = \\sum\_{i=1}^k \\textrm{Roll}(\\mathcal{V}, \\tau\_i) \\cdot \\hat{\\mathcal{R}}\\mathcal{\_{Q,K}}(\\tau \_i) \\\]

å°±æ˜¯è¿™æ ·ï¼éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ\\(k\\) æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º `autocorrelation_factor` (ç±»ä¼¼äºŽ [Informer](https://huggingface.co/blog/informer) é‡Œçš„ `sampling_factor` ) ; è€Œ softmax æ˜¯åœ¨ä¹˜æ³•æ“ä½œä¹‹å‰è¿ç”¨åˆ°è‡ªç›¸å…³ä¸Šé¢çš„ã€‚

çŽ°åœ¨ï¼Œæˆ‘ä»¬å·²ç»å¯ä»¥çœ‹çœ‹æœ€ç»ˆçš„ä»£ç äº†:

    import torch
    import math
    
    def time_delay_aggregation(attn_weights, value_states, autocorrelation_factor=2):
        """
        Computes aggregation as value_states.roll(delay)* top_k_autocorrelations(delay).
        The final result is the autocorrelation-attention output.
        Think about it as a replacement of the dot-product between attn_weights and value states.
        
        The autocorrelation_factor is used to find top k autocorrelations delays.
        Assumption: value_states and attn_weights shape: [batch_size, time_length, embedding_dim]
        """
        bsz, num_heads, tgt_len, channel = ...
        time_length = value_states.size(1)
        autocorrelations = attn_weights.view(bsz, num_heads, tgt_len, channel)
    
        # find top k autocorrelations delays
        top_k = int(autocorrelation_factor * math.log(time_length))
        autocorrelations_mean = torch.mean(autocorrelations, dim=(1, -1)) # bsz x tgt_len
        top_k_autocorrelations, top_k_delays = torch.topk(autocorrelations_mean, top_k, dim=1)
    
        # apply softmax on the channel dim
        top_k_autocorrelations = torch.softmax(top_k_autocorrelations, dim=-1) # bsz x top_k
    
        # compute aggregation: value_states.roll(delay)* top_k_autocorrelations(delay)
        delays_agg = torch.zeros_like(value_states).float() # bsz x time_length x channel
        for i in range(top_k):
            value_states_roll_delay = value_states.roll(shifts=-int(top_k_delays[i]), dims=1)
            top_k_at_delay = top_k_autocorrelations[:, i]
            # aggregation
            top_k_resized = top_k_at_delay.view(-1, 1, 1).repeat(num_heads, tgt_len, channel)
            delays_agg += value_states_roll_delay * top_k_resized
    
        attn_output = delays_agg.contiguous()
        return attn_output
    

å®Œæˆï¼Autoformer æ¨¡åž‹çŽ°åœ¨å·²ç»å¯ä»¥åœ¨ ðŸ¤— Transformers ä¸­ [ä½¿ç”¨](https://huggingface.co/docs/transformers/main/en/model_doc/autoformer) äº†ï¼Œåå­—å°±å« `AutoformerModel` ã€‚

é’ˆå¯¹è¿™ä¸ªæ¨¡åž‹ï¼Œæˆ‘ä»¬è¦å¯¹æ¯”å•å˜é‡ transformer æ¨¡åž‹ä¸Ž DLinear çš„æ€§èƒ½ï¼ŒDLinear æœ¬è´¨ä¹Ÿæ˜¯å•å˜é‡çš„ã€‚åŽé¢æˆ‘ä»¬ä¹Ÿä¼šå±•ç¤ºä¸¤ä¸ªå¤šå˜é‡ transformer æ¨¡åž‹çš„æ€§èƒ½ (åœ¨åŒä¸€æ•°æ®ä¸Šè®­ç»ƒçš„)ã€‚

DLinear è¯¦ç»†ä»‹ç»
------------

å®žé™…ä¸Šï¼ŒDLinear ç»“æž„éžå¸¸ç®€å•ï¼Œä»…ä»…æ˜¯ä»Ž Autoformer çš„ `DecompositionLayer` ä¸Šè¿žæŽ¥å…¨è¿žæŽ¥å±‚ã€‚å®ƒä½¿ç”¨ `DecompositionLayer` æ¥åˆ†è§£è¾“å…¥çš„ä¸–ç•Œåºåˆ—åˆ°æ®‹å·®éƒ¨åˆ† (å­£èŠ‚æ€§) å’Œè¶‹åŠ¿éƒ¨åˆ†ã€‚å‰å‘è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½è¢«è¾“å…¥åˆ°å„è‡ªçš„çº¿æ€§å±‚ï¼Œå¹¶è¢«æ˜ å°„æˆ `prediction_length` é•¿åº¦çš„è¾“å‡ºã€‚æœ€ç»ˆçš„è¾“å‡ºå°±æ˜¯ä¸¤ä¸ªè¾“å…¥çš„å’Œ:

    def forward(self, context):
        seasonal, trend = self.decomposition(context)
        seasonal_output = self.linear_seasonal(seasonal)
        trend_output = self.linear_trend(trend)
        return seasonal_output + trend_output
    

åœ¨è¿™ç§è®¾å®šä¸‹ï¼Œé¦–å…ˆæˆ‘ä»¬æŠŠè¾“å…¥çš„åºåˆ—æ˜ å°„æˆ `prediction-length * hidden` ç»´åº¦ (é€šè¿‡ `linear_seasonal` å’Œ `linear_trend` ä¸¤ä¸ªå±‚) ; å¾—åˆ°çš„ç»“æžœä¼šè¢«ç›¸åŠ èµ·æ¥ï¼Œå¹¶è½¬æ¢ä¸º `(prediction_length, hidden)` å½¢çŠ¶; æœ€åŽï¼Œç»´åº¦ä¸º `hidden` çš„éšæ€§è¡¨å¾ä¼šè¢«æ˜ å°„åˆ°æŸç§åˆ†å¸ƒçš„å‚æ•°ä¸Šã€‚

åœ¨æˆ‘ä»¬çš„æµ‹è¯„ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ [GluonTS](https://github.com/awslabs/gluonts) ä¸­ DLinear çš„å®žçŽ°ã€‚

ç¤ºä¾‹: Traffic æ•°æ®é›†
---------------

æˆ‘ä»¬å¸Œæœ›ç”¨å®žéªŒç»“æžœå±•ç¤ºåº“ä¸­åŸºäºŽ transformer æ¨¡åž‹çš„æ€§èƒ½ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ Traffic æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æœ‰ 862 æ¡æ—¶é—´åºåˆ—æ•°æ®ã€‚æˆ‘ä»¬å°†åœ¨æ¯æ¡æ—¶é—´åºåˆ—ä¸Šè®­ç»ƒä¸€ä¸ªå…±äº«çš„æ¨¡åž‹ (å•å˜é‡è®¾å®š)ã€‚æ¯ä¸ªæ—¶é—´åºåˆ—éƒ½ä»£è¡¨äº†ä¸€ä¸ªä¼ æ„Ÿå™¨çš„å æœ‰çŽ‡å€¼ï¼Œå€¼çš„èŒƒå›´åœ¨ 0 åˆ° 1 ä¹‹é—´ã€‚ä¸‹é¢çš„è¿™äº›è¶…å‚æ•°æˆ‘ä»¬å°†åœ¨æ‰€æœ‰æ¨¡åž‹ä¸­ä¿æŒä¸€è‡´ã€‚

    # Traffic prediction_length is 24. Reference:
    # https://github.com/awslabs/gluonts/blob/6605ab1278b6bf92d5e47343efcf0d22bc50b2ec/src/gluonts/dataset/repository/_lstnet.py#L105
    
    prediction_length = 24
    context_length = prediction_length*2
    batch_size = 128
    num_batches_per_epoch = 100
    epochs = 50
    scaling = "std"
    

ä½¿ç”¨çš„ transformer æ¨¡åž‹éƒ½å¾ˆå°:

    encoder_layers=2
    decoder_layers=2
    d_model=16
    

è¿™é‡Œæˆ‘ä»¬ä¸å†è®²è§£å¦‚ä½•ç”¨ `Autoformer` è®­ç»ƒæ¨¡åž‹ï¼Œè¯»è€…å¯ä»¥å‚è€ƒä¹‹å‰ä¸¤ç¯‡åšå®¢ ([TimeSeriesTransformer](https://huggingface.co/blog/time-series-transformers) å’Œ [Informer](https://huggingface.co/blog/informer)) å¹¶æ›¿æ¢æ¨¡åž‹ä¸º `Autoformer` ã€æ›¿æ¢æ•°æ®é›†ä¸º `traffic` ã€‚æˆ‘ä»¬ä¹Ÿè®­ç»ƒäº†çŽ°æˆçš„æ¨¡åž‹æ”¾åœ¨ HuggingFace Hub ä¸Šï¼Œç¨åŽçš„è¯„æµ‹å°†ä¼šä½¿ç”¨è¿™é‡Œçš„æ¨¡åž‹ã€‚

è½½å…¥æ•°æ®é›†
-----

é¦–å…ˆå®‰è£…å¿…è¦çš„åº“:

    !pip install -q transformers datasets evaluate accelerate "gluonts[torch]" ujson tqdm
    

`traffic` æ•°æ®é›† ([Lai et al. (2017)](https://arxiv.org/abs/1703.07015)) åŒ…å«äº†æ—§é‡‘å±±çš„äº¤é€šæ•°æ®ã€‚å®ƒåŒ…å« 862 æ¡ä»¥å°æ—¶ä¸ºæ—¶é—´å•ä½çš„æ—¶é—´åºåˆ—ï¼Œä»£è¡¨äº†é“è·¯å æœ‰çŽ‡çš„æ•°å€¼ï¼Œå…¶æ•°å€¼èŒƒå›´ä¸º \\(\[0, 1\]\\)ï¼Œè®°å½•äº†æ—§é‡‘å±±æ¹¾åŒºé«˜é€Ÿå…¬è·¯ä»Ž 2015 å¹´åˆ° 2016 å¹´çš„æ•°æ®ã€‚

    from gluonts.dataset.repository.datasets import get_dataset
    
    dataset = get_dataset("traffic")
    freq = dataset.metadata.freq
    prediction_length = dataset.metadata.prediction_length
    

æˆ‘ä»¬å¯è§†åŒ–ä¸€æ¡æ—¶é—´åºåˆ—çœ‹çœ‹ï¼Œå¹¶ç”»å‡ºè®­ç»ƒå’Œæµ‹è¯•é›†çš„åˆ’åˆ†:

    import matplotlib.pyplot as plt
    
    train_example = next(iter(dataset.train))
    test_example = next(iter(dataset.test))
    
    num_of_samples = 4*prediction_length
    
    figure, axes = plt.subplots()
    axes.plot(train_example["target"][-num_of_samples:], color="blue")
    axes.plot(
        test_example["target"][-num_of_samples - prediction_length :],
        color="red",
        alpha=0.5,
    )
    
    plt.show()
    

![](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202307041743408.png)

å®šä¹‰è®­ç»ƒå’Œæµ‹è¯•é›†åˆ’åˆ†:

    train_dataset = dataset.train
    test_dataset = dataset.test
    

å®šä¹‰æ•°æ®å˜æ¢
------

æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰æ•°æ®çš„å˜æ¢ï¼Œå°¤å…¶æ˜¯æ—¶é—´ç›¸å…³ç‰¹å¾çš„åˆ¶ä½œ (åŸºäºŽæ•°æ®é›†æœ¬èº«å’Œä¸€äº›æ™®é€‚åšæ³•)ã€‚

æˆ‘ä»¬å®šä¹‰ä¸€ä¸ª `Chain` ï¼Œä»£è¡¨ GluonTS ä¸­ä¸€ç³»åˆ—çš„å˜æ¢ (è¿™ç±»ä¼¼å›¾åƒé‡Œ `torchvision.transforms.Compose` )ã€‚è¿™è®©æˆ‘ä»¬å°†ä¸€ç³»åˆ—å˜æ¢é›†æˆåˆ°ä¸€ä¸ªå¤„ç†æµæ°´çº¿ä¸­ã€‚

ä¸‹é¢ä»£ç ä¸­ï¼Œæ¯ä¸ªå˜æ¢éƒ½æ·»åŠ äº†æ³¨é‡Šï¼Œç”¨ä»¥è¯´æ˜Žå®ƒä»¬çš„ä½œç”¨ã€‚ä»Žæ›´é«˜å±‚æ¬¡è®²ï¼Œæˆ‘ä»¬å°†éåŽ†æ¯ä¸€ä¸ªæ—¶é—´åºåˆ—ï¼Œå¹¶æ·»åŠ æˆ–åˆ é™¤ä¸€äº›ç‰¹å¾:

    from transformers import PretrainedConfig
    from gluonts.time_feature import time_features_from_frequency_str
    
    from gluonts.dataset.field_names import FieldName
    from gluonts.transform import (
        AddAgeFeature,
        AddObservedValuesIndicator,
        AddTimeFeatures,
        AsNumpyArray,
        Chain,
        ExpectedNumInstanceSampler,
        RemoveFields,
        SelectFields,
        SetField,
        TestSplitSampler,
        Transformation,
        ValidationSplitSampler,
        VstackFeatures,
        RenameFields,
    )
    
    def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:
        # create a list of fields to remove later
        remove_field_names = []
        if config.num_static_real_features == 0:
            remove_field_names.append(FieldName.FEAT_STATIC_REAL)
        if config.num_dynamic_real_features == 0:
            remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)
        if config.num_static_categorical_features == 0:
            remove_field_names.append(FieldName.FEAT_STATIC_CAT)
    
        return Chain(
            # step 1: remove static/dynamic fields if not specified
            [RemoveFields(field_names=remove_field_names)]
            # step 2: convert the data to NumPy (potentially not needed)
            + (
                [
                    AsNumpyArray(
                        field=FieldName.FEAT_STATIC_CAT,
                        expected_ndim=1,
                        dtype=int,
                    )
                ]
                if config.num_static_categorical_features > 0
                else []
            )
            + (
                [
                    AsNumpyArray(
                        field=FieldName.FEAT_STATIC_REAL,
                        expected_ndim=1,
                    )
                ]
                if config.num_static_real_features > 0
                else []
            )
            + [
                AsNumpyArray(
                    field=FieldName.TARGET,
                    # we expect an extra dim for the multivariate case:
                    expected_ndim=1 if config.input_size == 1 else 2,
                ),
                # step 3: handle the NaN's by filling in the target with zero
                # and return the mask (which is in the observed values)
                # true for observed values, false for nan's
                # the decoder uses this mask (no loss is incurred for unobserved values)
                # see loss_weights inside the xxxForPrediction model
                AddObservedValuesIndicator(
                    target_field=FieldName.TARGET,
                    output_field=FieldName.OBSERVED_VALUES,
                ),
                # step 4: add temporal features based on freq of the dataset
                # these serve as positional encodings
                AddTimeFeatures(
                    start_field=FieldName.START,
                    target_field=FieldName.TARGET,
                    output_field=FieldName.FEAT_TIME,
                    time_features=time_features_from_frequency_str(freq),
                    pred_length=config.prediction_length,
                ),
                # step 5: add another temporal feature (just a single number)
                # tells the model where in the life the value of the time series is
                # sort of running counter
                AddAgeFeature(
                    target_field=FieldName.TARGET,
                    output_field=FieldName.FEAT_AGE,
                    pred_length=config.prediction_length,
                    log_scale=True,
                ),
                # step 6: vertically stack all the temporal features into the key FEAT_TIME
                VstackFeatures(
                    output_field=FieldName.FEAT_TIME,
                    input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]
                    + (
                        [FieldName.FEAT_DYNAMIC_REAL]
                        if config.num_dynamic_real_features > 0
                        else []
                    ),
                ),
                # step 7: rename to match HuggingFace names
                RenameFields(
                    mapping={
                        FieldName.FEAT_STATIC_CAT: "static_categorical_features",
                        FieldName.FEAT_STATIC_REAL: "static_real_features",
                        FieldName.FEAT_TIME: "time_features",
                        FieldName.TARGET: "values",
                        FieldName.OBSERVED_VALUES: "observed_mask",
                    }
                ),
            ]
        )
    

å®šä¹‰ `InstanceSplitter`
---------------------

æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª `InstanceSplitter` ï¼Œç”¨æ¥ç»™è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†æä¾›é‡‡æ ·çª—å£ï¼Œå¾—åˆ°ä¸€æ®µæ—¶é—´çš„å†…çš„æ—¶é—´åºåˆ— (æˆ‘ä»¬ä¸å¯èƒ½æŠŠå®Œæ•´çš„æ•´æ®µæ•°æ®è¾“å…¥ç»™æ¨¡åž‹ï¼Œæ¯•ç«Ÿæ—¶é—´å¤ªé•¿ï¼Œè€Œä¸”ä¹Ÿæœ‰å†…å­˜é™åˆ¶)ã€‚

è¿™ä¸ªå®žä¾‹åˆ†å‰²å·¥å…·æ¯ä¸€æ¬¡å°†ä¼šéšæœºé€‰å– `context_length` é•¿åº¦çš„æ•°æ®ï¼Œä»¥åŠç´§éšå…¶åŽçš„ `prediction_length` é•¿åº¦çš„çª—å£ï¼Œå¹¶ä¸ºç›¸åº”çš„çª—å£æ ‡æ³¨ `past_` æˆ– `future_` ã€‚è¿™æ ·å¯ä»¥ä¿è¯ `values` èƒ½è¢«åˆ†ä¸º `past_values` å’ŒéšåŽçš„ `future_values` ï¼Œå„è‡ªä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨çš„è¾“å…¥ã€‚é™¤äº† `values` ï¼Œå¯¹äºŽ `time_series_fields` ä¸­çš„å…¶å®ƒ key å¯¹åº”çš„æ•°æ®ä¹Ÿæ˜¯ä¸€æ ·ã€‚

    from gluonts.transform import InstanceSplitter
    from gluonts.transform.sampler import InstanceSampler
    from typing import Optional
    
    def create_instance_splitter(
        config: PretrainedConfig,
        mode: str,
        train_sampler: Optional[InstanceSampler] = None,
        validation_sampler: Optional[InstanceSampler] = None,
    ) -> Transformation:
        assert mode in ["train", "validation", "test"]
    
        instance_sampler = {
            "train": train_sampler
            or ExpectedNumInstanceSampler(
                num_instances=1.0, min_future=config.prediction_length
            ),
            "validation": validation_sampler
            or ValidationSplitSampler(min_future=config.prediction_length),
            "test": TestSplitSampler(),
        }[mode]
    
        return InstanceSplitter(
            target_field="values",
            is_pad_field=FieldName.IS_PAD,
            start_field=FieldName.START,
            forecast_start_field=FieldName.FORECAST_START,
            instance_sampler=instance_sampler,
            past_length=config.context_length + max(config.lags_sequence),
            future_length=config.prediction_length,
            time_series_fields=["time_features", "observed_mask"],
        )
    

åˆ›å»º PyTorch çš„ DataLoader
-----------------------

æŽ¥ä¸‹æ¥å°±è¯¥åˆ›å»º PyTorch DataLoader äº†: è¿™è®©æˆ‘ä»¬èƒ½æŠŠæ•°æ®æ•´ç†æˆ batch çš„å½¢å¼ï¼Œå³ (input, output) å¯¹çš„å½¢å¼ï¼Œæˆ–è€…è¯´æ˜¯ ( `past_values` , `future_values` ) çš„å½¢å¼ã€‚

    from typing import Iterable
    
    import torch
    from gluonts.itertools import Cyclic, Cached
    from gluonts.dataset.loader import as_stacked_batches
    
    def create_train_dataloader(
        config: PretrainedConfig,
        freq,
        data,
        batch_size: int,
        num_batches_per_epoch: int,
        shuffle_buffer_length: Optional[int] = None,
        cache_data: bool = True,
     **kwargs,
    ) -> Iterable:
        PREDICTION_INPUT_NAMES = [
            "past_time_features",
            "past_values",
            "past_observed_mask",
            "future_time_features",
        ]
        if config.num_static_categorical_features > 0:
            PREDICTION_INPUT_NAMES.append("static_categorical_features")
    
        if config.num_static_real_features > 0:
            PREDICTION_INPUT_NAMES.append("static_real_features")
    
        TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [
            "future_values",
            "future_observed_mask",
        ]
    
        transformation = create_transformation(freq, config)
        transformed_data = transformation.apply(data, is_train=True)
        if cache_data:
            transformed_data = Cached(transformed_data)
    
        # we initialize a Training instance
        instance_splitter = create_instance_splitter(config, "train")
    
        # the instance splitter will sample a window of
        # context length + lags + prediction length (from the 366 possible transformed time series)
        # randomly from within the target time series and return an iterator.
        stream = Cyclic(transformed_data).stream()
        training_instances = instance_splitter.apply(stream, is_train=True)
    
        return as_stacked_batches(
            training_instances,
            batch_size=batch_size,
            shuffle_buffer_length=shuffle_buffer_length,
            field_names=TRAINING_INPUT_NAMES,
            output_type=torch.tensor,
            num_batches_per_epoch=num_batches_per_epoch,
        )
    
    def create_test_dataloader(
        config: PretrainedConfig,
        freq,
        data,
        batch_size: int,
     **kwargs,
    ):
        PREDICTION_INPUT_NAMES = [
            "past_time_features",
            "past_values",
            "past_observed_mask",
            "future_time_features",
        ]
        if config.num_static_categorical_features > 0:
            PREDICTION_INPUT_NAMES.append("static_categorical_features")
    
        if config.num_static_real_features > 0:
            PREDICTION_INPUT_NAMES.append("static_real_features")
    
        transformation = create_transformation(freq, config)
        transformed_data = transformation.apply(data, is_train=False)
    
        # we create a Test Instance splitter which will sample the very last
        # context window seen during training only for the encoder.
        instance_sampler = create_instance_splitter(config, "test")
    
        # we apply the transformations in test mode
        testing_instances = instance_sampler.apply(transformed_data, is_train=False)
    
        return as_stacked_batches(
            testing_instances,
            batch_size=batch_size,
            output_type=torch.tensor,
            field_names=PREDICTION_INPUT_NAMES,
        )
    

åœ¨ Autoformer ä¸Šè¯„æµ‹
----------------

æˆ‘ä»¬å·²ç»åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šé¢„è®­ç»ƒäº†ä¸€ä¸ª Autoformer äº†ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç›´æŽ¥æ‹¿æ¥æ¨¡åž‹åœ¨æµ‹è¯•é›†ä¸Šæµ‹ä¸€ä¸‹:

    from transformers import AutoformerConfig, AutoformerForPrediction
    
    config = AutoformerConfig.from_pretrained("kashif/autoformer-traffic-hourly")
    model = AutoformerForPrediction.from_pretrained("kashif/autoformer-traffic-hourly")
    
    test_dataloader = create_test_dataloader(
        config=config,
        freq=freq,
        data=test_dataset,
        batch_size=64,
    )
    

åœ¨æŽ¨ç†æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡åž‹çš„ `generate()` æ–¹æ³•æ¥é¢„æµ‹ `prediction_length` æ­¥çš„æœªæ¥æ•°æ®ï¼ŒåŸºäºŽæœ€è¿‘ä½¿ç”¨çš„å¯¹åº”æ—¶é—´åºåˆ—çš„çª—å£é•¿åº¦ã€‚

    from accelerate import Accelerator
    
    accelerator = Accelerator()
    device = accelerator.device
    model.to(device)
    model.eval()
    
    forecasts_ = []
    for batch in test_dataloader:
        outputs = model.generate(
            static_categorical_features=batch["static_categorical_features"].to(device)
            if config.num_static_categorical_features > 0
            else None,
            static_real_features=batch["static_real_features"].to(device)
            if config.num_static_real_features > 0
            else None,
            past_time_features=batch["past_time_features"].to(device),
            past_values=batch["past_values"].to(device),
            future_time_features=batch["future_time_features"].to(device),
            past_observed_mask=batch["past_observed_mask"].to(device),
        )
        forecasts_.append(outputs.sequences.cpu().numpy())
    

æ¨¡åž‹è¾“å‡ºçš„æ•°æ®å½¢çŠ¶æ˜¯ ( `batch_size` , `number of samples` , `prediction length` , `input_size` )ã€‚

åœ¨ä¸‹é¢è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä¸ºé¢„æµ‹æŽ¥ä¸‹æ¥ 24 å°æ—¶çš„äº¤é€šæ•°æ®è€Œå¾—åˆ°äº† 100 æ¡å¯èƒ½çš„æ•°å€¼ï¼Œè€Œ batch size æ˜¯ 64:

    forecasts_[0].shape
    
    >>> (64, 100, 24)
    

æˆ‘ä»¬åœ¨åž‚ç›´æ–¹å‘æŠŠå®ƒä»¬å †å èµ·æ¥ (ä½¿ç”¨ `numpy.vstack` å‡½æ•°)ï¼Œä»¥æ­¤èŽ·å–æ‰€æœ‰æµ‹è¯•é›†æ—¶é—´åºåˆ—çš„é¢„æµ‹: æˆ‘ä»¬æœ‰ `7` ä¸ªæ»šåŠ¨çš„çª—å£ï¼Œæ‰€ä»¥æœ‰ `7 * 862 = 6034` ä¸ªé¢„æµ‹ã€‚

    import numpy as np
    
    forecasts = np.vstack(forecasts_)
    print(forecasts.shape)
    
    >>> (6034, 100, 24)
    

æˆ‘ä»¬å¯ä»¥æŠŠé¢„æµ‹ç»“æžœå’Œ ground truth åšä¸ªå¯¹æ¯”ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) è¿™ä¸ªåº“ï¼Œå®ƒé‡Œé¢åŒ…å«äº† [MASE](https://huggingface.co/spaces/evaluate-metric/mase) çš„åº¦é‡æ–¹æ³•ã€‚

æˆ‘ä»¬å¯¹æ¯ä¸ªæ—¶é—´åºåˆ—ç”¨è¿™ä¸€åº¦é‡æ ‡å‡†è®¡ç®—ç›¸åº”çš„å€¼ï¼Œå¹¶ç®—å‡ºå…¶å¹³å‡å€¼:

    from tqdm.autonotebook import tqdm
    from evaluate import load
    from gluonts.time_feature import get_seasonality
    
    mase_metric = load("evaluate-metric/mase")
    
    forecast_median = np.median(forecasts, 1)
    
    mase_metrics = []
    for item_id, ts in enumerate(tqdm(test_dataset)):
        training_data = ts["target"][:-prediction_length]
        ground_truth = ts["target"][-prediction_length:]
        mase = mase_metric.compute(
            predictions=forecast_median[item_id],
            references=np.array(ground_truth),
            training=np.array(training_data),
            periodicity=get_seasonality(freq))
        mase_metrics.append(mase["mase"])
    

æ‰€ä»¥ Autoformer æ¨¡åž‹çš„ç»“æžœæ˜¯:

    print(f"Autoformer univariate MASE: {np.mean(mase_metrics):.3f}")
    
    >>> Autoformer univariate MASE: 0.910
    

æˆ‘ä»¬è¿˜å¯ä»¥ç”»å‡ºä»»æ„æ—¶é—´åºåˆ—é¢„æµ‹é’ˆå¯¹å…¶ ground truth çš„å¯¹æ¯”ï¼Œè¿™éœ€è¦ä»¥ä¸‹å‡½æ•°:

    import matplotlib.dates as mdates
    import pandas as pd
    
    test_ds = list(test_dataset)
    
    def plot(ts_index):
        fig, ax = plt.subplots()
    
        index = pd.period_range(
            start=test_ds[ts_index][FieldName.START],
            periods=len(test_ds[ts_index][FieldName.TARGET]),
            freq=test_ds[ts_index][FieldName.START].freq,
        ).to_timestamp()
    
        ax.plot(
            index[-5*prediction_length:],
            test_ds[ts_index]["target"][-5*prediction_length:],
            label="actual",
        )
    
        plt.plot(
            index[-prediction_length:],
            np.median(forecasts[ts_index], axis=0),
            label="median",
        )
        
        plt.gcf().autofmt_xdate()
        plt.legend(loc="best")
        plt.show()
    

æ¯”å¦‚ï¼Œæµ‹è¯•é›†ä¸­ç¬¬å››ä¸ªæ—¶é—´åºåˆ—çš„ç»“æžœå¯¹æ¯”ï¼Œç”»å‡ºæ¥æ˜¯è¿™æ ·:

    plot(4)
    

![](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202307041743418.png)

åœ¨ DLinear ä¸Šè¯„æµ‹
-------------

`gluonts` æä¾›äº†ä¸€ç§ DLinear çš„å®žçŽ°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªå®žçŽ°åŒºè®­ç»ƒã€æµ‹è¯„è¯¥ç®—æ³•:

    from gluonts.torch.model.d_linear.estimator import DLinearEstimator
    
    # Define the DLinear model with the same parameters as the Autoformer model
    estimator = DLinearEstimator(
        prediction_length=dataset.metadata.prediction_length,
        context_length=dataset.metadata.prediction_length*2,
        scaling=scaling,
        hidden_dimension=2,
        
        batch_size=batch_size,
        num_batches_per_epoch=num_batches_per_epoch,
        trainer_kwargs=dict(max_epochs=epochs)
    )
    

è®­ç»ƒæ¨¡åž‹:

    predictor = estimator.train(
        training_data=train_dataset,
        cache_data=True,
        shuffle_buffer_length=1024
    )
    
    >>> INFO:pytorch_lightning.callbacks.model_summary:
          | Name  | Type         | Params
        ---------------------------------------
        0 | model | DLinearModel | 4.7 K 
        ---------------------------------------
        4.7 K     Trainable params
        0 Non-trainable params
        4.7 K     Total params
        0.019 Total estimated model params size (MB)
    
        Training: 0it [00:00, ?it/s]
        ...
        INFO:pytorch_lightning.utilities.rank_zero:Epoch 49, global step 5000: 'train_loss' was not in top 1
        INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
    

åœ¨æµ‹è¯•é›†ä¸Šè¯„æµ‹:

    from gluonts.evaluation import make_evaluation_predictions, Evaluator
    
    forecast_it, ts_it = make_evaluation_predictions(
        dataset=dataset.test,
        predictor=predictor,
    )
    
    d_linear_forecasts = list(forecast_it)
    d_linear_tss = list(ts_it)
    
    evaluator = Evaluator()
    
    agg_metrics, _ = evaluator(iter(d_linear_tss), iter(d_linear_forecasts))
    

æ‰€ä»¥ DLinear å¯¹åº”çš„ç»“æžœæ˜¯:

    dlinear_mase = agg_metrics["MASE"]
    print(f"DLinear MASE: {dlinear_mase:.3f}")
    
    >>> DLinear MASE: 0.965
    

åŒæ ·åœ°ï¼Œæˆ‘ä»¬ç”»å‡ºé¢„æµ‹ç»“æžœä¸Ž ground truth çš„å¯¹æ¯”æ›²çº¿å›¾:

    def plot_gluonts(index):
        plt.plot(d_linear_tss[index][-4 * dataset.metadata.prediction_length:].to_timestamp(), label="target")
        d_linear_forecasts[index].plot(show_label=True, color='g')
        plt.legend()
        plt.gcf().autofmt_xdate()
        plt.show()
    

    plot_gluonts(4)
    

![](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202307041743757.png)

å®žé™…ä¸Šï¼Œ `traffic` æ•°æ®é›†åœ¨å¹³æ—¥å’Œå‘¨æœ«ä¼šå‡ºçŽ°ä¼ æ„Ÿå™¨ä¸­æ¨¡å¼çš„åˆ†å¸ƒåç§»ã€‚é‚£æˆ‘ä»¬è¿˜åº”è¯¥æ€Žä¹ˆåšå‘¢ï¼Ÿç”±äºŽ DLinear æ²¡æœ‰è¶³å¤Ÿçš„èƒ½åŠ›åŽ»å¤„ç†åæ–¹å·®ä¿¡æ¯ï¼Œæˆ–è€…è¯´æ˜¯ä»»ä½•çš„æ—¥æœŸæ—¶é—´çš„ç‰¹å¾ï¼Œæˆ‘ä»¬ç»™å‡ºçš„çª—å£å¤§å°æ— æ³•è¦†ç›–å…¨é¢ï¼Œä½¿å¾—è®©æ¨¡åž‹æœ‰è¶³å¤Ÿä¿¡æ¯åŽ»çŸ¥é“å½“å‰æ˜¯åœ¨é¢„æµ‹å¹³æ—¥æ•°æ®è¿˜æ˜¯å‘¨æœ«æ•°æ®ã€‚å› æ­¤æ¨¡åž‹åªä¼šåŽ»é¢„æµ‹æ›´ä¸ºæ™®é€‚çš„ç»“æžœï¼Œè¿™å°±å¯¼è‡´å…¶é¢„æµ‹åˆ†å¸ƒåå‘å¹³æ—¥æ•°æ®ï¼Œå› è€Œå¯¼è‡´å¯¹å‘¨æœ«æ•°æ®çš„é¢„æµ‹å˜å¾—æ›´å·®ã€‚å½“ç„¶ï¼Œå¦‚æžœæˆ‘ä»¬ç»™ä¸€ä¸ªè¶³å¤Ÿå¤§çš„çª—å£ï¼Œä¸€ä¸ªçº¿æ€§æ¨¡åž‹ä¹Ÿå¯ä»¥è¯†åˆ«å‡ºå‘¨æœ«çš„æ¨¡å¼ï¼Œä½†å½“æˆ‘ä»¬çš„æ•°æ®ä¸­å­˜åœ¨ä»¥æœˆæˆ–ä»¥å­£åº¦ä¸ºå•ä½çš„æ¨¡å¼åˆ†å¸ƒæ—¶ï¼Œé‚£å°±éœ€è¦æ›´å¤§çš„çª—å£äº†ã€‚

æ€»ç»“
--

æ‰€ä»¥ transformer æ¨¡åž‹å’Œçº¿æ€§æ¨¡åž‹å¯¹æ¯”çš„ç»“è®ºæ˜¯ä»€ä¹ˆå‘¢ï¼Ÿä¸åŒæ¨¡åž‹åœ¨æµ‹è¯•é›†ä¸Šçš„ MASE æŒ‡æ ‡å¦‚ä¸‹æ‰€ç¤º:

Dataset

Transformer (uni.)

Transformer (mv.)

Informer (uni.)

Informer (mv.)

Autoformer (uni.)

DLinear

`Traffic`

**0.876**

1.046

0.924

1.131

0.910

0.965

å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬åŽ»å¹´å¼•å…¥çš„ [æœ€åŽŸå§‹çš„ Transformer æ¨¡åž‹](https://huggingface.co/docs/transformers/model_doc/time_series_transformer) èŽ·å¾—äº†æœ€å¥½çš„æ€§èƒ½æŒ‡æ ‡ã€‚å…¶æ¬¡ï¼Œå¤šå˜é‡æ¨¡åž‹ä¸€èˆ¬éƒ½æ¯”å¯¹åº”çš„å•å˜é‡æ¨¡åž‹æ›´å·®ï¼ŒåŽŸå› åœ¨äºŽåºåˆ—é—´çš„ç›¸å…³æ€§å…³ç³»ä¸€èˆ¬éƒ½è¾ƒéš¾é¢„æµ‹ã€‚é¢å¤–æ·»åŠ çš„æ³¢åŠ¨é€šå¸¸ä¼šæŸåé¢„æµ‹ç»“æžœï¼Œæˆ–è€…æ¨¡åž‹å¯èƒ½ä¼šå­¦åˆ°ä¸€äº›é”™è¯¯çš„ç›¸å…³æ€§ä¿¡æ¯ã€‚æœ€è¿‘çš„ä¸€äº›è®ºæ–‡ï¼Œå¦‚ [CrossFormer](https://openreview.net/forum?id=vSVLM2j9eie) (ICLR 23) å’Œ [CARD](https://arxiv.org/abs/2305.12095) ä¹Ÿåœ¨å°è¯•è§£å†³è¿™äº› transformer æ¨¡åž‹ä¸­çš„é—®é¢˜ã€‚  
å¤šå˜é‡æ¨¡åž‹é€šå¸¸åœ¨è®­ç»ƒæ•°æ®è¶³å¤Ÿå¤§çš„æ—¶å€™æ‰ä¼šè¡¨çŽ°å¾—å¥½ã€‚ä½†å½“æˆ‘ä»¬ä¸Žå•å˜é‡æ¨¡åž‹åœ¨å°çš„å…¬å¼€æ•°æ®é›†ä¸Šå¯¹æ¯”æ—¶ï¼Œé€šå¸¸å•å˜é‡æ¨¡åž‹ä¼šè¡¨çŽ°å¾—æ›´å¥½ã€‚ç›¸å¯¹äºŽçº¿æ€§æ¨¡åž‹ï¼Œé€šå¸¸å…¶ç›¸åº”å°ºå¯¸çš„å•å˜é‡ transformer æ¨¡åž‹æˆ–å…¶å®ƒç¥žç»ç½‘ç»œç±»æ¨¡åž‹ä¼šè¡¨çŽ°å¾—æ›´å¥½ã€‚

æ€»ç»“æ¥è®²ï¼Œtransformer æ¨¡åž‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸï¼Œè¿œæ²¡æœ‰è¾¾åˆ°è¦è¢«æ·˜æ±°çš„å¢ƒåœ°ã€‚  
ç„¶è€Œå¤§è§„æ¨¡è®­ç»ƒæ•°æ®å¯¹å®ƒå·¨å¤§æ½œåŠ›çš„æŒ–æŽ˜æ˜¯è‡³å…³é‡è¦çš„ï¼Œè¿™ä¸€ç‚¹ä¸åƒ CV æˆ– NLP é¢†åŸŸï¼Œæ—¶é—´åºåˆ—é¢„æµ‹ç¼ºä¹å¤§è§„æ¨¡å…¬å¼€æ•°æ®é›†ã€‚  
å½“å‰ç»å¤§å¤šæ•°çš„æ—¶é—´åºåˆ—é¢„è®­ç»ƒæ¨¡åž‹ä¹Ÿä¸è¿‡æ˜¯åœ¨è¯¸å¦‚ [UCR & UEA](https://www.timeseriesclassification.com/) è¿™æ ·çš„å°‘é‡æ ·æœ¬ä¸Šè®­ç»ƒçš„ã€‚  
å³ä½¿è¿™äº›åŸºå‡†æ•°æ®é›†ä¸ºæ—¶é—´åºåˆ—é¢„æµ‹çš„å‘å±•è¿›æ­¥æä¾›äº†åŸºçŸ³ï¼Œå…¶è¾ƒå°çš„è§„æ¨¡å’Œæ³›åŒ–æ€§çš„ç¼ºå¤±ä½¿å¾—å¤§è§„æ¨¡é¢„è®­ç»ƒä»ç„¶é¢ä¸´è¯¸å¤šå›°éš¾ã€‚

æ‰€ä»¥å¯¹äºŽæ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸæ¥è®²ï¼Œå‘å±•å¤§è§„æ¨¡ã€å¼ºæ³›åŒ–æ€§çš„æ•°æ®é›† (å°±åƒ CV é¢†åŸŸçš„ ImageNet ä¸€æ ·) æ˜¯å½“å‰æœ€é‡è¦çš„äº‹æƒ…ã€‚è¿™å°†ä¼šæžå¤§åœ°ä¿ƒè¿›æ—¶é—´åºåˆ—åˆ†æžé¢†åŸŸä¸Žè®­ç»ƒæ¨¡åž‹çš„å‘å±•ç ”ç©¶ï¼Œæå‡ä¸Žè®­ç»ƒæ¨¡åž‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹æ–¹é¢çš„èƒ½åŠ›ã€‚

å£°æ˜Ž
--

æˆ‘ä»¬è¯šæŒšæ„Ÿè°¢ [Lysandre Debut](https://github.com/LysandreJik) å’Œ [Pedro Cuenca](https://github.com/pcuenca) æä¾›çš„æ·±åˆ»è§è§£å’Œå¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ã€‚ â¤ï¸

* * *

> è‹±æ–‡åŽŸæ–‡:[https://hf.co/blog/autoformer](https://hf.co/blog/autoformer)
> 
> ä½œè€…: Eli Simhayev, Kashif Rasul, Niels Rogge
> 
> è¯‘è€…: Hoi2022
> 
> å®¡æ ¡/æŽ’ç‰ˆ: zhongdongy (é˜¿ä¸œ)