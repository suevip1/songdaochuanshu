---
layout: post
title: "论文解读（SentiX）《SentiX: A Sentiment-Aware Pre-Trained Model for Cross-Domain Sentiment Analysis》"
date: "2023-08-15T00:55:33.894Z"
---
论文解读（SentiX）《SentiX: A Sentiment-Aware Pre-Trained Model for Cross-Domain Sentiment Analysis》
=============================================================================================

Note：\[ wechat：Y466551 | 可加勿骚扰，付费咨询 \]

论文信息
====

> 论文标题：SentiX: A Sentiment-Aware Pre-Trained Model for Cross-Domain Sentiment Analysis  
> 论文作者：Jie Zhou, Junfeng Tian, Rui Wang, Yuanbin Wu, Wenming Xiao, Liang He  
> 论文来源：  
> 论文地址：[download](https://www.semanticscholar.org/paper/SentiX%3A-A-Sentiment-Aware-Pre-Trained-Model-for-Zhou-Tian/092442a694b811dff5b7715fba9e363e0ce4108c)   
> 论文代码：[download](https://github.com/12190143/SentiX)  
> 视屏讲解：click

1 介绍 
=====

　　**出发点**：预先训练好的语言模型已被广泛应用于跨领域的 NLP 任务，如情绪分析，实现了最先进的性能。然而，由于用户在不同域间的情绪表达的多样性，在源域上对预先训练好的模型进行微调往往会过拟合，导致在目标域上的结果较差；

　　思路：通过大规模 review 数据集的领域不变情绪知识对情感软件语言模型（SENTIX）进行预训练，并将其用于跨领域情绪分析任务，而无需进行微调。本文提出了一些基于现有的标记和句子级别的词汇和注释的训练前任务，如表情符号、情感词汇和评级，而不受人为干扰。进行了一系列的实验，结果表明，该模型具有巨大的优势。

　　预训练模型在跨域情感分析上存在的问题：

*   *   现有的预训练模型侧重于通过自监督策略学习语义内容，而忽略了预训练短语的情绪特定知识；
    *   在微调阶段，预训练好的模型可能会通过学习过多的特定领域的情绪知识而过拟合源域，从而导致目标域的性能下降；

　　贡献：

*   *   提出了 SENTIX 用于跨域情绪分类，以在大规模未标记的多域数据中学习丰富的域不变情绪知识；
    *   在标记水平和句子水平上设计了几个预训练目标，通过掩蔽和预测来学习这种领域不变的情绪知识；
    *   实验表明，SENTIX 获得了最先进的跨领域情绪分析的性能，并且比 BERT 需要更少的注释数据才能达到等效的性能；

2 方法
====

2.1 模型框架
--------

　　![](https://img2023.cnblogs.com/blog/1664108/202308/1664108-20230815000743660-1655155076.png)

2.2 Sentiment Masking
---------------------

　　评论包含了许多半监督的情绪信号，如 情绪词汇、表情符号 和 评级，而大规模的评论数据可以从像  Yelp 这样的在线评论网站上获得。

*   *   情绪词汇（Sentiment Words）：积极（P），消极（N），其他（0）；
    *   情感符（Emoticons）：经常用于表示用户情感的特殊符号，如（“)”、“(”、“:”、“D”），本文选择语料库中经常出现的 100 个特殊符号作为情感符，并将其标记为 “E”，其他为 “0”；
    *   评分（Rating）：情绪评分分为 5 个等级；

　　策略：

*   Sentiment Word Masking (SWM)：为丰富情绪信息，用 30% 的比率掩盖了情绪词；
*   Emoticon Masking (EM)：由于一个句子中的表情符号数量相对较少，并且删除表情符号不会影响句子的语义信息，所以为每个句子屏蔽了 50% 的表情符号；
*   General Word Masking (GWM)：如果只关注情感词和表情符号，模型可能会失去其他单词的一般语义信息。因此，使用 \[MASK\] 并用 15% 的比率替换句子中的一般单词来学习语义信息；

2.3 Pre-training Objectives 
----------------------------

**Sentiment-aware Word Prediction (SWP)** 

　　将损坏的句子 $\\hat{x}$ 输入编码器，获得单词表示 $h\_{i}$ 和句子表示 $h\_{\[C L S\]}$，然后计算单词概率 $P\\left(x\_{i} \\mid \\hat{x}\_{i}\\right)=\\operatorname{Softmax}\\left(W\_{w} \\cdot h\_{i}+b\_{w}\\right)$。损失函数 $L\_{w}$ 是预测概率与真词标签之间的交叉熵：

　　$\\mathcal{L}\_{w}=-\\frac{1}{|\\hat{\\mathcal{X}}|} \\sum\_{\\hat{x} \\in \\hat{\\mathcal{X}}} \\frac{1}{|\\hat{x}|} \\sum\_{i=1}^{|\\hat{x}|} \\log \\left(P\\left(\\left|x\_{i}\\right| \\hat{x}\_{i}\\right)\\right)$

**Word Sentiment Prediction (WSP)**

　　根据情感知识，把词的情绪分为积极的、消极的和其他的。因此，设计了 WSP 来学习标记的情感知识。我们的目的是推断单词 $w\_{i}$ 的情绪极性 $s\_{i}$ 根据 $h\_{i}$，$P\\left(s\_{i} \\mid \\hat{x\_{i}}\\right)= \\operatorname{Softmax}\\left(W\_{s} \\cdot h\_{i}+b\_{s}\\right) $。这里使用交叉熵损失：

　　　　$\\mathcal{L}\_{s}=-\\frac{1}{|\\hat{\\mathcal{X}}|} \\sum\_{\\hat{x} \\in \\hat{\\mathcal{X}}} \\frac{1}{|\\hat{x}|} \\sum\_{i=1}^{|\\hat{x}|} \\log \\left(P\\left(s\_{i} \\mid \\hat{x}\_{i}\\right)\\right)$

**Rating Prediction (RP)**

　　以上任务侧重于学习 Token 水平的情感知识。评级代表了句子级评论的情绪得分。推断评级将带来句子水平的情感知识。与BERT类似，使用最终状态 $h\_{\[\\mathrm{CLS}\]}$ 作为句子表示。该评级由 $P(r \\mid \\hat{x})=\\operatorname{Softmax}\\left(W\_{r} \\cdot h\_{\[C L S\]}+b\_{r}\\right)$ 进行预测，并根据预测的评级分布计算损失：

　　　　$\\mathcal{L}\_{r}=-\\frac{1}{|\\hat{\\mathcal{X}}|} \\sum\_{\\hat{x} \\in \\hat{\\mathcal{X}}} \\log (P(r \\mid \\hat{x}))$

2.4 Joint Training
------------------

　　最后，我们共同优化了标记级目标 $\\mathcal{L}\_{T}$ 和句子级目标 $\\mathcal{L}\_{S}$。总损失为

　　　　$\\mathcal{L}=\\mathcal{L}\_{T}+\\mathcal{L}\_{S}$

　　其中：

　　　　$\\mathcal{L}\_{T}=\\mathcal{L}\_{w}+\\mathcal{L}\_{s}+\\mathcal{L}\_{e} $

　　　　$\\mathcal{L}\_{S}=\\mathcal{L}\_{r}$

3 实验
====

　　![](https://img2023.cnblogs.com/blog/1664108/202308/1664108-20230815020805696-1991415.png)

因上求缘，果上努力~~~~ 作者：[Wechat~Y466551](https://www.cnblogs.com/BlairGrowing/)，转载请注明原文链接：[https://www.cnblogs.com/BlairGrowing/p/17629248.html](https://www.cnblogs.com/BlairGrowing/p/17629248.html)