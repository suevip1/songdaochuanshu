---
layout: post
title: "Llama 2 æ¥è¢­ - åœ¨ Hugging Face ä¸ŠçŽ©è½¬å®ƒ"
date: "2023-07-26T01:12:05.588Z"
---
Llama 2 æ¥è¢­ - åœ¨ Hugging Face ä¸ŠçŽ©è½¬å®ƒ
================================

å¼•è¨€
--

ä»Šå¤©ï¼ŒMeta å‘å¸ƒäº† Llama 2ï¼Œå…¶åŒ…å«äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„å¼€æ”¾å¤§è¯­è¨€æ¨¡åž‹ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´èƒ½å¤Ÿå°†å…¶å…¨é¢é›†æˆå…¥ Hugging Faceï¼Œå¹¶å…¨åŠ›æ”¯æŒå…¶å‘å¸ƒã€‚ Llama 2 çš„ç¤¾åŒºè®¸å¯è¯ç›¸å½“å®½æ¾ï¼Œä¸”å¯å•†ç”¨ã€‚å…¶ä»£ç ã€é¢„è®­ç»ƒæ¨¡åž‹å’Œå¾®è°ƒæ¨¡åž‹å‡äºŽä»Šå¤©å‘å¸ƒäº†ðŸ”¥ã€‚

é€šè¿‡ä¸Ž Meta åˆä½œï¼Œæˆ‘ä»¬å·²ç»é¡ºåˆ©åœ°å®Œæˆäº†å¯¹ Llama 2 çš„é›†æˆï¼Œä½ å¯ä»¥åœ¨ Hub ä¸Šæ‰¾åˆ° 12 ä¸ªå¼€æ”¾æ¨¡åž‹ (3 ä¸ªåŸºç¡€æ¨¡åž‹ä»¥åŠ 3 ä¸ªå¾®è°ƒæ¨¡åž‹ï¼Œæ¯ä¸ªæ¨¡åž‹éƒ½æœ‰ 2 ç§ checkpoint: ä¸€ä¸ªæ˜¯ Meta çš„åŽŸå§‹ checkpointï¼Œä¸€ä¸ªæ˜¯ `transformers` æ ¼å¼çš„ checkpoint)ã€‚ä»¥ä¸‹åˆ—å‡ºäº† Hugging Face æ”¯æŒ Llama 2 çš„ä¸»è¦å·¥ä½œ:

*   [Llama 2 å·²å…¥é©» Hub](https://huggingface.co/meta-llama): åŒ…æ‹¬æ¨¡åž‹å¡åŠç›¸åº”çš„è®¸å¯è¯ã€‚
*   [æ”¯æŒ Llama 2 çš„ transformers åº“](https://github.com/huggingface/transformers/releases/tag/v4.31.0)
*   ä½¿ç”¨å• GPU å¾®è°ƒ Llama 2 å°æ¨¡åž‹çš„ç¤ºä¾‹
*   [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) å·²é›†æˆ Llama 2ï¼Œä»¥å®žçŽ°å¿«é€Ÿé«˜æ•ˆçš„ç”Ÿäº§åŒ–æŽ¨ç†
*   æŽ¨ç†ç»ˆç«¯ (Inference Endpoints) å·²é›†æˆ Llama 2

ç›®å½•
--

*   [ä½•ä»¥ Llama 2?](#%E4%BD%95%E4%BB%A5-llama-2)
*   [æ¼”ç¤º](#%E6%BC%94%E7%A4%BA)
*   [æŽ¨ç†](#%E6%8E%A8%E7%90%86)
    *   [ç”¨ transformers](#%E7%94%A8-transformers)
    *   [ç”¨ TGI å’ŒæŽ¨ç†ç»ˆç«¯](#%E7%94%A8-TGI-%E5%92%8C%E6%8E%A8%E7%90%86%E7%BB%88%E7%AB%AF)
*   [ç”¨ -PEFT- å¾®è°ƒ](#%E7%94%A8-PEFT-%E5%BE%AE%E8%B0%83)
*   [å…¶ä»–èµ„æº](#%E5%85%B6%E4%BB%96%E8%B5%84%E6%BA%90)
*   [æ€»ç»“](#%E6%80%BB%E7%BB%93)

ä½•ä»¥ Llama 2?
-----------

Llama 2 å¼•å…¥äº†ä¸€ç³»åˆ—é¢„è®­ç»ƒå’Œå¾®è°ƒ LLMï¼Œå‚æ•°é‡èŒƒå›´ä»Ž 7B åˆ° 70B (7Bã€13Bã€70B)ã€‚å…¶é¢„è®­ç»ƒæ¨¡åž‹æ¯” Llama 1 æ¨¡åž‹æœ‰äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®çš„æ€»è¯å…ƒæ•°å¢žåŠ äº† 40%ã€ä¸Šä¸‹æ–‡é•¿åº¦æ›´é•¿ (4k è¯å…ƒðŸ¤¯)ï¼Œä»¥åŠåˆ©ç”¨äº†åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶æ¥åŠ é€Ÿ 70B æ¨¡åž‹çš„æŽ¨ç†ðŸ”¥ï¼

ä½†æœ€ä»¤äººå…´å¥‹çš„è¿˜æ˜¯å…¶å‘å¸ƒçš„å¾®è°ƒæ¨¡åž‹ (Llama 2-Chat)ï¼Œè¯¥æ¨¡åž‹å·²ä½¿ç”¨ [åŸºäºŽäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning from Human Feedbackï¼ŒRLHF)](https://huggingface.co/blog/rlhf) æŠ€æœ¯é’ˆå¯¹å¯¹è¯åœºæ™¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚åœ¨ç›¸å½“å¹¿æ³›çš„æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§æµ‹è¯•åŸºå‡†ä¸­ï¼ŒLlama 2-Chat æ¨¡åž‹çš„è¡¨çŽ°ä¼˜äºŽå¤§å¤šæ•°å¼€æ”¾æ¨¡åž‹ï¼Œä¸”å…¶åœ¨äººç±»è¯„ä¼°ä¸­è¡¨çŽ°å‡ºä¸Ž ChatGPT ç›¸å½“çš„æ€§èƒ½ã€‚æ›´å¤šè¯¦æƒ…ï¼Œå¯å‚é˜…å…¶ [è®ºæ–‡](https://huggingface.co/papers/2307.09288)ã€‚

![æ¨¡åž‹è®­ç»ƒä¸Žå¾®è°ƒå·¥ä½œæµ](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/llama-rlhf.png)

_å›¾æ¥è‡ª [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://scontent-fra3-2.xx.fbcdn.net/v/t39.2365-6/10000000_6495670187160042_4742060979571156424_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=GK8Rh1tm_4IAX8b5yo4&_nc_ht=scontent-fra3-2.xx&oh=00_AfDtg_PRrV6tpy9UmiikeMRuQgk6Rej7bCPOkXZQVmUKAg&oe=64BBD830) ä¸€æ–‡_

å¦‚æžœä½ ä¸€ç›´åœ¨ç­‰ä¸€ä¸ªé—­æºèŠå¤©æœºå™¨äººçš„å¼€æºæ›¿ä»£ï¼Œé‚£ä½ ç®—æ˜¯ç­‰ç€äº†ï¼Llama 2-Chat å°†æ˜¯ä½ çš„æœ€ä½³é€‰æ‹©ï¼

æ¨¡åž‹

è®¸å¯è¯

å¯å¦å•†ç”¨?

é¢„è®­ç»ƒè¯å…ƒæ•°

æŽ’è¡Œæ¦œå¾—åˆ†

[Falcon-7B](https://huggingface.co/tiiuae/falcon-7b)

Apache 2.0

âœ…

1,500B

47.01

[MPT-7B](https://huggingface.co/mosaicml/mpt-7b)

Apache 2.0

âœ…

1,000B

48.7

Llama-7B

Llama è®¸å¯è¯

âŒ

1,000B

49.71

[Llama-2-7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)

Llama 2 è®¸å¯è¯

âœ…

2,000B

54.32

Llama-33B

Llama è®¸å¯è¯

âŒ

1,500B

\*

[Llama-2-13B](https://huggingface.co/meta-llama/Llama-2-13b-hf)

Llama 2 è®¸å¯è¯

âœ…

2,000B

58.67

[mpt-30B](https://huggingface.co/mosaicml/mpt-30b)

Apache 2.0

âœ…

1,000B

55.7

[Falcon-40B](https://huggingface.co/tiiuae/falcon-40b)

Apache 2.0

âœ…

1,000B

61.5

Llama-65B

Llama è®¸å¯è¯

âŒ

1,500B

62.1

[Llama-2-70B](https://huggingface.co/meta-llama/Llama-2-70b-hf)

Llama 2 è®¸å¯è¯

âœ…

2,000B

\*

[Llama-2-70B-chat](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)\*

Llama 2 è®¸å¯è¯

âœ…

2,000B

66.8

\*ç›®å‰ï¼Œæˆ‘ä»¬æ­£åœ¨å¯¹ Llama 2 70B (éžèŠå¤©ç‰ˆ) è¿›è¡Œè¯„æµ‹ã€‚è¯„æµ‹ç»“æžœåŽç»­å°†æ›´æ–°è‡³æ­¤è¡¨ã€‚

æ¼”ç¤º
--

ä½ å¯ä»¥é€šè¿‡ [è¿™ä¸ªç©ºé—´](https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI) æˆ–ä¸‹é¢çš„åº”ç”¨è½»æ¾è¯•ç”¨ Llama 2 å¤§æ¨¡åž‹ (700 äº¿å‚æ•°ï¼):

![](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202307250818653.png)

å®ƒä»¬èƒŒåŽéƒ½æ˜¯åŸºäºŽ Hugging Face çš„ [TGI](https://github.com/huggingface/text-generation-inference) æ¡†æž¶ï¼Œè¯¥æ¡†æž¶ä¹Ÿæ”¯æ’‘äº† [HuggingChat](https://huggingface.co/chat/)ï¼Œæˆ‘ä»¬ä¼šåœ¨ä¸‹æ–‡åˆ†äº«æ›´å¤šç›¸å…³å†…å®¹ã€‚

æŽ¨ç†
--

æœ¬èŠ‚ï¼Œæˆ‘ä»¬ä¸»è¦ä»‹ç»å¯ç”¨äºŽå¯¹ Llama 2 æ¨¡åž‹è¿›è¡ŒæŽ¨ç†çš„ä¸¤ç§ä¸åŒæ–¹æ³•ã€‚åœ¨ä½¿ç”¨è¿™äº›æ¨¡åž‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²åœ¨ [Meta Llama 2](https://huggingface.co/meta-llama) å­˜å‚¨åº“é¡µé¢ç”³è¯·äº†æ¨¡åž‹è®¿é—®æƒé™ã€‚

\*\*æ³¨æ„: è¯·åŠ¡å¿…æŒ‰ç…§é¡µé¢ä¸Šçš„æŒ‡ç¤ºå¡«å†™ Meta å®˜æ–¹è¡¨æ ¼ã€‚å¡«å®Œä¸¤ä¸ªè¡¨æ ¼æ•°å°æ—¶åŽï¼Œç”¨æˆ·å°±å¯ä»¥è®¿é—®æ¨¡åž‹å­˜å‚¨åº“ã€‚

### ä½¿ç”¨ transformers

ä»Ž transformers [4.31](https://github.com/huggingface/transformers/releases/tag/v4.31.0) ç‰ˆæœ¬å¼€å§‹ï¼ŒHF ç”Ÿæ€ä¸­çš„æ‰€æœ‰å·¥å…·å’Œæœºåˆ¶éƒ½å¯ä»¥é€‚ç”¨äºŽ Llama 2ï¼Œå¦‚:

*   è®­ç»ƒã€æŽ¨ç†è„šæœ¬åŠå…¶ç¤ºä¾‹
*   å®‰å…¨æ–‡ä»¶æ ¼å¼ (`safetensors` )
*   ä¸Ž bitsandbytes (4 æ¯”ç‰¹é‡åŒ–) å’Œ PEFT ç­‰å·¥å…·
*   å¸®åŠ©æ¨¡åž‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆçš„è¾…åŠ©å·¥å…·
*   å¯¼å‡ºæ¨¡åž‹ä»¥è¿›è¡Œéƒ¨ç½²çš„æœºåˆ¶

ä½ åªéœ€ç¡®ä¿ä½¿ç”¨æœ€æ–°çš„ `transformers` ç‰ˆæœ¬å¹¶ç™»å½•ä½ çš„ Hugging Face å¸æˆ·ã€‚

    pip install transformers
    huggingface-cli login
    

ä¸‹é¢æ˜¯å¦‚ä½•ä½¿ç”¨ `transformers` è¿›è¡ŒæŽ¨ç†çš„ä»£ç ç‰‡æ®µ:

    from transformers import AutoTokenizer
    import transformers
    import torch
    
    model = "meta-llama/Llama-2-7b-chat-hf"
    
    tokenizer = AutoTokenizer.from_pretrained(model)
    pipeline = transformers.pipeline(
        "text-generation",
        model=model,
        torch_dtype=torch.float16,
        device_map="auto",
    )
    
    sequences = pipeline(
        'I liked "Breaking Bad" and "Band of Brothers". Do you have any recommendations of other shows I might like?\n',
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        max_length=200,
    )
    for seq in sequences:
        print(f"Result: {seq['generated_text']}")
    

    Result: I liked "Breaking Bad" and "Band of Brothers". Do you have any recommendations of other shows I might like?
    Answer:
    Of course! If you enjoyed "Breaking Bad" and "Band of Brothers," here are some other TV shows you might enjoy:
    1. "The Sopranos" - This HBO series is a crime drama that explores the life of a New Jersey mob boss, Tony Soprano, as he navigates the criminal underworld and deals with personal and family issues.
    2. "The Wire" - This HBO series is a gritty and realistic portrayal of the drug trade in Baltimore, exploring the impact of drugs on individuals, communities, and the criminal justice system.
    3. "Mad Men" - Set in the 1960s, this AMC series follows the lives of advertising executives on Madison Avenue, expl
    

å¦å¤–ï¼Œå°½ç®¡æ¨¡åž‹æœ¬èº«çš„ä¸Šä¸‹æ–‡é•¿åº¦ _ä»…_ 4k è¯å…ƒï¼Œä½†ä½ å¯ä»¥ä½¿ç”¨ `transformers` æ”¯æŒçš„æŠ€æœ¯ï¼Œå¦‚æ—‹è½¬ä½ç½®åµŒå…¥ç¼©æ”¾ (rotary position embedding scaling) ([æŽ¨ç‰¹](https://twitter.com/joao_gante/status/1679775399172251648))ï¼Œè¿›ä¸€æ­¥æŠŠå®ƒå˜é•¿ï¼

### ä½¿ç”¨ TGI å’ŒæŽ¨ç†ç»ˆç«¯

**[Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference)** æ˜¯ Hugging Face å¼€å‘çš„ç”Ÿäº§çº§æŽ¨ç†å®¹å™¨ï¼Œå¯ç”¨äºŽè½»æ¾éƒ¨ç½²å¤§è¯­è¨€æ¨¡åž‹ã€‚å®ƒæ”¯æŒæµå¼ç»„æ‰¹ã€æµå¼è¾“å‡ºã€åŸºäºŽå¼ é‡å¹¶è¡Œçš„å¤š GPU å¿«é€ŸæŽ¨ç†ï¼Œå¹¶æ”¯æŒç”Ÿäº§çº§çš„æ—¥å¿—è®°å½•å’Œè·Ÿè¸ªç­‰åŠŸèƒ½ã€‚

ä½ å¯ä»¥åœ¨è‡ªå·±çš„åŸºç¡€è®¾æ–½ä¸Šéƒ¨ç½²å¹¶å°è¯• TGIï¼Œä¹Ÿå¯ä»¥ç›´æŽ¥ä½¿ç”¨ Hugging Face çš„ [**æŽ¨ç†ç»ˆç«¯**](https://huggingface.co/inference-endpoints)ã€‚å¦‚æžœè¦ç”¨æŽ¨ç†ç»ˆç«¯éƒ¨ç½² Llama 2 æ¨¡åž‹ï¼Œè¯·ç™»é™† **[æ¨¡åž‹é¡µé¢](https://huggingface.co/meta-llama/Llama-2-7b-hf)** å¹¶å•å‡» **[Deploy -> Inference Endpoints](https://ui.endpoints.huggingface.co/new?repository=meta-llama/Llama-2-7b-hf)** èœå•ã€‚

*   è¦æŽ¨ç† 7B æ¨¡åž‹ï¼Œæˆ‘ä»¬å»ºè®®ä½ é€‰æ‹© â€œGPU \[medium\] - 1x Nvidia A10Gâ€ã€‚
*   è¦æŽ¨ç† 13B æ¨¡åž‹ï¼Œæˆ‘ä»¬å»ºè®®ä½ é€‰æ‹© â€œGPU \[xlarge\] - 1x Nvidia A100â€ã€‚
*   è¦æŽ¨ç† 70B æ¨¡åž‹ï¼Œæˆ‘ä»¬å»ºè®®ä½ é€‰æ‹© â€œGPU \[xxxlarge\] - 8x Nvidia A100â€ã€‚

_æ³¨æ„: å¦‚æžœä½ é…é¢ä¸å¤Ÿï¼Œè¯·å‘é€é‚®ä»¶è‡³ **[api-enterprise@huggingface.co](mailto:api-enterprise@huggingface.co)** ç”³è¯·å‡çº§é…é¢ï¼Œé€šè¿‡åŽä½ å°±å¯ä»¥è®¿é—® A100 äº†ã€‚_

ä½ è¿˜å¯ä»¥ä»Žæˆ‘ä»¬çš„å¦ä¸€ç¯‡åšæ–‡ä¸­äº†è§£æ›´å¤šæœ‰å…³ [å¦‚ä½•ä½¿ç”¨ Hugging Face æŽ¨ç†ç»ˆç«¯éƒ¨ç½² LLM](https://huggingface.co/blog/zh/inference-endpoints-llm) çš„çŸ¥è¯† , æ–‡ä¸­åŒ…å«äº†æŽ¨ç†ç»ˆç«¯æ”¯æŒçš„è¶…å‚ä»¥åŠå¦‚ä½•ä½¿ç”¨å…¶ Python å’Œ Javascript API å®žçŽ°æµå¼è¾“å‡ºç­‰ä¿¡æ¯ã€‚

ç”¨ PEFT å¾®è°ƒ
---------

è®­ç»ƒ LLM åœ¨æŠ€æœ¯å’Œè®¡ç®—ä¸Šéƒ½æœ‰ä¸€å®šçš„æŒ‘æˆ˜ã€‚æœ¬èŠ‚ï¼Œæˆ‘ä»¬å°†ä»‹ç» Hugging Face ç”Ÿæ€ä¸­æœ‰å“ªäº›å·¥å…·å¯ä»¥å¸®åŠ©å¼€å‘è€…åœ¨ç®€å•çš„ç¡¬ä»¶ä¸Šé«˜æ•ˆè®­ç»ƒ Llama 2ï¼Œæˆ‘ä»¬è¿˜å°†å±•ç¤ºå¦‚ä½•åœ¨å•å¼  NVIDIA T4 (16GB - Google Colab) ä¸Šå¾®è°ƒ Llama 2 7B æ¨¡åž‹ã€‚ä½ å¯ä»¥é€šè¿‡ [è®© LLM æ›´å¯å¾—](https://huggingface.co/blog/4bit-transformers-bitsandbytes) è¿™ç¯‡åšæ–‡äº†è§£æ›´å¤šä¿¡æ¯ã€‚

æˆ‘ä»¬æž„å»ºäº†ä¸€ä¸ª [è„šæœ¬](https://github.com/lvwerra/trl/blob/main/examples/scripts/sft_trainer.py)ï¼Œå…¶ä¸­ä½¿ç”¨äº† QLoRA å’Œ [`trl`](https://github.com/lvwerra/trl) ä¸­çš„ `SFTTrainer` æ¥å¯¹ Llama 2 è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚

ä¸‹é¢çš„å‘½ä»¤ç»™å‡ºäº†åœ¨ `timdettmers/openassistant-guanaco` æ•°æ®é›†ä¸Šå¾®è°ƒ Llama 2 7B çš„ä¸€ä¸ªç¤ºä¾‹ã€‚è¯¥è„šæœ¬å¯ä»¥é€šè¿‡ `merge_and_push` å‚æ•°å°† LoRA æƒé‡åˆå¹¶åˆ°æ¨¡åž‹æƒé‡ä¸­ï¼Œå¹¶å°†å…¶ä¿å­˜ä¸º `safetensor` æ ¼å¼ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±èƒ½ä½¿ç”¨ TGI å’ŒæŽ¨ç†ç»ˆç«¯éƒ¨ç½²å¾®è°ƒåŽçš„æ¨¡åž‹ã€‚

é¦–å…ˆå®‰è£… `trl` åŒ…å¹¶ä¸‹è½½è„šæœ¬:

    pip install trl
    git clone https://github.com/lvwerra/trl
    

ç„¶åŽï¼Œä½ å°±å¯ä»¥è¿è¡Œè„šæœ¬äº†:

    python trl/examples/scripts/sft_trainer.py \
        --model_name meta-llama/Llama-2-7b-hf \
        --dataset_name timdettmers/openassistant-guanaco \
        --load_in_4bit \
        --use_peft \
        --batch_size 4 \
        --gradient_accumulation_steps 2
    

å…¶ä»–èµ„æº
----

*   [è®ºæ–‡](https://huggingface.co/papers/2307.09288)
*   [Hub ä¸Šçš„æ¨¡åž‹](https://huggingface.co/meta-llama)
*   [Open LLM æŽ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
*   [Meta æä¾›çš„ Llama 2 æ¨¡åž‹ä½¿ç”¨å¤§å…¨](https://github.com/facebookresearch/llama-recipes/tree/main)

æ€»ç»“
--

Llama 2 çš„æŽ¨å‡ºè®©æˆ‘ä»¬éžå¸¸å…´å¥‹ï¼åŽé¢æˆ‘ä»¬ä¼šå›´ç»•å®ƒé™†é™†ç»­ç»­æŽ¨å‡ºæ›´å¤šå†…å®¹ï¼ŒåŒ…æ‹¬å¦‚ä½•å¾®è°ƒä¸€ä¸ªè‡ªå·±çš„æ¨¡åž‹ï¼Œå¦‚ä½•åœ¨è®¾å¤‡ä¾§è¿è¡Œ Llama 2 å°æ¨¡åž‹ç­‰ï¼Œæ•¬è¯·æœŸå¾…ï¼

* * *

> è‹±æ–‡åŽŸæ–‡: [https://huggingface.co/blog/llama2](https://huggingface.co/blog/llama2)
> 
> åŽŸæ–‡ä½œè€…: Philipp Schmidï¼ŒOmar Sansevieroï¼ŒPedro Cuencaï¼ŒLewis Tunstall
> 
> è¯‘è€…: Matrix Yao (å§šä¼Ÿå³°)ï¼Œè‹±ç‰¹å°”æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œå·¥ä½œæ–¹å‘ä¸º transformer-family æ¨¡åž‹åœ¨å„æ¨¡æ€æ•°æ®ä¸Šçš„åº”ç”¨åŠå¤§è§„æ¨¡æ¨¡åž‹çš„è®­ç»ƒæŽ¨ç†ã€‚
> 
> å®¡æ ¡/æŽ’ç‰ˆ: zhongdongy (é˜¿ä¸œ)