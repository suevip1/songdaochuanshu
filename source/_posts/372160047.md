---
layout: post
title: "深度学习（十）——神经网络：非线性激活"
date: "2023-08-06T01:01:48.989Z"
---
深度学习（十）——神经网络：非线性激活
===================

![深度学习（十）——神经网络：非线性激活](https://img2023.cnblogs.com/blog/2744125/202308/2744125-20230805181619905-1842125642.png) 主要介绍了ReLU和Sigmiod两种非线性激活函数，以及在神经网络中进行非线性转换的目的。

一、Padding Layers简介
==================

*   **nn.ZeroPad2d**：在输入的tensor数据类型周围**用0进行填充**
    
*   **nn.ConstantPad2d**：在输入的tensor数据类型周围**用常数进行填充**
    

这个函数的主要作用是对输入的图像进行填充，但里面所有功能都能用nn.Conv2d实现。

二、Non-linear Activations
========================

非线性激活主要作用是为神经网络引入一些**非线性特质**

1\. nn.ReLU介绍
-------------

    class torch.nn.ReLU(inplace=False)
    

作用：

*   \\(input\\leq{0}\\)，\\(output=0\\)
    
*   \\(input>0\\)，\\(output=input\\)
    

计算公式：

\\\[ReLU(x)=(x)^+=max(0,x) \\\]

inplace参数：

*   inplace=True，则会自动**替换输入时的变量参数**。如：input=-1，ReLU(input,implace=True)，那么输出后，_input=output=0_
    
*   inplace=True，则**不替换输入时的变量参数**。如：input=-1，ReLU(input,implace=True)，那么输出后，_input=-1，output=0_
    

2\. nn.Sigmoid介绍
----------------

    class torch.nn.Sigmoid(*args, **kwargs)
    

计算公式：

\\\[Sigmiod(x)=\\sigma(x)=\\frac{1}{1+exp(-x)} \\\]

三、代码栗子
======

1\. nn.ReLU函数
-------------

    import torch
    import torchvision
    from torch import nn
    from torch.nn import ReLU,Sigmoid
    from torch.utils.data import DataLoader
    from torch.utils.tensorboard import SummaryWriter
    
    input=torch.tensor([[1,-0.5],
                        [-1,3]])
    output=torch.reshape(input,(-1,1,2,2))
    
    #构建神经网络
    class Demo(nn.Module):
        def __init__(self):
            super(Demo,self).__init__()
            self.relu1=ReLU()
    
        def forward(self,input):
            output=self.relu1(input)
            return output
    
    demo=Demo()
    output=demo(input)
    print(output)
    
    """
    [Run]
    tensor([[1., 0.],
            [0., 3.]])
    """
    

2\. nn.Sigmoid函数
----------------

    import torch
    import torchvision
    from torch import nn
    from torch.nn import ReLU,Sigmoid
    from torch.utils.data import DataLoader
    from torch.utils.tensorboard import SummaryWriter
    
    dataset=torchvision.datasets.CIFAR10("./dataset",train=False,download=True,transform=torchvision.transforms.ToTensor())
    dataloder=DataLoader(dataset,batch_size=64)
    
    class Demo1(nn.Module):
        def __init__(self):
            super(Demo1,self).__init__()
            self.sigmoid=Sigmoid()
    
        def forward(self,input):
            output=self.sigmoid(input)
            return output
    
    demo1=Demo1()
    writer=SummaryWriter("logs_sigmoid")
    step=0
    for data in dataloder:
        imgs,targets=data
        writer.add_images("input",imgs,global_step=step)
        output=demo1(imgs)
        writer.add_images("output",output,global_step=step)
        step+=1
    writer.close()
    

输出结果：  
![](https://img2023.cnblogs.com/blog/2744125/202308/2744125-20230805181319430-615847965.png)

3\. 非线性变换的目的
------------

*   非线性变换的目的是为神经网络**引入一些非线性特征**，使其训练出一些符合各种曲线或各种特征的模型。
    
*   换句话来说，如果模型都是直线特征的话，它的**泛化能力会不够好**。