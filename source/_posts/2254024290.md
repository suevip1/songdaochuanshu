---
layout: post
title: "基于词袋（Bag of Words）和SVM的图片分类"
date: "2023-07-05T01:24:21.992Z"
---
基于词袋（Bag of Words）和SVM的图片分类
===========================

目录

*   [摘要](#摘要)
*   [源码及完整报告：](#源码及完整报告)
*   [词袋（Bag of Words, BoW）](#词袋bag-of-words-bow)
*   [基于词袋模型的图片分类基本流程](#基于词袋模型的图片分类基本流程)
    *   [多尺度空间极值点检测](#多尺度空间极值点检测)
    *   [关键点精确定位](#关键点精确定位)
    *   [关键点主方向计算](#关键点主方向计算)
    *   [生成描述子](#生成描述子)
*   [特征词典的生成](#特征词典的生成)
    *   [SVM分类器](#svm分类器)
*   [实验结果](#实验结果)
*   [总结](#总结)
*   [参考文献](#参考文献)
*   [源码及完整报告：](#源码及完整报告-1)

摘要
==

虽然现在已经是深度学习的时代了，传统的机器学习方法日渐甚微，不过有些算法还是有着旺盛的生命力，比如SIFT、SVM，在一些深度学习算法中也能看到它们的影子。这些人工设计的经典特征和分类器可以让我们感受到那个时代计算机视觉的魅力。互联网上，需要处理识别的图像越来越多，为此图像分类搜索引擎所需的分类能力要求越来越高，在图像处理领域内也成为越来越热点的课题。对目标进行分类是人类的天生能力，对于计算机，现实世界的呈现方式大多是以图片的方式（视频本质上也是图片）输入，因而图片的分类是实现人工智能的重要部分。词袋模型是基于自然语言模型而提出的一种文档描述方法，被广泛研究用于图像的分析和处理中。本文介绍了一种基于词袋的图片分类方法，首先提取图片的SIFT特征通过聚类得到“视觉词典”，然后统计词频获得突破的词袋描述，最后通过SVM进行有监督的图片分类。

源码及完整报告：
========

算法使用C++ MFC实现，基于OpenCV。  
[https://mbd.pub/o/bread/ZJuTlZ5u](https://mbd.pub/o/bread/ZJuTlZ5u) 或  
[https://www.syjshare.com/res/1TUR49UG](https://www.syjshare.com/res/1TUR49UG)  
![img](https://img2023.cnblogs.com/blog/675649/202307/675649-20230704231134940-1736247512.png)

词袋（Bag of Words, BoW）
=====================

在图片的分类中首先需要解决的是对图像的描述，最初对图像的描述是直接利用图片的颜色信息，但是图像的色彩往往随着光强，物体方向等变化而变化，因此鲁棒性很差。现在的方式是在图片中提取能够表征图片全局或局部的特征作为对图像的描述，从而使得得到的图像描述具有光照不变性、旋转不变性、尺度不变性等特性。然而这种直接基于图像底层基础上的特征不含有语义信息，无法完成对图像生成语义的理解。而基于词袋的图像描述则在一定程度上解决了这一问题。

词袋模型最先是由Josef等基于自然语言处理模型而提出的。在这种模型中，文本（段落或者文档）被看作是无序的词汇集合，忽略语法甚至是单词的顺序。词袋模型的原理非常简单，词袋模型是文本的简化描述模型。在此模型中， 文本\\(x\_{1} ,x\_{2},x\_{3}\\) 被表达成无序的单词组合，不去考虑语法与词序。以文本为例，如果一个文本\\(X\\)表示一连串有顺序的词的排列，那么机器对于\\(X\\)的识别其实就是计算出它在所有文本词语中出现的可能，也就是概率为多个词语概率的相乘如\\(p(X)=p(x\_{1})\*p(x\_{2}|x\_{1})\*p(x\_{3}|x\_{2})...p(x\_{n}|x\_{n-1})\\)，而词袋模型就是这种文本模型的特例，即\\(x\_{k}\\) 出现的概率与前面的文本无关，故有\\(p(X)=p(x\_{1})\*p(x\_{2})\*...p(x\_{n})\\)  成立。

类比一篇文章由很多文字 (textual words) 组合而成，如果將一張图片表示成由许多 视觉单词（visual words） 组合而成，就能将过去在文本检索（text retrieval）领域的技巧直接利用在图像检索（image retrieval）中，以文字检索系统现在的效率，图像表示的“文字化”也有助于大规模(large-scale)图像检索系统的效率。

词袋模型中关键的一步是要生成词典，对图片来说即是要生成“视觉词典”，而这首先需要提取图片的特征形成图片特征描述子集合，然后通过聚类的方式形成视觉单词，从而形成视觉词典。再统计待分类图片的词频，通过分类器即可实现对图片的分类或高级语义的理解。

基于词袋模型的图片分类基本流程
===============

![img](https://img2023.cnblogs.com/blog/675649/202307/675649-20230704231234973-695621449.png)  
如引言所述，基于词袋的模型首先需要要在大量的已知类别的训练图片集中提取图片特征形成多个预选词，然后通过聚类形成视觉词典，然后通过词典统计图片的词频从而形成图片的基于视觉词典的特征描述。将得到图片词袋描述及其类别作为输入训练分类器。分类时，首先提取待分类图片的特征，然后形成图片的词袋描述，将描述输入到训练得到的分类器，即可得到待分类图片的类别  
特征提取是形成词袋的关键一步，也是一个研究的热点。目前比较著名的特征提取方法主要有HoG，SIFT，LBP等方法。SIFT（Scale-Invariant Feature Transform），即尺度不变特征最早于1999年由David Lowe 提出，并在2004年进一步改进。SIFT特征是图像的局部特征，其对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性。SIFT特征提取通过尺度空间极值检测、关键点定位、主方向确定、关键点描述四步最终形成128维的具有多种不变特性的特征点，能够很好的表征图片的局部特征。这些不变性使得SIFT成为目前最火的特征提取算法之一。

尽管有计算复杂度高的不足，SIFT算法仍然是性能最好、应用最广泛的基于局部特征的图像特征提取方法。因而本文在特征提取部分选取SIFT算法作简要介绍。

SIFT特征提取通过尺度空间极值检测、关键点定位、主方向确定、关键点描述四步最终形成128维的具有多种不变特性的特征点。

多尺度空间极值点检测
----------

SIFT要查找的点是一些十分突出的点，这些点不会因光照条件改变，尺度变化等环境变化而消失，比如角点、边缘点、暗区域的亮点以及亮区域的暗点。要保证尺度不变性，首先要保证提取出的特征点不因目标的尺度变化而变化。这个可以通过建立尺度空间来完成。  
尺度空间中各尺度图像的模糊程度逐渐变大，能够模拟人在距离目标由近到远时目标在视网膜上的形成过程。  
高斯核是唯一可以产生多尺度空间的核，一个图像的尺度空间\\(L(x,y,\\delta )\\) 定义为原始图像\\(I(x,y)\\) 与一个尺度可变的2维高斯函\\(G(x,y,\\delta)\\) 的卷积运算。

\\\[L(x,y,\\delta)=G(x,y,\\delta)I(x,y) \\\]

\\\[G(x,y,\\delta)=\\frac{1}{2\\pi\\delta^{2}}exp(-\\frac{x^{2}+y^2}{2\\delta^2} ) \\\]

其中\\((x,y)\\)是尺度空间坐标，\\({\\delta}\\) 代表尺度，决定图像的平滑程度，值越大图像越粗糙（模糊），反之越精细。通过卷积运算，一幅图像可以生成图2所示的高斯金字塔空间，这个空间由几塔（octave）图像组成，一塔图像包括几层（interval）图像。同塔图像大小一致，尺度不同，塔间图片是降采样的关系。  
![img](https://img2023.cnblogs.com/blog/675649/202307/675649-20230704231309049-1027610703.png)

通过上面的操作，得到了塔数为O，塔内层数为S+3的高斯金字塔。LoG(Laplacian of Gaussian) 算子能够很好的找到图像中的兴趣点，但是需要大量的计算。而DoG(Difference of Gaussian)算子与LoG算子由直接关系，且计算量很小，所有可以用来代替LoG来进行兴趣点的查找。DoG：

\\\[D(x,y,\\delta)=(G(x,y,k\\delta)-G(x,y,\\delta))\*I(x,y)=L(x,y,k\\delta)-L(x,y,\\delta) \\\]

通过DoG可以得到尺度空间的差分高斯金字塔  
![img](https://img2023.cnblogs.com/blog/675649/202307/675649-20230704231328661-955413333.jpg)  
差分高斯金字塔  
![img](https://img2023.cnblogs.com/blog/675649/202307/675649-20230704231414306-1044234212.jpg)  
关键点查找

生成了DoG高斯金字塔，就可以在该尺度空间检测极值点。 一个点如果在DOG尺度空间本层以及上下两层的26个领域中是最大或最小值时，就认为该点是图像在该尺度下的一个特征点。如图3。

通过上面的步骤，就可以粗略的检测到图像中的关键点。

关键点精确定位
-------

以上极值点的搜索是在离散空间中进行的，检测到的极值点并不是真正意义上的极值点。同时还要去除低对比度和边界响应这些不稳定的关键点。

通过关键点周围点的信息拟合三维二次函数可以精确确定关键点的位置, DoG函数在图像边缘有较强的边缘响应，而一旦特征落在图形的边缘上，这些点就是不稳定的点，因此还需要排除边缘响应。这一点可以通过Hessian矩阵完成。

关键点主方向计算
--------

上一节中确定了每幅图中的特征点，为每个特征点计算一个方向，依照这个方向做进一步的计算， 利用关键点邻域像素的梯度方向分布特性为每个关键点指定方向参数，使算子具备旋转不变性。

首先在与关键点尺度相应的高斯图像\\(L(x,y,\\delta)\\) 上计算关键点领域窗口内像素点的梯度幅度和方向：

\\\[m(x,y)=\\sqrt{(L(x+1,y)-L(x-1,y))^2+(L(x,y+1)-L(x,y-1))^2} \\\]

\\\[\\theta(x,y)=\\arctan{\\frac{L(x+1,y)-L(x-1,y)}{L(x,y+1)-L(x,y-1)}} \\\]

上式中，\\(m(x,y)\\)和 \\(\\theta(x,y)\\)分别为高斯金字塔\\((x,y)\\)处梯度的大小和方向。

然后用直方图统计领域像素的梯度方向和幅值，梯度直方图的范围是0~360度，其中，每10度一个柱，共36个柱。直方图的主峰值（最大峰值）代表了关键点处邻域梯度的主方向，即关键点的主方向。  
![img](https://img2023.cnblogs.com/blog/675649/202307/675649-20230704231512189-1397499216.jpg)  
![img](https://img2023.cnblogs.com/blog/675649/202307/675649-20230704231530284-636801960.jpg)  
关键点主方向计算

至此，图像的关键点已检测完毕，每个关键点有三个信息：位置、尺度、方向；同时也就使关键点具备平移、缩放、和旋转不变性。

生成描述子
-----

描述的目的是在关键点计算后，用一组向量将这个关键点描述出来，这个描述子不但包括关键点，也包括关键点周围对其有贡献的像素点。用来作为目标匹配的依据，也可使关键点具有更多的不变特性，如光照变化、3D视点变化等。  
![img](https://img2023.cnblogs.com/blog/675649/202307/675649-20230704231703755-755048912.jpg)  
描述子生成图示

首先，将坐标轴旋转到关键点的主方向。只有以主方向为零点方向来描述关键点才能使其具有旋转不变性。  
其次，以关键点为中心取16_16的窗口。然后计算每个4_4的小块上计算8个方向的梯度方向直方图，绘制每个梯度方向的累加值（高斯加权），即可形成一个种子点。每个种子点有8个方向向量信息。  
这样就可以对每个特征点形成一个4_4_8=128维的描述子，每一维都可以表示4\*4个格子中一个的scale/orientation. 将这个向量归一化之后，就进一步去除了光照的影响。  
这个128维的描述向量就可以用于图像匹配等应用中了。

特征词典的生成
=======

上一节得到的SIFT特征点即是预选的“视觉单词”，当提取了若干特征点后，其数量往往巨大，而且特征点是局部特征。但是在庞大的特征数据库中，仅仅几个单一的特征点无法完成图像描述的重大任务。此时，聚类就显得格外重要，它将大量相似的128维特征向量通过一定的规则进行整合，寻找出若干相似的特征点聚为一类作为词典中的一个词，以此来代表一类特征点，方便后期的图像描述，增加描述的可信度，提高描述的速度。  
K-means算法是很典型的基于距离的聚类算法，采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。该算法认为簇是由距离靠近的对象组成的，因此把得到紧凑且独立的簇作为最终目标。其算法过程如下：  
1）从N个文档随机选取K个文档作为质心  
2）对剩余的每个文档测量其到每个质心的距离，并把它归到最近的质心的类  
3）重新计算已经得到的各个类的质心  
4）迭代2～3步直至新的质心与原质心相等或小于指定阈值，算法结束。

经过K-means聚类，图片中提取出的SIFT特征就形成了对应的视觉词典。

SVM分类器
------

通过聚类得到的词典，即可将图片的特征转化为对应的词频表（如统计直方图）。这样每幅图片就可以用一个词典向量来表示了，然后将这些向量作为分类器的输入进行训练即可得到分类器。本文中选择的是SVM（支持向量机）进行分类。  
支持向量机（英语：Support Vector Machine，常简称为SVM）是一种监督式学习的方法，可广泛地应用于统计分类以及回归分析。  
支持向量机将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面。建立方向合适的分隔超平面使两个与之平行的超平面间的距离最大化。其假定为，平行超平面间的距离或差距越大，分类器的总误差越小。支持向量机通过学习已知样本来获得决策边界，来完成对未知样本的分类。这种学习是有监督的学习。  
本文中训练时，是将多类问题看作二类问题，即训练某类时以此类为正样本，其它类为负样本。测试时，计算测试样本分作某一类的概率，取最大概率的分类为测试样本的类别。  
![img](https://img2023.cnblogs.com/blog/675649/202307/675649-20230704231732092-319975720.png)

利用决策边界进行有监督分类

实验结果
====

测试图片使用corel图片库，选择了其中5个类别各20张图作为训练集训练分类器，并选择同类别各20张作为测试集测试分类的准确性。并且与K阶最近邻分类器作对比（取K为9）。测试平台为T6600CPU，系统为WINDOWS8.1。

训练图片集  
![img](https://img2023.cnblogs.com/blog/675649/202307/675649-20230704231757304-474595564.png)

表2 测试图片集  
![img](https://img2023.cnblogs.com/blog/675649/202307/675649-20230704231825339-604519986.png)  
测试时通过加大Kmeans聚类的类数可以达到不同的效果，基本上Kmeans类数越大，SVM分类的效果越好。这也很容易理解，K-means聚类个数即代表了生成的词典中词的个数，词数越多，对图像的描述更精确。但随着词数的增加，字典的生成时间会增加。

2者的关系如下表：

视觉词个数

SVM分类

KNN分类

字典生成耗时（s）

正确分类个数

正确率

分类耗时(s)

正确分类个数

正确率

分类耗时(s)

2

18

0.36

69

21

0.42

69

69

5

33

0.66

72

34

0.68

71

81

10

38

0.76

77

36

0.72

77

110

20

42

0.84

100

37

0.74

97

232

50

42

0.84

102

37

0.74

98

240

100

43

0.86

111

35

0.70

103

447

200

44

0.88

110

38

0.76

94

647

500

47

0.94

118

28

0.56

101

978

1000

47

0.94

135

27

0.54

108

1294

由表可以看出，对比KNN分类，SVM在单词个数不太小时，都会好于KNN，并且在单词数较大时，效果远好于KNN的分类，准确率达到了94%。

总结
==

本文介绍了一个典型的基于词袋和SVM分类器的图片分类方法。基于词袋的图片分析技术由于具有对图片进行语义理解的潜力，因此得到广泛研究。SVM分类器也是目前非常火热的分类方法之一。实验结果表明，二者结合用于图片的有监督分类可以得到很好的效果。但由于词袋没有考虑词的顺序，对于图片也就是没有考虑特征的空间分布信息，因而还有待改进，也有人提出了Spatial pyramid matching方法进行分尺度，分块的处理，得到了一定改善，但仍有待研究。

参考文献
====

\[1\] Lowe, David G. "Distinctive image features from scale-invariant keypoints." _International journal of computer vision_ 60.2 (2004): 91-110.

\[2\] Wallach, Hanna M. "Topic modeling: beyond bag-of-words." _Proceedings of the 23rd international conference on Machine learning_. ACM, 2006.

\[3\] Dalal, Navneet, and Bill Triggs. "Histograms of oriented gradients for human detection." _Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on_. Vol. 1. IEEE, 2005.

\[4\] Chang, Chih-Chung, and Chih-Jen Lin. "LIBSVM: a library for support vector machines." ACM Transactions on Intelligent Systems and Technology (TIST) 2.3 (2011): 27.

\[5\] Tong, Simon, and Edward Chang. "Support vector machine active learning for image retrieval." _Proceedings of the ninth ACM international conference on Multimedia_. ACM, 2001.

源码及完整报告：
========

算法使用C++ MFC实现，基于OpenCV。  
[https://mbd.pub/o/bread/ZJuTlZ5u](https://mbd.pub/o/bread/ZJuTlZ5u) 或  
[https://www.syjshare.com/res/1TUR49UG](https://www.syjshare.com/res/1TUR49UG)

本文来自博客园，作者：[CoderInCV](https://www.cnblogs.com/haoliuhust/)，转载请注明原文链接：[https://www.cnblogs.com/haoliuhust/p/17527366.html](https://www.cnblogs.com/haoliuhust/p/17527366.html)