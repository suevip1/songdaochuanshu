---
layout: post
title: "Lite-Mono(CVPR2023)论文解读"
date: "2023-07-25T01:17:16.338Z"
---
Lite-Mono(CVPR2023)论文解读

　　Lite-Mono: A Lightweight CNN and Transformer Architecture for Self-Supervised Monocular Depth Estimation 是CVPR2023收录的论文，从它的标题能很清晰了解到它所在的领域或解决的问题是：自监督的单目深度估计；它的网络结构是轻量化的卷积和Transformer的混合结构。

解决的问题
-----

　　大多数的研究都是往提高任务的准确率，这必然造成所设计的网络更注重准确率，使网络的体积或计算量大幅提高，这使在边缘终端上使用的可能性越来越小。本文针对自监督的单目深度估计，探讨在控制网络参数量的同时保持或提高准确性。

主要贡献
----

1\. 在自监督的单目深度估计方面，降低网络的参数量和运算量，同时提升了准确率。提高在边缘终端上使用的可能性。

相关研究
----

### 1）监督的深度估计

\[1\] Depth map prediction from a single image using a multi-scale deep netwrk. (NeurIPS2014)

\[2\] Deeper depth prediction with fully convolutional residual networks. (3DV2016)

\[3\] Deep ordinal regression network for monocular depth estimation. (CVPR2018)

### 2）自监督的深度估计

\[4\] Unsupervised cnn for single view depth estimation: Geometry to the rescue. (ECCV2016)

\[5\] Unsupervised monocular depth estimation with left-right consistency. (CVPR2017)

\[6\] Unsupervised learning of depth and ego-motion from video. (CVPR2017)

\[7\] Digging into self-supervised monocular depth estimation. (ICCV2019)

### 3）这方面的网络架构

\[8\] Self-supervised monocular depth estimation with internal feature fusion. (2021)

\[9\] R-msfm: Recurrent multi-scale feature modulation for monocular depth estimating. (ICCV2021)

\[10\] Transformers in self-supervised monocular depth estimation with unknown camera intrinsics. (2022)

\[11\] Vision transformers for dense prediction. (ICCV2021)

\[12\] Monoformer: Towards generalization of self-supervised monocular depth estimation with transformers. (2022)

\[13\] Monovit: Self-supervised monocular depth estimation with a vision transformers. (3DV2022)

\[14\] Mpvit: Multi-path vision transformer for dense prediction. (CVPR2022)

解决方案和关键点
--------

### 1）网络结构

![](https://img2023.cnblogs.com/blog/39858/202307/39858-20230724153020917-2054454592.png)

　　作者的工作主要在DepthNet上，PoseNet是一直沿用ResNet-18的方案（从 \[7\] 之后没有改变）。

　　DepthNet是4-Stage结构，每个Stage进行1/2分辨率的下采样（新的特征图是原来的1/4），这是比较常规的做法，另一种是5-Stage，考虑到是轻量化的工作，选择5-Stage的可能性更小（参数量和计算量）。在Encoder里另一个特别的做法是Stage-1与Decoder没有连接，与常见的类UNet结构有些区别，估计也是为了轻量化考虑。

　　Encoder一共4个创新点：

　　1. CDC Block；

　　2. LGFI Block；

　　3. Pooled Concatenation；

　　4. Cross-Stage Connction.

### 2）Consecutive Dilated Convolutions (CDC)

![](https://img2023.cnblogs.com/blog/39858/202307/39858-20230724155744803-324993840.png)

 　　CDC主要由三部分组成：3 x 3的空洞深度可分离卷积和两层全连接。作者提到与之前的空洞卷积通常在并联使用，而这里会串联使用，简单说，每个CDC Block的Dilation值可能配置不一样，这使CDC的感受野有所区别，增加抓取特征的多样性。使用深度可分离卷积(MobileNet v1)是轻量化手段之一，但同时也会弱化网络的表现能力。为了解决这个问题，depth-wise卷积通常连接着point-wise卷积，从作用上等效普通的卷积层。作者在这里没有使用point-wise，而是在通道（channel）上使用全连接，变相达到point-wise效果并且进行channel膨胀。再由最后一层全连接投影回原通道，从结构上有点深度可分离卷积接传统MLP的味道，但参数量更少。

### 3）Local-Global Features Interaction (LGFI)

![](https://img2023.cnblogs.com/blog/39858/202307/39858-20230724162522346-394440083.png)

 　　LGFI主要由两部分组成：自注意力模块和MLP。两部分都没有什么特别的地方，Transformer模块本身不会增加很多参数量，它的消耗主要在计算量上，所以作者在Stage-2 到 Stage-4，每个Stage只使用一个LGFI。把全局关联性作为一个补充，而不是主要。

### 4）Pooled Concatenation

　　在Stage-2 到 Stage-4的输入端增加3个原图下采样的特征，3个特征图分别是R，G，B，而下采样的方式是平均池化。由于从原图直接平均池化到目标分辨率会引起大量细节丢失，所以作者使用多次1/2平均池化的做法。相比较每个Stage的输入通道数，Pooled Concatenation所提供3个通道的影响比较有限，从消融实验中能看出来。

### 5）Cross-Stage Connection

　　在Stage-3和Stage-4的输入端，除了上一个Stage的输入之外，增加上一个Stage的“生图”。由于每个Stage的通道数是固定的，所以下一个Stage的输入通道数成倍增加，应该是从增加特征多样性上考虑。但从消融实验中能看出来，它的作用不是太大。

### 6）网络配置

![](https://img2023.cnblogs.com/blog/39858/202307/39858-20230724164642753-400325680.png)

 　　作者提供了4种不同的网络配置，最小只有2.0M参数量，比现在一般的网络都小得多，能为不同的边缘终端选择合适的配置。

总结
--

　　在实验部分可以看到本网络结构在参数量、计算量和准确率均比之前的网络有所提高，主要还是得益于CDC和LGFI。实验部分我没有谈到，如果需要的话，可以补充上去。

　　Transformer模块在视觉上的使用都得到广泛的认可，它主要还是计算量方面的损耗比较大，但在小尺寸特征图上的损耗没这么突出，所以在Stage-4里可以尝试使用更多的Transformer模块。