---
layout: post
title: "å¤§è¯AIç»˜ç”»æŠ€æœ¯åŸç†ä¸ç®—æ³•ä¼˜åŒ–"
date: "2023-05-01T01:11:00.880Z"
---
å¤§è¯AIç»˜ç”»æŠ€æœ¯åŸç†ä¸ç®—æ³•ä¼˜åŒ–
===============

### å¼•å­

åšä¸»å¾ˆé•¿ä¸€æ®µæ—¶é—´éƒ½æ²¡æœ‰å‘æ–‡ï¼Œç¡®å®æ˜¯åœ¨å¿™ä¸€äº›æŠ€æœ¯ç ”ç©¶ã€‚

å¦‚æ ‡é¢˜æ‰€ç¤ºï¼Œæœ¬ç¯‡åšæ–‡ä¸»è¦æŠŠè¿‘æ®µæ—¶é—´çš„ç ”ç©¶å·¥ä½œåšä¸€ä¸ªreviewã€‚

çœ‹è¿‡å„ç§ç›¸å…³æŠ€æœ¯çš„å…¬å…³æ–‡ç« ï¼Œæ—æ—æ€»æ€»ï¼Œæ°´åˆ†å¾ˆå¤šã€‚

ä¹Ÿç¡®å®æ²¡æœ‰å¤šå°‘äººèƒ½æŠŠä¸€äº›æŠ€æœ¯ç»†èŠ‚ç”¨ä¸€äº›æ¯”è¾ƒé€šä¿—çš„è¯­è¨€é˜è¿°æ¸…æ¥šã€‚

æ•…æ­¤ï¼Œå†ä¸€æ¬¡å† ä»¥å¤§è¯ä¸ºé¢˜ï¼Œå¯¹AIç»˜ç”»ä¸»è¦æ˜¯stable diffusionåšä¸€ä¸ªæŠ€æœ¯æ¢³ç†ã€‚

**å¦‚ä½•å­¦ä¹ ä»¥åŠç›¸å…³èµ„æº**

ç›¸ä¿¡å¾ˆå¤šæœ‹å‹éƒ½æƒ³å…¥é—¨åˆ°è¿™ä¸ªæŠ€æœ¯é¢†åŸŸæ£è…¾æ£è…¾ï¼Œ

è€Œæ‘†åœ¨çœ¼å‰çš„ç¡®æ˜¯ä¸€æ¡è‡­æ°´æ²Ÿã€‚

ä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€äº›æ•°æ®ã€‚

*   **Hardware:**Â 32 x 8 x A100 GPUs
    
*   **Optimizer:**Â AdamW
    
*   **Gradient Accumulations**: 2
    
*   **Batch:**Â 32 x 8 x 2 x 4 = 2048
    
*   **Learning rate:**Â warmup to 0.0001 for 10,000 steps and then kept constant
    
    **Hardware Type:**Â A100 PCIe 40GB
    
*   **Hours used:**Â 150000
    
*   **Cloud Provider:**Â AWS
    
*   **Compute Region:**Â US-east
    
*   **Carbon Emitted (Power consumption x Time x Carbon produced based on location of power grid):**Â 11250 kg CO2 eq.
    

æ‘˜è‡ªï¼š[CompVis/stable-diffusion-v1-4 Â· Hugging Face](https://huggingface.co/CompVis/stable-diffusion-v1-4)

è¯¥æ¨¡å‹æ˜¯åœ¨äºšé©¬é€Šäº‘è®¡ç®—æœåŠ¡ä¸Šä½¿ç”¨256ä¸ªNVIDIA A100 GPUè®­ç»ƒï¼Œå…±èŠ±è´¹15ä¸‡ä¸ªGPUå°æ—¶ï¼Œæˆæœ¬ä¸º60ä¸‡ç¾å…ƒ

æ‘˜è‡ªï¼š[Stable Diffusion - ç»´åŸºç™¾ç§‘ï¼Œè‡ªç”±çš„ç™¾ç§‘å…¨ä¹¦ (wikipedia.org)](https://zh.wikipedia.org/wiki/Stable_Diffusion)

è¿™ä¸ªæ•°æ®å°±æ˜¯ä¸€ä¸ªåŠé€€è­¦å‘Šï¼Œä½†æ˜¯ç”±äºæ•ˆæœå¤ªè¿‡äºâ€œå“äººâ€ï¼Œæ‰€ä»¥é£è›¾æ‰‘ç«ï¼Œå…¨ä¸–ç•Œéƒ½æ‰“èµ·æ¶æ¥äº†ã€‚

å½“ç„¶ï¼Œåˆšå¼€å§‹å­¦ä¹ ï¼Œå°±ç›´æ¥å¥”ç€æœ€ç»ˆç« å»ï¼Œç¡®å®ä¹Ÿä¸æ˜¯å¾ˆç°å®ã€‚

éšç€è¿™ä¸ªé¢†åŸŸçš„çˆ†ç«ï¼Œå„ç§èµ„æºçˆ†ç‚¸å¼å¢é•¿ã€‚

ä»¥ä¸‹æ˜¯åšä¸»ç»™å‡ºçš„ä¸€éƒ¨åˆ†å‚è€ƒèµ„æºï¼Œä¾¿äºå‚é˜…ã€‚

ç›¸å…³æ•´åˆèµ„æº:

[heejkoo/Awesome-Diffusion-Models: A collection of resources and papers on Diffusion Models (github.com)](https://github.com/heejkoo/Awesome-Diffusion-Models)

ç¬¬ä¸‰æ–¹:

[Generative Deep Learning (keras.io)](https://keras.io/examples/generative/)

[huggingface/diffusers: ğŸ¤— Diffusers: State-of-the-art diffusion models for image and audio generation in PyTorch (github.com)](https://github.com/huggingface/diffusers)

[AUTOMATIC1111/stable-diffusion-webui: Stable Diffusion web UI (github.com)](https://github.com/AUTOMATIC1111/stable-diffusion-webui)

å®˜æ–¹:

[CompVis/stable-diffusion: A latent text-to-image diffusion model (github.com)](https://github.com/CompVis/stable-diffusion)

[Stability-AI/stablediffusion: High-Resolution Image Synthesis with Latent Diffusion Models (github.com)](https://github.com/Stability-AI/StableDiffusion)

æƒ³è¦å¼€ç®±å¼å¿«é€Ÿä¸Šæ‰‹ï¼Œå»ºè®®æŠŠkerasç¤¾åŒºçš„è¿™ä¸ªç”ŸæˆèŠ±æœµçš„ç©å…·è·‘èµ·æ¥æ„Ÿå—ä¸€ä¸‹ã€‚

[Denoising Diffusion Implicit Models (keras.io)](https://keras.io/examples/generative/ddim/)

è¿™ä¸ªæ˜¯ä¸€ä¸ªéå¸¸ç®€æ´çš„å®ç°ï¼Œéº»é›€è™½å°äº”è„ä¿±å…¨ï¼Œå¯ä»¥å¿«é€Ÿçƒ­èº«èµ·æ¥ã€‚

**Stable Diffusionçš„åŸºæœ¬åŸç†**

è¿™é‡Œåšä¸»å¹¶ä¸æ‰“ç®—å±•å¼€è®²è§£è¿‡å¤šï¼Œ

åªåšä¸€ä¸ªå¤§è¯æ¦‚è§ˆé˜è¿°ï¼Œä¾¿äºå¿«é€Ÿå…¥é—¨äº†è§£ã€‚

è‹¥è¿˜æœ‰ç–‘æƒ‘ï¼Œå¤§å®¶å¯ä»¥å‚é˜…å…¶ä»–çš„èµ„æºã€‚

äº†è§£åŸç†ï¼Œä»…ä»…é˜…è¯»ä»£ç è‚¯å®šæ˜¯è¿œè¿œä¸å¤Ÿçš„ï¼Œ

ä½†æ˜¯ä¸é˜…è¯»ä»£ç ï¼Œä½ å°±ä¸çŸ¥é“å…·ä½“çš„å®ç°ç»†èŠ‚ã€‚

å®˜æ–¹å®ç°ï¼Œé‚£ä¸ªä»£ç ä»“åº“çœŸæ˜¯ä¸€åº§å±å±±ï¼Œä¹±ä¸ƒå…«ç³Ÿçš„ã€‚

æ‰€ä»¥æä½³çš„é˜…è¯»ç‰ˆæœ¬æ˜¯kerasç¤¾åŒºçš„å®ç°ã€‚

[keras-cv/keras\_cv/models/stable\_diffusion](https://github.com/keras-team/keras-cv/tree/master/keras_cv/models/stable_diffusion)

stable\_diffusionçš„ä¸»è¦ç»„ä»¶å¦‚ä¸‹:

1.æ–‡æ¡ˆç¼–ç å™¨ï¼š

[keras-cv/text\_encoder.py](https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/text_encoder.py)

è´Ÿè´£å¯¹è¾“å…¥çš„æ–‡å­—è¿›è¡Œç‰¹å¾ç¼–ç ï¼Œç”¨äºå¼•å¯¼ diffusion æ¨¡å‹è¿›è¡Œå†…å®¹ç”Ÿæˆã€‚

2.æ½œåœ¨ç‰¹å¾ç¼–ç å™¨ï¼š

[keras-cv/image\_encoder.py](https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/image_encoder.py)

å°†å›¾ç‰‡ç¼–ç æˆæ½œåœ¨ç‰¹å¾

3.æ½œåœ¨ç‰¹å¾è§£ç å™¨ï¼š

[keras-cv/decoder.py](https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/decoder.py)

å°†æ½œåœ¨ç‰¹å¾è§£ç æˆå›¾ç‰‡

4.diffusion æ¨¡å‹:

[keras-cv/diffusion\_model.py](https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/diffusion_model.py)

ç”±å™ªå£°å’Œæ–‡å­—è¯­ä¹‰ç”Ÿæˆç›®æ ‡æ½œåœ¨ç‰¹å¾ã€‚

ç”¨é€šä¿—çš„è¯æ¥è¯´ç¼–ç å™¨å°±æ˜¯å‹ç¼©ï¼Œè§£ç å™¨å°±æ˜¯è§£å‹ï¼Œdiffusion æ¨¡å‹å°±æ˜¯ç¼–è¾‘å‹ç¼©çš„ä¿¡æ¯ï¼Œè€Œæ–‡æ¡ˆæ˜¯å¼•å¯¼ç¼–è¾‘çš„æ–¹å‘ã€‚

5.è¾…åŠ©ç»„ä»¶:

[keras-cv/clip\_tokenizer.py](https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/clip_tokenizer.py)

é’ˆå¯¹æ–‡æ¡ˆè¾“å…¥çš„ç¼–ç è½¬æ¢ä»¥åŠé¢„å¤„ç†ã€‚

ç®€è€Œè¨€ä¹‹å°±æ˜¯å°†æ–‡å­—è½¬æ¢æˆæ•°å­—ã€‚

[keras-cv/noise\_scheduler.py](https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/noise_scheduler.py)

å™ªå£°è§„åˆ’ï¼Œä¸»è¦ç”¨äºè®­ç»ƒå’Œåˆæˆçš„åŠ å™ªå’Œå»å™ªçš„æ¯”ç‡è®¡ç®—ã€‚

æ•´ä½“çš„æ¶æ„ä¸²èµ·æ¥å°±æ˜¯stable\_diffusion

[keras-cv/stable\_diffusion.py](https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/stable_diffusion.py)

æ•´ä¸ªä½¿ç”¨çš„æµç¨‹å¤§æ¦‚æ˜¯è¿™ä¸ªæ ·å­çš„ï¼š

æ–‡å­—ç”Ÿæˆå›¾ç‰‡:

æ–‡å­— -> clip\_tokenizer -> text\_encoder -> diffusion\_model -> decoder

å›¾ç‰‡ç”Ÿæˆå›¾ç‰‡:

\[\[å›¾ç‰‡ -> image\_encoder\] + \[æ–‡å­— -> clip\_tokenizer -> text\_encoder\]\] -> diffusion\_model -> decoder

è¿™æ˜¯ä¸»æµçš„ä¸¤ç§åšæ³•ï¼Œè¿˜æœ‰é€šè¿‡maskè¿›è¡Œå†…å®¹ä¿®å¤ï¼Œä»¥åŠé€šè¿‡å…¶ä»–æ¨¡å—è¾…åŠ©ç”Ÿæˆçš„ï¼Œä¾‹å¦‚ç”Ÿæˆå°å§å§ä¹‹ç±»ç­‰ç­‰ã€‚

åˆ—å®Œç»„ä»¶ï¼Œ

ä½ å°±ä¼šå‘ç°æ¶‰åŠçš„æŠ€æœ¯å¹¶ä¸ç®€å•ã€‚

è¯´å¥½çš„ï¼Œè¦è®²åŸç†çš„ã€‚

ä¸€å¥è¯æè¿°ï¼š è¿™æ˜¯ä¸€ä¸ªä¿¡å·å‹ç¼©å’Œè§£å‹çš„ç®—æ³•ã€‚

å…¶ä¸­çš„ä¿¡å·æ˜¯é€šè¿‡å™ªå£°æ‰©æ•£å»ºçš„æ¨¡ï¼Œå»ºæ¨¡åç¬¦åˆæ­£å‘å’Œé€†å‘çš„æ•°å­¦è§„å¾‹ã€‚

è€Œåœ¨æ­£å‘å’Œé€†å‘ä¸­éƒ½å¯ä»¥æ³¨å…¥å™ªå£°æˆ–è€…æ”¹å˜å™ªå£°æ¥è¾¾åˆ°ä¿¡å·å¼•å¯¼é‡å»ºã€‚

ä¸´æ—¶å¼€ä¸ªå°å‰ï¼š

æˆ‘ä»¬çŸ¥é“:

å£°éŸ³æ–‡ä»¶wavè¿›è¡Œä½™å¼¦å˜æ¢å°±å¯ä»¥å‹ç¼©æˆMp3

ä½å›¾æ–‡ä»¶bmpè¿›è¡Œä½™å¼¦å˜æ¢å°±å¯ä»¥å‹ç¼©æˆjpg

é‚£ä¹ˆä½ è¯´å¦‚æœæˆ‘ä»¬å¯¹ä½™å¼¦å˜æ¢è¿›è¡Œæ•°å­¦ç¼–è¾‘ï¼Œæ˜¯ä¸æ˜¯å¯ä»¥ç›´æ¥ç¼–è¾‘ç›®æ ‡çš„mp3æˆ–è€…ç›®æ ‡çš„jpg

ä½ ç­”å¯¹äº†ï¼Œæ˜¯å¯ä»¥çš„ã€‚

æœ€ç»å…¸çš„åšæ³•å°±æ˜¯é‡‡ç”¨ä½™å¼¦å˜æ¢å¯¹ä¿¡å·è¿›è¡Œæ»¤æ³¢ï¼Œç„¶åå¯ä»¥åšåˆ°é™å™ªï¼Œä¹Ÿå°±å¯ä»¥åšåˆ°éŸ³é¢‘é™å™ªæˆ–è€…å›¾ç‰‡é™å™ªã€‚

æˆ‘ä»¬å›åˆ°ä¸»é¢˜ä¸Šæ¥ï¼ŒåŸºäºå™ªå£°å»ºæ¨¡ä¹‹åï¼Œæœ‰ä»€ä¹ˆå¥½å¤„ï¼Œè¿™æ ·åšäº†ä¹‹åæ•…äº‹å˜å¾—æ›´åŠ æœ‰æ„æ€çš„ã€‚

åªè¦ç¼–è¾‘åçš„æ•°æ®ç¬¦åˆå™ªå£°åˆ†å¸ƒï¼Œé‚£ç†è®ºä¸Šæ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨æ­£å‘æˆ–è€…é€†å‘æ‰©æ•£çš„æ–¹å¼å¯¹ä»»æ„æ—¶åˆ»çš„ä¿¡å·è¿›è¡Œç¼–è¾‘ã€‚

è€ŒStable Diffusion å°±æ˜¯è¿™æ ·ä¸€ä¸ªæŠ€æœ¯æ–¹æ¡ˆã€‚

è®­ç»ƒæ—¶å€™å®ƒå°†æ•°æ®é€šè¿‡noise\_schedulerè¿›è¡Œdiffusion\_modelæ­£å‘æ‰©æ•£åˆ†æ²»å‹ç¼©ï¼Œ

ç”Ÿæˆä½¿ç”¨çš„æ—¶å€™é€šè¿‡diffusion\_modelè¿›è¡Œé€†å‘æ‰©æ•£ç¼–è¾‘è§£å‹ï¼Œ

æœ€ç»ˆè¾¾åˆ°å¯ä»¥ç”Ÿæˆä»»æ„ä¿¡å·ç»„åˆçš„å›¾ç‰‡ã€‚

æˆ‘çŸ¥é“ä½ æœ‰ç–‘é—®äº†ï¼Œé‚£æ˜¯ä¸æ˜¯åœ¨ç”Ÿæˆçš„æ—¶å€™å¯ä»¥åœ¨ä»»æ„æ—¶å€™æ’å…¥ä¿¡æ¯ï¼Œæˆ–è€…æ˜¯æ­£å‘æ‰©æ•£å’Œé€†å‘æ‰©æ•£æ··åˆç€æ¥ã€‚

æ²¡é”™ï¼Œå®Œå…¨å¯ä»¥çš„ã€‚

å›¾ç‰‡ç”Ÿæˆå›¾ç‰‡ï¼Œå°±æ˜¯åœ¨é€†å‘æ‰©æ•£è¿‡ç¨‹ä¸­æ’å…¥ä¸€ä¸ªæˆ‘ä»¬é¢„è®¾çš„å›¾ç‰‡ä¿¡æ¯èŠ‚ç‚¹ä¸å™ªå£°è¿›è¡Œæ··åˆï¼Œç„¶åé€šè¿‡æ–‡å­—è¯­ä¹‰å¼•å¯¼åˆæˆçš„è¿‡ç¨‹ã€‚

çŸ¥é“äº†æŠ€æœ¯åŸç†ï¼Œæˆ‘ç›¸ä¿¡ä½ ä»¬è·Ÿæˆ‘ä¸€æ ·ä¼šæœ‰ä¸€ä¸ªéå¸¸å¤§èƒ†çš„å‡è®¾ã€‚

å‡è®¾æŠŠå…¨ä¸–ç•Œçš„ç”µè„‘è¿æ¥èµ·æ¥ï¼Œç„¶åé‡‡ç”¨æ‰©æ•£åˆ†æ²»ï¼Œé‚£å®Œå…¨å¯ä»¥è®©ä¸–ç•Œä¸Šä»»ä½•ä¸€ä¸ªåœ°åŒºä»»ä½•ä¸€ä¸ªäººçš„ä¸€å°æœºå™¨ï¼Œå¸®ä½ æŠŠæŸä¸ªèŠ‚ç‚¹çš„æ‰©æ•£ç¼–è¾‘ç»™ç®—äº†ã€‚

é‚£è¿™ä¸ªäº‹æƒ…å°±ææ€–äº†ï¼Œå…¨ä¸–ç•Œéƒ½æ˜¯p2p ç®—åŠ›å…±äº«äº†ï¼Œäººå·¥æ™ºèƒ½çš„å¤§æ—¶ä»£å°±åˆ°æ¥äº†ã€‚

å°±é—®ä¸€ä¸ªé—®é¢˜ï¼Œç°åœ¨ä¸Šèˆ¹è¿˜æ¥å¾—åŠå—ï¼Ÿ

åšä¸»ä¹Ÿä¸çŸ¥é“ï¼Œåªæ˜¯è¿™ä¸ªäº‹å·²ç»æ˜¯é“æ¿é’‰é’‰äº†ï¼Œè¶‹åŠ¿ä¸å¯é€†ã€‚

**å…³é”®ç®—æ³•ä»¥åŠç›¸å…³ä¼˜åŒ–**

å‰é¢è®²åˆ°æŠ€æœ¯åŸç†ï¼Œä½†æ˜¯åˆ°åº•å…¶ä¸­å…³é”®ç®—æ³•æ˜¯ä»€ä¹ˆï¼Ÿ

ç­”æ¡ˆæ˜¯ï¼šè·¨æ¨¡æ€æ³¨æ„åŠ›ã€‚

ä¸€åˆ‡éƒ½ç”±è°·æ­Œçš„ä¸€ç¯‡è®ºæ–‡å¼€å§‹ã€‚

[\[1706.03762\] Attention Is All You Need (arxiv.org)](https://arxiv.org/abs/1706.03762)

åœ¨è¿™ç¯‡è®ºæ–‡å‡ºç°åï¼Œç™¾èŠ±é½æ”¾ï¼Œå„ç§attention, å„ç§transformerå±‚å‡ºä¸ç©·ã€‚

transformeræ¶æ„çš„æå‡ºï¼Œç»™å‡ºä¸åŒç»´åº¦æ•°æ®å»ºæ¨¡çš„å¯è¡Œæ€§ã€‚

å…³äºè¿™æ–¹é¢çš„å‘å±•å’Œèµ„æºï¼Œåšä¸»åªç»™å‡ºå¦‚ä¸‹èµ„æºã€‚

ä¸€ä¸ªå€¼å¾—å…³æ³¨çš„æŠ€æœ¯github:

[lucidrains (Phil Wang) Â· GitHub](https://github.com/lucidrains)

lucidrainsæ˜¯ä¸€ä¸ªéå¸¸å‹¤å¥‹ä¸”é«˜äº§çš„äººï¼Œå‡ ä¹æ‰€æœ‰ç¬¬ä¸‰æ–¹çš„transformerå®ç°éƒ½æœ‰ä»–çš„å½±å­ï¼Œå½“ç„¶ä¹ŸåŒ…æ‹¬stable\_diffusionã€‚

åœ¨stable\_diffusionä¸­ä¹Ÿæ˜¯å› ä¸ºä½¿ç”¨transformeræ‰€ä»¥å¸¦æ¥äº†ä¸¥é‡çš„è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚

é’ˆå¯¹stable\_diffusion çš„ç®—æ³•ä¼˜åŒ–ï¼Œå„å¤§å‚ä¹Ÿæ˜¯å‹¤å¥‹çš„ã€‚

é™„ç›¸å…³ä¿¡æ¯:

è‹±ç‰¹å°”

[Accelerating Stable Diffusion Inference on Intel CPUs (huggingface.co)](https://huggingface.co/blog/stable-diffusion-inference-intel)

é«˜é€š

[Worldâ€™s first on-device demonstration of Stable Diffusion on an Android phone | Qualcomm](https://www.qualcomm.com/news/onq/2023/02/worlds-first-on-device-demonstration-of-stable-diffusion-on-android)

è‹¹æœ

[Stable Diffusion with Core ML on Apple Silicon - Apple Machine Learning Research](https://machinelearning.apple.com/research/stable-diffusion-coreml-apple-silicon)

è°·æ­Œ

[\[2304.11267\] Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations (arxiv.org)](https://arxiv.org/abs/2304.11267)

å½“ç„¶æˆ‘ä»¬è¿™é‡Œè®²çš„ä¼˜åŒ–æŒ‡çš„æ˜¯ä½¿ç”¨é˜¶æ®µçš„ä¼˜åŒ–ï¼Œå¹¶ä¸æ˜¯è®­ç»ƒé˜¶æ®µçš„ä¼˜åŒ–ã€‚

è¿™æ˜¯å®Œå…¨ä¸åŒçš„ä¸¤ä»¶äº‹ï¼Œè€Œç°åœ¨å›½å†…æ³›æŒ‡çš„stable\_diffusionè®­ç»ƒä¹Ÿä¸æ˜¯æŒ‡çš„ä»é›¶è®­ç»ƒï¼Œ

è€Œæ˜¯åœ¨stable\_diffusionçš„åŸºç¡€ä¸Šè¿›è¡ŒäºŒæ¬¡è®­ç»ƒå¾®è°ƒã€‚

æ¯•ç«Ÿè®­ç»ƒæˆæœ¬æ‘†åœ¨é‚£é‡Œï¼Œæ²¡æœ‰å‡ å®¶å…¬å¸ä¼šè¿™ä¹ˆè±ªæ°”å»å¤ç°ï¼ŒåŒ…æ‹¬å›½å†…æŸäº›å¤§å‚ã€‚

è®­ç»ƒä¼˜åŒ–å±•å¼€ä¸€æ—¶åŠä¼šä¹Ÿè®²ä¸å®Œï¼Œæ‰€ä»¥åšä¸»ç€é‡è®²ä¸€ä¸‹ï¼Œä½¿ç”¨é˜¶æ®µçš„ä¼˜åŒ–ã€‚

å‚è€ƒä¿¡æ¯æ¥è‡ª:

[Optimizations Â· AUTOMATIC1111/stable-diffusion-webui Wiki (github.com)](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Optimizations)

commandline argument

explanation

`--opt-sdp-attention`

Faster speeds than using xformers, only available for user who manually install torch 2.0 to their venv. (non-deterministic)

`--opt-sdp-no-mem-attention`

Faster speeds than using xformers, only available for user who manually install torch 2.0 to their venv. (deterministic, slight slower thanÂ `--opt-sdp-attention`)

`--xformers`

UseÂ [xformers](https://github.com/facebookresearch/xformers)Â library. Great improvement to memory consumption and speed. Will only be enabled on small subset of configuration because that's what we have binaries for.Â [Documentation](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers)

`--force-enable-xformers`

Enables xformers above regardless of whether the program thinks you can run it or not. Do not report bugs you get running this.

`--opt-split-attention`

Cross attention layer optimization significantly reducing memory use for almost no cost (some report improved performance with it). Black magic.  
On by default forÂ `torch.cuda`, which includes both NVidia and AMD cards.

`--disable-opt-split-attention`

Disables the optimization above.

`--opt-sub-quad-attention`

Sub-quadratic attention, a memory efficient Cross Attention layer optimization that can significantly reduce required memory, sometimes at a slight performance cost. Recommended if getting poor performance or failed generations with a hardware/software configuration that xformers doesn't work for. On macOS, this will also allow for generation of larger images.

`--opt-split-attention-v1`

Uses an older version of the optimization above that is not as memory hungry (it will use less VRAM, but will be more limiting in the maximum size of pictures you can make).

`--medvram`

Makes the Stable Diffusion model consume less VRAM by splitting it into three parts - cond (for transforming text into numerical representation), first\_stage (for converting a picture into latent space and back), and unet (for actual denoising of latent space) and making it so that only one is in VRAM at all times, sending others to CPU RAM. Lowers performance, but only by a bit - except if live previews are enabled.

`--lowvram`

An even more thorough optimization of the above, splitting unet into many modules, and only one module is kept in VRAM. Devastating for performance.

`*do-not-batch-cond-uncond`

Prevents batching of positive and negative prompts during sampling, which essentially lets you run at 0.5 batch size, saving a lot of memory. Decreases performance. Not a command line option, but an optimization implicitly enabled by usingÂ `--medvram`Â orÂ `--lowvram`.

`--always-batch-cond-uncond`

Disables the optimization above. Only makes sense together withÂ `--medvram`Â orÂ `--lowvram`

`--opt-channelslast`

Changes torch memory type for stable diffusion to channels last. Effects not closely studied.

`--upcast-sampling`

For Nvidia and AMD cards normally forced to run withÂ `--no-half`,Â [should improve generation speed](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/8782).

ä»è¿™ä¸ªè¡¨å•ä¸Šï¼Œæˆ‘ä»¬èƒ½å¾—å‡ºçš„ä¿¡æ¯å°±æ˜¯ä¼˜åŒ–çš„æ–¹å‘ï¼ŒåŸºæœ¬éƒ½æ˜¯æ˜¾å­˜å ç”¨ã€‚

é™¤äº†æ˜¾å­˜å ç”¨å¤–çš„ä¼˜åŒ–ä¹Ÿæœ‰å‡ æ¡è·¯å­å¯ä»¥èµ°ã€‚

1.ä¼˜åŒ–ç®—å­ï¼Œå¹¶è¡Œè®¡ç®—æé€Ÿ

ä¾‹å¦‚ï¼š

å°†sigmoid æ›¿æ¢æˆç­‰ä»·çš„tanhï¼Œå› ä¸ºæ•°å­¦è®¡ç®—ä¸­tanhçš„è®¡ç®—æ›´åŠ ç®€å•

2.é‡åŒ–ç²¾åº¦ï¼Œé‡‡ç”¨fp16æˆ–æ›´ä½çš„ç²¾åº¦æ±‚è¿‘ä¼¼è§£

fp16å¸¦æ¥çš„ä¸€äº›è®¡ç®—æˆ–æº¢å‡ºå–å€¼èŒƒå›´ï¼Œæ‰€ä»¥å¦‚æœé‡‡ç”¨é‡åŒ–ç²¾åº¦çš„æ–¹æ¡ˆï¼Œå¿…é¡»è€ƒè™‘è®¡ç®—çš„å€¼åŸŸé—®é¢˜ã€‚

3.å¾®è°ƒè’¸é¦ï¼Œå°†æ¨¡å‹ä¸­çš„æŸäº›è€—æ—¶è®¡ç®—è’¸é¦

ä¾‹å¦‚å°†æ¨¡å‹ä¸­çš„transformeré‡‡ç”¨ç­‰ä»·çš„è¿‘ä¼¼å®ç°æ›¿æ¢æ‰æˆ–è’¸é¦ä¸€ä¸‹ã€‚

4.ä¼˜åŒ–æ‰©æ•£é‡‡æ ·ç®—æ³•ï¼ŒåŸºäºæ‰©æ•£çš„æ•°å­¦å…ˆéªŒï¼ŒåŠ é€Ÿæ±‚è§£

æ‰©æ•£é‡‡æ ·éœ€è¦å¤šæ­¥é‡‡æ ·æ‰èƒ½è¾¾åˆ°ç†æƒ³çš„è§†è§‰æ•ˆæœï¼Œæ”¹è¿›é‡‡æ ·ç®—æ³•ï¼Œå‡å°‘é‡‡æ ·æ­¥æ•°ï¼Œæ˜¯ä¸ªéå¸¸ç†æƒ³çš„ä¼˜åŒ–æ–¹å‘ï¼Œä½†æ˜¯å¹¶æ²¡æœ‰é‚£ä¹ˆå®¹æ˜“åšåˆ°ã€‚

è€Œä¸Šè¡¨ä¸­çš„xformerså°±æ˜¯ä¸€ä¸ªé’ˆå¯¹äº†transformerè¿›è¡Œå†…å­˜ä¼˜åŒ–å’Œè®¡ç®—ä¼˜åŒ–çš„å¼€æºæ–¹æ¡ˆã€‚

å¯¹æ€§èƒ½ä¼˜åŒ–æ„Ÿå…´è¶£çš„æœ‹å‹ï¼Œå»ºè®®å¥½å¥½é˜…è¯»ä¸€ä¸‹xformersçš„å®ç°ã€‚

æ˜¾å­˜æ–¹é¢çš„ä¼˜åŒ–ä¸»è¦ç ”è¯»ä»¥ä¸‹ä¸¤ç¯‡è®ºæ–‡ï¼š

[\[2112.05682v3\] Self-attention Does Not Need O(n^2) Memory](https://arxiv.org/abs/2112.05682v3)

[\[2205.14135v2\] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (arxiv.org)](https://arxiv.org/abs/2205.14135v2)

å¾ˆå¤šå£°ç§°å¯¹stable\_diffusionåšäº†ä¼˜åŒ–çš„å‚å®¶ï¼ŒåŸºæœ¬ä¸Šéƒ½æ˜¯åœ¨æ¨¡å‹è½¬æ¢æˆ–è€…ç¼–è¯‘é˜¶æ®µå°†è¿™ä¸¤ç¯‡è®ºæ–‡çš„æ€è·¯åšäº†åº”ç”¨è€Œå·²ï¼Œæ²¡æœ‰å„å…¬ä¼—å·æ–‡ç« è¯´å¾—é‚£ä¹ˆç„ä¹ã€‚

å¦å¤–è¯´ä¸€å¥ï¼Œå…¬ä¼—å·æ–‡ç« å¾ˆå¤šæ°´åˆ†å’Œå¤¸å¤§ï¼Œçœ‹çœ‹ç¬‘ç¬‘å°±å¥½ï¼Œä¸ç”¨è¾ƒçœŸã€‚

åšä¸»ä¹Ÿåœ¨TensorFlowä¸‹åšäº†ç›¸å…³çš„ç®—æ³•ç§»æ¤å·¥ä½œï¼Œå®ç°èµ·æ¥å¹¶ä¸éº»çƒ¦ã€‚

å½“ç„¶é™¤äº†ä»¥ä¸Šæåˆ°ä¼˜åŒ–æ–¹æ¡ˆä¹‹å¤–è¿˜æœ‰ä¸å°‘æ€è·¯ï¼Œåªæ˜¯å…¶ä¸­æœ‰ä¸€äº›é€šç”¨æ€§ä¸å¼ºæˆ–è€…è¯´å­˜åœ¨ç‰¹å®šçš„å±€é™æ€§ã€‚

ä¼˜åŒ–è¿™ä»¶äº‹ï¼Œè§ä»è§æ™ºã€‚

**è½åœ°éƒ¨ç½²çš„æ—æ—æ€»æ€»**

å¾…æœ‰ç©ºå†è®²è®²éƒ¨ç½²ä½¿ç”¨æ¨¡å‹çš„ä¸€äº›é“é“ã€‚

ä¸»è¦æ¶‰åŠæ¨¡å‹è½¬æ¢ï¼Œç²¾åº¦å¤„ç†ï¼Œé—®é¢˜æ’æŸ¥ã€‚

stable\_diffusion éƒ¨ç½²å’Œä½¿ç”¨ï¼Œ

å±•å¼€è¯´ä¹Ÿç¡®å®ä¸‰è¨€ä¸¤è¯­è¯´ä¸å®Œã€‚

åšä¸»æˆåŠŸå°†stable\_diffusionç§»æ¤åˆ°ä½ç«¯æ‰‹æœºè·‘èµ·æ¥ï¼Œå¹¶ä¸”æˆåŠŸå‡ºå›¾ã€‚

ä¸è¿‡è¿˜æœ‰å¾ˆå¤šå·¥ä½œè¦åšæ‰èƒ½è¾¾åˆ°åœ¨æ™®é€šæ‰‹æœºä¸Šç§’çº§å‡ºå›¾ï¼Œè·¯æ¼«æ¼«å…¶ä¿®è¿œå…®ã€‚

è¿™æ–¹é¢åšä¸»è¿˜åœ¨åŠªåŠ›ï¼Œæ¬¢è¿æœ‰ç›¸å…³ç»éªŒçš„æœ‹å‹ä¸€èµ·æ¢è®¨ã€‚

**å‘å±•å’Œç ”ç©¶çš„å»ºè®®**

ä»0è®­ç»ƒå¤ç°stable\_diffusionçš„å¯è¡Œæ€§ï¼Œæ˜¯å®Œå…¨å¯èƒ½çš„ï¼Œ

åªä¸è¿‡è®­ç»ƒæˆæœ¬ä¸ä¼šä½çš„ï¼Œè¦åšçš„å·¥ä½œä¹Ÿä¸å°‘ï¼Œ

åšä¸»ï¼Œå·²ç»å®ç°äº†ä¸€å¥—å…¨æ–°çš„è®­ç»ƒæ–¹æ¡ˆï¼Œæ”¹è¿›äº†è®­ç»ƒä¸­çš„å„ä¸ªç¯èŠ‚ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨ç®—æ³•ï¼Œéƒ½æ˜¯ç‹¬åˆ›çš„ï¼Œä½†æ˜¯å¥ˆä½•ç®—åŠ›èµ„æºä»ç„¶ä¸å¤Ÿç”¨ï¼Œè‹¥æœ‰æœºæ„æ„¿æ„èµåŠ©ï¼Œæ¬¢è¿æ¥é‚®æ´½è°ˆã€‚

stable\_diffusionåŸæœ¬çš„æ¶æ„éå¸¸çš„å†—ä½™ï¼Œå¹¶ä¸”å¾ˆé‡è›®ï¼Œæ‰€è°“å¤§åŠ›å‡ºå¥‡è¿¹ã€‚

é™¤äº†ä¹‹å‰è¯Ÿç—…è¯´çš„ç”Ÿæˆäººæ‰‹é—®é¢˜ç­‰ï¼Œè¿˜æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ï¼Œ

å¸Œæœ›æœ‰æ›´å¤šçš„æœ‹å‹åŠ å…¥è¿›æ¥ï¼Œä¸€èµ·æ‹¥æŠ±è¶‹åŠ¿ï¼Œæ‹¥æŠ±æœªæ¥ã€‚

è°¨ä»¥æ­¤æ–‡ï¼Œä¸åŒé“äººå£«ï¼Œå…±å‹‰ä¹‹ã€‚

è‹¥æœ‰å…¶ä»–ç›¸å…³é—®é¢˜æˆ–è€…ç›¸å…³æŠ€æœ¯éœ€æ±‚æ¬¢è¿æ¥é‚®è”ç³»ã€‚

é‚®ç®±åœ°å€æ˜¯:Â [gaozhihan@vip.qq.com](mailto:gaozhihan@vip.qq.com)