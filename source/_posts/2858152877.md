---
layout: post
title: "如何真正“不花一分钱”部署一个属于你的大模型"
date: "2023-04-23T01:13:02.867Z"
---
如何真正“不花一分钱”部署一个属于你的大模型
======================

因此，本文是为AI初学者们（包括我自己）编写的保姆级大型模型部署和使用指南。现在正值阿里云免费试用计划，我们可以不花一分钱就可以体验部署自己的大型模型的乐趣。

### 前言

看了那么多chatGPT的文章，作为一名不精通算法的开发，也对大模型心痒痒。但想要部署自己的大模型，且不说没有算法相关的经验了，光是大模型占用的算力资源，手头的个人电脑其实也很难独立部署。就算使用算法压缩后的大模型，部署在个人电脑上，还要忍受极端缓慢的计算速度以及与chatGPT相差甚远的模型效果。

有什么办法能够部署属于我们自己的大模型呢？其实很简单，我们将目标拆解一下：

*   有编程基础：作为一个合格的程序员，这应该是必备素质。
*   有足够的算力资源：专业GPU自己买不起怎么办，阿里云最近推出了”飞天“免费试用计划，AI模型训练，GPU资源免费用！
*   要懂大模型：真的要精通大模型吗？不需要。如果只是简单的模型部署和使用，现在的开源模型部署已经非常简单化，只需要掌握基础的Python知识就可以。

随着chatGPT的火爆，许多开源爱好者涌入AI领域，将许多与大型模型相关的工具进行进一步封装，使得我们这些AI初学者也能够用很少的工作量搭建一个私有大型语言模型。而且，有许多成熟的工具可供我们使用，可以帮助我们进一步使用和微调大型模型。

**因此，本文是为AI初学者们（包括我自己）编写的保姆级大型模型部署和使用指南。现在正值阿里云免费试用计划，我们可以不花一分钱就可以体验部署自己的大型模型的乐趣。**

下图便是我通过阿里云免费申请的PAI平台资源（显卡是Nvidia V100），部署的清华大学chatGLM对话大模型，在网页端和手机端都能直接体验：

电脑端

![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1eba8be361474201b5ca535c29df6bef~tplv-k3u1fbpfcp-watermark.image?)

手机端

![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/260b702093eb4481a0d05f58778008b9~tplv-k3u1fbpfcp-watermark.image?)

下文围绕如何手把手搭建一个大模型Demo展开，文章主要目录：

*   领取阿里云免费使用资源
*   创建并使用PAI平台实例
*   部署清华ChatGLM大模型
*   加餐：免费额度用量查询
*   总结和展望

### 领取阿里云免费使用资源

免费试用活动页

[https://free.aliyun.com/](https://free.aliyun.com/?spm=5176.28055625.J_5831864660.6.e93976f4L2wpLu&scm=20140722.M_9553144.P_154.MO_1802-ID_9553144-MID_9553144-CID_20080-ST_7663-V_1)

只要没有申请过PAI-DSW资源的新老用户皆可申请5000CU的免费额度，3个月内使用。

至于5000CU能用多久，和实际申请实例的性能相关，在下面会讲解。

![](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/34300b8d993d4eb88ea5632ff75a4702~tplv-k3u1fbpfcp-watermark.image?)

### 创建并使用PAI平台实例

官方有PAI-DSW使用教程，教你如何用领取的免费资源搭建一个Stable Diffusion来做AI画图，如果对SD感兴趣，可以按照官方教程实践。

[https://help.aliyun.com/document\_detail/615220.html](https://help.aliyun.com/document_detail/615220.html?spm=5176.28008736.J_6443120770.2.67f83e4d2x8aJH&pipCode=learn&goodsId=960469&scm=20140722.M_960469._.V_1)

我们领取额度后，秒到账。之后在阿里云页面内搜索PAI平台，点击立即开通，开通PAI控制台。

![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d99a05969aec4ac9921af6cfd8aa620c~tplv-k3u1fbpfcp-watermark.image?)

开通时的页面没有截图，其中有一些可选的开通项，比如NAS，比如网关等，可以按照自己需求选取，**比如希望保存自己的模型**，那可以关联NAS资源。我当时没有选其他资源，仅开通了PAI，这样没有额外的收费。

随后进入控制台，创建DSW实例。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/70bc66f3135e434daf32cb593d454e0e~tplv-k3u1fbpfcp-watermark.image?)

这里选取资源，注意选择GPU资源，并选择支持资源包抵扣的资源。比如下图的ecs.gn6v-c8g1.2xlarg。可以看到他们的价格里，写明了每小时消耗的CU，你可以大致计算一下，5000CU可以用多久，ecs.gn6v-c8g1.2xlarg这个型号可以跑333小时，大概连续13天。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4c38dd3f4cd343d28d94fcd0b8889d4f~tplv-k3u1fbpfcp-watermark.image?)

系统可以任意选择，本文为了部署chatGLM，选择pytorch1.12

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/964cdbb0f2a14236bc6e2f90c0905876~tplv-k3u1fbpfcp-watermark.image?)

当然，中间你可以**随时停止机器，就不会继续扣费**。注意，这里的机器，是只有系统盘的，如果停止了机器，挂载的系统盘会被回收，你在上面下载的各种文件，模型，都会回收掉。你重新启动，是新的系统盘，文件需要重新下载。（别问我怎么知道的- -!）

![](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2b79b757de674383b36c7b3a1dce7150~tplv-k3u1fbpfcp-watermark.image?)

创建完成后，点击打开，就进入了交互式的Web页面，可以开始你的模型开发之旅。

### 部署清华ChatGLM大模型

上面已经讲完了资源的申请和实例的创建、使用，之后小伙伴们可以自行发挥，部署自己的大模型（或者任何AI相关资源）。本文后半部分介绍一下我自己折腾部署ChatGLM对话大模型的过程，给完全不了解大模型的小伙伴打个样。

ChatGLM代码仓库：

[https://github.com/THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)

大家完全可以按照官方文档自行部署，忽略我下面的教程。也可以按照我下方的流程来操作，避免再把我踩得坑再踩一遍。

#### 下载模型

由于模型较大（13G左右），我们最好先把模型拉到本地，再运行。

当然，如果你也可以不下载离线模型，直接在运行时拉取模型文件。

模型文件仓库：

[https://huggingface.co/THUDM/chatglm-6b](https://huggingface.co/THUDM/chatglm-6b)

下载模型仓库，需要安装Git LFS（Large File Storage），它用来拉去Git仓库中的大型文件，比如这次用到的模型bin文件。

![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/56fc0a32280f4292aa4cd3577b28c218~tplv-k3u1fbpfcp-watermark.image?)

由于我们申请的系统正好是Ubuntu，在Ubuntu下安装Git LFS，可以使用如下命令：

sudo apt-get update

sudo apt-get install git-lfs

完成后，克隆模型仓库：

git clone [https://huggingface.co/THUDM/chatglm-6b](https://huggingface.co/THUDM/chatglm-6b)

有可能网络波动，会导致拉取卡住，可以手动停止掉，然后进入文件夹内：

git lfs pull

这样会拉取到上图红框中LFS文件。

#### 部署模型

完成模型下载后，我们下载模型运行代码：

git clone [https://github.com/THUDM/ChatGLM-6B.git](https://github.com/THUDM/ChatGLM-6B.git)

进入文件夹，创建python的虚拟环境：

virtualenv -p /usr/bin/python3.8 venv

激活Python虚拟环境：

source venv/bin/activate

如果遇到如下的报错，需要额外安装一下python3-dev：

Could not build wheels for pycocotools, which is required to install pyproject.toml-based projects

error: command '/usr/bin/x86\_64-linux-gnu-gcc' failed with exit code 1

sudo apt-get install python3-dev

由于我们本地提前下载好了模型，我们修改一下代码，使用本地模型的路径。

将使用到的代码中，路径都从THUDM/chatglm-6b改为你的路径，比如我们的模型下载在/mnt/workspace/chatglm-6b，我们将两种部署模型的主代码文件都修改一下，都修改为如下图所示的路径：

*   cli\_demo.py: 命令行交互界面
*   web\_demo.py:Web图形交互界面

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/59948152ecac4f45a9a931f2cd73a420~tplv-k3u1fbpfcp-watermark.image?)

我们直接用官方提供的网页版Demo来跑模型，启动web\_demo.py，开始模型部署，首先会将模型读入内存，占用约13个G。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4e1fe361222a4f6a937582ea7d86b663~tplv-k3u1fbpfcp-watermark.image?)

完成后，内存会被释放，开始占用GPU显存13g。

![](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a5e616a2ffd14de1a4c8c376bdc75d08~tplv-k3u1fbpfcp-watermark.image?)

最终，部署完成会得到服务公网和本地网址。

![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5e1a1c7e641a490ebf382869076644b4~tplv-k3u1fbpfcp-watermark.image?)

#### 对话效果展示

回答还是比较快的，在我们V100显卡的机器上，提问后1-2秒内就会开始回答问题。

![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2cddf00703a04e3587c11fae017b3dfd~tplv-k3u1fbpfcp-watermark.image?)

**以下是一个完整的对话：**

我：写一个Java单例模式代码

ChatGLM：以下是一个简单的Java单例模式代码示例：

    public class Singleton {
        private static final Singleton INSTANCE = new Singleton();
        private Singleton() {}
        public static Singleton getInstance() {
            return INSTANCE;
        }
    }
    

在这个示例中，Singleton 类是一个静态类，它有一个静态成员变量 INSTANCE，它初始化为一个单例对象。getInstance 方法返回这个单例对象的实例，可以像任何其他方法一样使用。  
单例对象通常被用于确保一个类只有一个实例，并提供全局访问点。使用 getInstance 方法可以方便地获取单例对象，而无需担心多个线程同时访问同一个对象。  
需要注意的是，在实际应用中，单例对象应该尽量避免被频繁创建和销毁，因为这样会增加内存开销和维护成本。因此，可以考虑使用懒加载(lazy loading)的方式来避免频繁创建单例对象。

我：可以换一个方式实现吗，比如懒汉式？

ChatGLM：当然可以，懒汉式单例模式的具体实现方式如下：

1.  将 Singleton 类做成一个接口。
2.  实现 Singleton 接口的类可以是多个，但它们都应该使用 getInstance 方法来获取单例对象，并返回一个指向该对象的引用。
3.  实现 Singleton 接口的类中，可以有一个内部类来负责创建单例对象。
4.  内部类应该实现 static 关键字，并使用 synchronized 关键字实现线程安全。
5.  内部类应该覆盖 getInstance 方法，以实现懒加载。例如，可以创建一个静态变量来存储单例对象，并在需要时才创建该变量。
6.  在需要使用单例对象的地方，可以调用内部类的方法，来获取单例对象。  
    以下是一个懒汉式单例模式的示例代码：

    public interface Singleton {
        static Singleton INSTANCE;
    }
    public class ConcreteSingleton implements Singleton {
        private static Singleton INSTANCE = new ConcreteSingleton();
        @Override
        public Singleton getInstance() {
            return INSTANCE;
        }
    }
    public class LazySingleton implements Singleton {
        private static Singleton INSTANCE;
        private Singleton() {}
        public static Singleton getInstance() {
            synchronized (LazySingleton.class) {
                return INSTANCE;
            }
        }
    }
    

在这个示例中，ConcreteSingleton 类使用 getInstance 方法来创建单例对象，并使用 synchronized 关键字实现线程安全。LazySingleton 类使用 synchronized 关键字实现懒加载，只有在需要时才创建单例对象。  
需要注意的是，懒汉式单例模式仍然需要保证线程安全，因此在使用懒加载时，需要考虑线程安全问题。此外，懒汉式单例模式可能无法充分利用类的空间，因为它需要使用共享对象来实现懒加载，因此需要考虑如何提高类的性能和空间利用率。

### 查看免费额度使用量

免费的5000CU额度是有三个月时效的，超过三个月会直接过期。想要做到心中有数，防止超过免费额度导致扣费。我们可以查看免费额度的用量。在页面右上角有“费用”标签，选择“资源实例管理”，点击“资源包”，能看到你的免费资源包的用量。

![](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4951159374ad424c915f6336c2b4dc7b~tplv-k3u1fbpfcp-watermark.image?)

点击每行中的统计，可以看到是哪个实例在消耗CU。

![](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a6fccbeaef2c41268d76b0dd51b88648~tplv-k3u1fbpfcp-watermark.image?)

### 总结

整个部署流程，我自己折腾下来也只花了半天时间,就能够搭建好一个完整、可用的Demo。

不得不说，当AI逐渐破圈，成为一个风口时，真的是猪都会飞，普通程序员上手AI的难度瞬间被拉低了一个数量级。开源开发者的不断贡献让各种工具和文档变得更容易上手。每天数以万计的大模型相关issue和pr，让Github遇到了久违的繁荣。

在AI席卷全球的这一刻，作为一名程序员，我们无疑是对这个时代更有体感的那群人。此时此刻，非我莫属。要充分关注AI技术的应用场景和发展趋势，积极探索AI与其他领域的结合，为自己的职业发展和未来规划提供更多的可能性。