---
layout: post
title: "Falcon ç™»é™† Hugging Face ç”Ÿæ€"
date: "2023-06-17T01:10:33.294Z"
---
Falcon ç™»é™† Hugging Face ç”Ÿæ€
=========================

å¼•è¨€
--

Falcon æ˜¯ç”±ä½äºŽé˜¿å¸ƒæ‰Žæ¯”çš„ [æŠ€æœ¯åˆ›æ–°ç ”ç©¶é™¢ (Technology Innovation Instituteï¼ŒTII)](https://www.tii.ae/) åˆ›å»ºçš„ä¸€ç³»åˆ—çš„æ–°è¯­è¨€æ¨¡åž‹ï¼Œå…¶åŸºäºŽ Apache 2.0 è®¸å¯å‘å¸ƒã€‚ **å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ[Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) æ˜¯é¦–ä¸ªâ€œçœŸæ­£å¼€æ”¾â€çš„æ¨¡åž‹ï¼Œå…¶èƒ½åŠ›å¯ä¸Žå½“å‰è®¸å¤šé—­æºæ¨¡åž‹ç›¸åª²ç¾Ž**ã€‚è¿™å¯¹ä»Žä¸šè€…ã€çˆ±å¥½è€…å’Œè¡Œä¸šæ¥è¯´éƒ½æ˜¯ä¸ªå¥½æ¶ˆæ¯ï¼Œå› ä¸ºâ€œçœŸå¼€æºâ€ä½¿å¤§å®¶å¯ä»¥æ¯«æ— é¡¾å¿Œåœ°åŸºäºŽå®ƒä»¬æŽ¢ç´¢ç™¾èŠ±é½æ”¾çš„åº”ç”¨ã€‚

æœ¬æ–‡ï¼Œæˆ‘ä»¬å°†æ·±å…¥æŽ¢è®¨ Falcon æ¨¡åž‹: é¦–å…ˆæŽ¢è®¨å®ƒä»¬çš„ç‹¬ç‰¹ä¹‹å¤„ï¼Œç„¶åŽ **å±•ç¤ºå¦‚ä½•åŸºäºŽ Hugging Face ç”Ÿæ€æä¾›çš„å·¥å…·è½»æ¾æž„å»ºåŸºäºŽ Falcon æ¨¡åž‹çš„å¤šç§åº”ç”¨ (å¦‚æŽ¨ç†ã€é‡åŒ–ã€å¾®è°ƒç­‰)** ã€‚

ç›®å½•
--

*   [Falcon æ¨¡åž‹](#Falcon-%E6%A8%A1%E5%9E%8B)
*   [æ¼”ç¤º](#%E6%BC%94%E7%A4%BA)
*   [æŽ¨ç†](#%E6%8E%A8%E7%90%86)
*   [è¯„ä¼°](#%E8%AF%84%E4%BC%B0)
*   [ç”¨ PEFT å¾®è°ƒæ¨¡åž‹](#%E7%94%A8-PEFT-%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B)
*   [æ€»ç»“](#%E6%80%BB%E7%BB%93)

Falcon æ¨¡åž‹
---------

Falcon å®¶æ—æœ‰ä¸¤ä¸ªåŸºç¡€æ¨¡åž‹: [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) åŠå…¶å°å…„å¼Ÿ [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b)ã€‚ **40B å‚æ•°æ¨¡åž‹ç›®å‰åœ¨ [Open LLM æŽ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) ä¸­ååˆ—å‰èŒ…ï¼Œè€Œ 7B æ¨¡åž‹åœ¨åŒç­‰å‚æ•°é‡çš„æ¨¡åž‹ä¸­è¡¨çŽ°æœ€ä½³**ã€‚

è¿è¡Œ Falcon-40B éœ€è¦çº¦ 90GB çš„ GPU æ˜¾å­˜ â€”â€” è™½ç„¶è¿˜æ˜¯æŒºå¤šçš„ï¼Œä½†æ¯” LLaMA-65B å°‘äº†ä¸å°‘ï¼Œå†µä¸” Falcon-40B çš„æ€§èƒ½è¿˜ä¼˜äºŽ LLaMA-65Bã€‚è€Œ Falcon-7B åªéœ€è¦çº¦ 15GB æ˜¾å­˜ï¼Œå³ä½¿åœ¨æ¶ˆè´¹ç±»ç¡¬ä»¶ä¸Šä¹Ÿå¯ä»¥è¿›è¡ŒæŽ¨ç†å’Œå¾®è°ƒã€‚ _(æˆ‘ä»¬å°†åœ¨åŽæ–‡è®¨è®ºå¦‚ä½•ä½¿ç”¨é‡åŒ–æŠ€æœ¯åœ¨ä¾¿å®œçš„ GPU ä¸Šä½¿ç”¨ Falcon-40Bï¼)_

TII è¿˜æä¾›äº†ç»è¿‡æŒ‡ä»¤å¾®è°ƒçš„æ¨¡åž‹: [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) ä»¥åŠ [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)ã€‚è¿™ä¸¤ä¸ªå®žéªŒæ€§çš„æ¨¡åž‹å˜ä½“ç»ç”±æŒ‡ä»¤å’Œå¯¹è¯æ•°æ®å¾®è°ƒè€Œå¾—ï¼Œå› æ­¤æ›´é€‚åˆå½“å‰æµè¡Œçš„åŠ©ç†å¼ä»»åŠ¡ã€‚ **å¦‚æžœä½ åªæ˜¯æƒ³æŠŠ Falcon æ¨¡åž‹å¿«é€Ÿç”¨èµ·æ¥ï¼Œè¿™ä¸¤ä¸ªæ¨¡åž‹æ˜¯æœ€ä½³é€‰æ‹©ã€‚** å½“ç„¶ä½ ä¹Ÿå¯ä»¥åŸºäºŽç¤¾åŒºæž„å»ºçš„å¤§é‡æ•°æ®é›†å¾®è°ƒä¸€ä¸ªè‡ªå·±çš„æ¨¡åž‹ â€”â€” åŽæ–‡ä¼šç»™å‡ºå¾®è°ƒæ­¥éª¤ï¼

Falcon-7B å’Œ Falcon-40B åˆ†åˆ«åŸºäºŽ 1.5 ä¸‡äº¿å’Œ 1 ä¸‡äº¿è¯å…ƒæ•°æ®è®­ç»ƒè€Œå¾—ï¼Œå…¶æž¶æž„åœ¨è®¾è®¡æ—¶å°±å……åˆ†è€ƒè™‘äº†æŽ¨ç†ä¼˜åŒ–ã€‚ **Falcon æ¨¡åž‹è´¨é‡è¾ƒé«˜çš„å…³é”®åœ¨äºŽè®­ç»ƒæ•°æ®ï¼Œå…¶ 80% ä»¥ä¸Šçš„è®­ç»ƒæ•°æ®æ¥è‡ªäºŽ [RefinedWeb](https://arxiv.org/abs/2306.01116) â€”â€” ä¸€ä¸ªæ–°çš„åŸºäºŽ CommonCrawl çš„ç½‘ç»œæ•°æ®é›†**ã€‚ TII é€‰æ‹©ä¸åŽ»æ”¶é›†åˆ†æ•£çš„ç²¾é€‰æ•°æ®ï¼Œè€Œæ˜¯ä¸“æ³¨äºŽæ‰©å±•å¹¶æé«˜ Web æ•°æ®çš„è´¨é‡ï¼Œé€šè¿‡å¤§é‡çš„åŽ»é‡å’Œä¸¥æ ¼è¿‡æ»¤ä½¿æ‰€å¾—è¯­æ–™åº“ä¸Žå…¶ä»–ç²¾é€‰çš„è¯­æ–™åº“è´¨é‡ç›¸å½“ã€‚ åœ¨è®­ç»ƒ Falcon æ¨¡åž‹æ—¶ï¼Œè™½ç„¶ä»ç„¶åŒ…å«äº†ä¸€äº›ç²¾é€‰æ•°æ® (ä¾‹å¦‚æ¥è‡ª Reddit çš„å¯¹è¯æ•°æ®)ï¼Œä½†ä¸Ž GPT-3 æˆ– PaLM ç­‰æœ€å…ˆè¿›çš„ LLM ç›¸æ¯”ï¼Œç²¾é€‰æ•°æ®çš„ä½¿ç”¨é‡è¦å°‘å¾—å¤šã€‚ä½ çŸ¥é“æœ€å¦™çš„æ˜¯ä»€ä¹ˆå—ï¼Ÿ TII å…¬å¸ƒäº†ä»Ž [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) ä¸­æå–å‡ºçš„å«æœ‰ 6000 äº¿è¯å…ƒçš„æ•°æ®é›†ï¼Œä»¥ä¾›ç¤¾åŒºåœ¨è‡ªå·±çš„ LLM ä¸­ä½¿ç”¨ï¼

Falcon æ¨¡åž‹çš„å¦ä¸€ä¸ªæœ‰è¶£çš„ç‰¹æ€§æ˜¯å…¶ä½¿ç”¨äº† [**å¤šæŸ¥è¯¢æ³¨æ„åŠ› (multiquery attention)**](https://arxiv.org/abs/1911.02150)ã€‚åŽŸå§‹å¤šå¤´ (head) æ³¨æ„åŠ›æ–¹æ¡ˆæ¯ä¸ªå¤´éƒ½åˆ†åˆ«æœ‰ä¸€ä¸ªæŸ¥è¯¢ (query) ã€é”® (key) ä»¥åŠå€¼ (value)ï¼Œè€Œå¤šæŸ¥è¯¢æ³¨æ„åŠ›æ–¹æ¡ˆæ”¹ä¸ºåœ¨æ‰€æœ‰å¤´ä¸Šå…±äº«åŒä¸€ä¸ªé”®å’Œå€¼ã€‚

![mqa](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202306131320489.png)

**å¤šæŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶åœ¨æ³¨æ„åŠ›å¤´ä¹‹é—´å…±äº«åŒä¸€ä¸ªé”®åµŒå…¥å’Œå€¼åµŒå…¥ã€‚å›¾ç‰‡ç”± Harm de Vries æä¾›ã€‚**

è¿™ä¸ªæŠ€å·§å¯¹é¢„è®­ç»ƒå½±å“ä¸å¤§ï¼Œä½†å®ƒæžå¤§åœ° [æé«˜äº†æŽ¨ç†çš„å¯æ‰©å±•æ€§](https://arxiv.org/abs/2211.05102): äº‹å®žä¸Šï¼Œ **è¯¥æŠ€å·§å¤§å¤§å‡å°‘äº†è‡ªå›žå½’è§£ç æœŸé—´ K,V ç¼“å­˜çš„å†…å­˜å ç”¨ï¼Œå°†å…¶å‡å°‘äº† 10-100 å€** (å…·ä½“æ•°å€¼å–å†³äºŽæ¨¡åž‹æž¶æž„çš„é…ç½®)ï¼Œè¿™å¤§å¤§é™ä½Žäº†æ¨¡åž‹æŽ¨ç†çš„å†…å­˜å¼€é”€ã€‚è€Œå†…å­˜å¼€é”€çš„å‡å°‘ä¸ºè§£é”æ–°çš„ä¼˜åŒ–å¸¦æ¥äº†å¯èƒ½ï¼Œå¦‚çœä¸‹æ¥çš„å†…å­˜å¯ä»¥ç”¨æ¥å­˜å‚¨åŽ†å²å¯¹è¯ï¼Œä»Žè€Œä½¿å¾—æœ‰çŠ¶æ€æŽ¨ç†æˆä¸ºå¯èƒ½ã€‚

æ¨¡åž‹

è®¸å¯

èƒ½å¦å•†ç”¨ï¼Ÿ

é¢„è®­ç»ƒè¯å…ƒæ•°

é¢„è®­ç»ƒç®—åŠ› \[PF-å¤©\]

æŽ’è¡Œæ¦œå¾—åˆ†

K,V ç¼“å­˜å¤§å° (ä¸Šä¸‹æ–‡é•¿åº¦ä¸º 2048)

StableLM-Alpha-7B

CC-BY-SA-4.0

âœ…

1,500B

700

38.3\*

800MB

LLaMA-7B

LLaMA license

âŒ

1,000B

500

47.6

1,100MB

MPT-7B

Apache 2.0

âœ…

1,000B

500

48.6

1,100MB

Falcon-7B

Apache 2.0

âœ…

1,500B

700

48.8

20MB

LLaMA-33B

LLaMA license

âŒ

1,500B

3200

56.9

3,300MB

LLaMA-65B

LLaMA license

âŒ

1,500B

6300

58.3

5,400MB

Falcon-40B

Apache 2.0

âœ…

1,000B

2800

60.4

240MB

*   _ä¸Šè¡¨ä¸­å¾—åˆ†å‡ä¸ºç»è¿‡å¾®è°ƒçš„æ¨¡åž‹çš„å¾—åˆ†_

æ¼”ç¤º
==

é€šè¿‡ [è¿™ä¸ª Space](https://huggingface.co/spaces/HuggingFaceH4/falcon-chat) æˆ–ä¸‹é¢çš„åº”ç”¨ï¼Œä½ å¯ä»¥å¾ˆè½»æ¾åœ°è¯•ç”¨ä¸€ä¸‹å¤§çš„ Falcon æ¨¡åž‹ (400 äº¿å‚æ•°ï¼):

![](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202306131319797.png)  
è¯·ç‚¹å‡» **é˜…è¯»åŽŸæ–‡** æŸ¥çœ‹äº¤äº’å¼ç¤ºä¾‹ã€‚

ä¸Šé¢çš„åº”ç”¨ä½¿ç”¨äº† Hugging Face çš„ [Text Generation Inference](https://github.com/huggingface/text-generation-inference) æŠ€æœ¯ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„ã€å¿«é€Ÿé«˜æ•ˆçš„æ–‡æœ¬ç”ŸæˆæœåŠ¡ï¼Œä½¿ç”¨äº† Rustã€Python ä»¥åŠ gRPC ç­‰æŠ€æœ¯ã€‚[HuggingChat](https://huggingface.co/chat/) ä¹Ÿä½¿ç”¨äº†ç›¸åŒçš„æŠ€æœ¯ã€‚

æˆ‘ä»¬è¿˜æž„å»ºäº†ä¸€ä¸ª Core ML ç‰ˆæœ¬çš„ `falcon-7b-instruct` æ¨¡åž‹ï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å°†å…¶è¿è¡Œè‡³ M1 MacBook Pro:

![](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202306131326214.png)  
è¯·ç‚¹å‡» **é˜…è¯»åŽŸæ–‡** è§‚çœ‹è§†é¢‘ã€‚

è¯¥è§†é¢‘å±•ç¤ºäº†ä¸€ä¸ªè½»é‡çº§åº”ç”¨ç¨‹åºï¼Œè¯¥åº”ç”¨ç¨‹åºåˆ©ç”¨ä¸€ä¸ª Swift åº“å®Œæˆäº†åŒ…æ‹¬åŠ è½½æ¨¡åž‹ã€åˆ†è¯ã€å‡†å¤‡è¾“å…¥æ•°æ®ã€ç”Ÿæˆæ–‡æœ¬ä»¥åŠè§£ç åœ¨å†…çš„å¾ˆå¤šç¹é‡çš„æ“ä½œã€‚æˆ‘ä»¬æ­£åœ¨å¿«é©¬åŠ éž­æž„å»ºè¿™ä¸ªåº“ï¼Œè¿™æ ·å¼€å‘äººå‘˜å°±èƒ½åŸºäºŽå®ƒå°†å¼ºå¤§çš„ LLM é›†æˆåˆ°å„ç§åº”ç”¨ç¨‹åºä¸­ï¼Œè€Œæ— éœ€é‡æ–°å‘æ˜Žè½®å­ã€‚ç›®å‰å®ƒè¿˜æœ‰ç‚¹ç²—ç³™ï¼Œä½†æˆ‘ä»¬è¿«ä¸åŠå¾…åœ°æƒ³è®©å®ƒæ—©ç‚¹é¢ä¸–ã€‚åŒæ—¶ï¼Œä½ ä¹Ÿå¯ä»¥ä¸‹è½½ [Core ML çš„æƒé‡æ–‡ä»¶](https://huggingface.co/tiiuae/falcon-7b-instruct/tree/main/coreml/text-generation) è‡ªå·±æŽ¢ç´¢ï¼

æŽ¨ç†
==

åœ¨ä½¿ç”¨ç†Ÿæ‚‰çš„ transformers API åœ¨ä½ è‡ªå·±çš„ç¡¬ä»¶ä¸Šè¿è¡Œ Falcon æ¨¡åž‹æ—¶ï¼Œä½ éœ€è¦æ³¨æ„å‡ ä¸ªä»¥ä¸‹ç»†èŠ‚:

*   çŽ°æœ‰çš„æ¨¡åž‹æ˜¯ç”¨ `bfloat16` æ•°æ®ç±»åž‹è®­ç»ƒçš„ï¼Œå› æ­¤å»ºè®®ä½ ä¹Ÿä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»åž‹æ¥æŽ¨ç†ã€‚ä½¿ç”¨ `bfloat16` éœ€è¦ä½ å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ CUDAï¼Œè€Œä¸” `bfloat16` åœ¨æœ€æ–°çš„å¡ (å¦‚ A100) ä¸Šæ•ˆæžœæœ€å¥½ã€‚ä½ ä¹Ÿå¯ä»¥å°è¯•ä½¿ç”¨ `float16` è¿›è¡ŒæŽ¨ç†ï¼Œä½†è¯·è®°ä½ï¼Œç›®å‰æˆ‘ä»¬åˆ†äº«çš„æ¨¡åž‹æ•ˆæžœæ•°æ®éƒ½æ˜¯åŸºäºŽ `bfloat16` çš„ã€‚
*   ä½ éœ€è¦å…è®¸è¿œç¨‹ä»£ç æ‰§è¡Œã€‚è¿™æ˜¯å› ä¸º `transformers` å°šæœªé›†æˆ Falcon æ¨¡åž‹æž¶æž„ï¼Œæ‰€ä»¥ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨æ¨¡åž‹ä½œè€…åœ¨å…¶ä»£ç åº“ä¸­æä¾›çš„ä»£ç æ¥è¿è¡Œã€‚ä»¥ `falcon-7b-instruct` ä¸ºä¾‹ï¼Œå¦‚æžœä½ å…è®¸è¿œç¨‹æ‰§è¡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸‹åˆ—æ–‡ä»¶é‡Œçš„ä»£ç æ¥è¿è¡Œæ¨¡åž‹: [configuration\_RW.py](https://huggingface.co/tiiuae/falcon-7b-instruct/blob/main/configuration_RW.py)ï¼Œ[modelling\_RW.py](https://huggingface.co/tiiuae/falcon-7b-instruct/blob/main/modelling_RW.py)ã€‚

ç»¼ä¸Šï¼Œä½ å¯ä»¥å‚è€ƒå¦‚ä¸‹ä»£ç æ¥ä½¿ç”¨ transformers çš„ `pipeline` API åŠ è½½ `falcon-7b-instruct` æ¨¡åž‹:

    from transformers import AutoTokenizer
    import transformers
    import torch
    
    model = "tiiuae/falcon-7b-instruct"
    
    tokenizer = AutoTokenizer.from_pretrained(model)
    pipeline = transformers.pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        device_map="auto",
    )
    
    

ç„¶åŽï¼Œå†ç”¨å¦‚ä¸‹ä»£ç ç”Ÿæˆæ–‡æœ¬:

    sequences = pipeline(
       "Write a poem about Valencia.",
        max_length=200,
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
    )
    for seq in sequences:
        print(f"Result: {seq['generated_text']}")
    
    

æœ€åŽï¼Œä½ å¯èƒ½ä¼šå¾—åˆ°å¦‚ä¸‹è¾“å‡º:

    Valencia, city of the sun
    The city that glitters like a star
    A city of a thousand colors
    Where the night is illuminated by stars
    Valencia, the city of my heart
    Where the past is kept in a golden chest
    
    

### å¯¹ Falcon 40B è¿›è¡ŒæŽ¨ç†

å› ä¸º 40B æ¨¡åž‹å°ºå¯¸æ¯”è¾ƒå¤§ï¼Œæ‰€ä»¥è¦æŠŠå®ƒè¿è¡Œèµ·æ¥è¿˜æ˜¯æŒºæœ‰æŒ‘æˆ˜æ€§çš„ï¼Œå•ä¸ªæ˜¾å­˜ä¸º 80GB çš„ A100 éƒ½æ”¾ä¸ä¸‹å®ƒã€‚å¦‚æžœç”¨ 8 æ¯”ç‰¹æ¨¡åž‹çš„è¯ï¼Œéœ€è¦å¤§çº¦ 45GB çš„ç©ºé—´ï¼Œæ­¤æ—¶ A6000 (48GB) èƒ½æ”¾ä¸‹ä½† 40GB çš„ A100 è¿˜æ˜¯æ”¾ä¸ä¸‹ã€‚ç›¸åº”çš„æŽ¨ç†ä»£ç å¦‚ä¸‹:

    from transformers import AutoTokenizer, AutoModelForCausalLM
    import transformers
    import torch
    
    model_id = "tiiuae/falcon-40b-instruct"
    
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        load_in_8bit=True,
        device_map="auto",
    )
    
    pipeline = transformers.pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
    )
    

éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒINT8 æ··åˆç²¾åº¦æŽ¨ç†ä½¿ç”¨çš„æµ®ç‚¹ç²¾åº¦æ˜¯ `torch.float16` è€Œä¸æ˜¯ `torch.bfloat16`ï¼Œå› æ­¤è¯·åŠ¡å¿…è¯¦å°½åœ°å¯¹ç»“æžœè¿›è¡Œæµ‹è¯•ã€‚

å¦‚æžœä½ æœ‰å¤šå¼  GPU å¡å¹¶å®‰è£…äº† `accelerate`ï¼Œä½ è¿˜å¯ä»¥ç”¨ `device_map="auto"` å°†æ¨¡åž‹çš„å„å±‚è‡ªåŠ¨åˆ†å¸ƒåˆ°å¤šå¼ å¡ä¸Šè¿è¡Œã€‚å¦‚æœ‰å¿…è¦ï¼Œç”šè‡³å¯ä»¥å°†æŸäº›å±‚å¸è½½åˆ° CPUï¼Œä½†è¿™ä¼šå½±å“æŽ¨ç†é€Ÿåº¦ã€‚

åœ¨æœ€æ–°ç‰ˆæœ¬çš„ `bitsandbytes`ã€`transformers` ä»¥åŠ `accelerate` ä¸­æˆ‘ä»¬è¿˜æ”¯æŒäº† [4 æ¯”ç‰¹åŠ è½½](https://huggingface.co/blog/4bit-transformers-bitsandbytes)ã€‚æ­¤æ—¶ï¼Œ40B æ¨¡åž‹ä»…éœ€çº¦ 27GB çš„æ˜¾å­˜å°±èƒ½è¿è¡Œã€‚è™½ç„¶è¿™ä¸ªéœ€æ±‚è¿˜æ˜¯æ¯” 3090 æˆ– 4090 è¿™äº›å¡æ‰€èƒ½æä¾›çš„æ˜¾å­˜å¤§ï¼Œä½†å·²ç»è¶³ä»¥åœ¨æ˜¾å­˜ä¸º 30GB æˆ– 40GB çš„å¡ä¸Šè¿è¡Œäº†ã€‚

### Text Generation Inference

[Text Generation Inference](https://github.com/huggingface/text-generation-inference) æ˜¯ Hugging Face å¼€å‘çš„ä¸€ä¸ªå¯ç”¨äºŽç”Ÿäº§çš„æŽ¨ç†å®¹å™¨ã€‚æœ‰äº†å®ƒï¼Œç”¨æˆ·å¯ä»¥è½»æ¾éƒ¨ç½²å¤§è¯­è¨€æ¨¡åž‹ã€‚

å…¶ä¸»è¦ç‰¹ç‚¹æœ‰:

*   å¯¹è¾“å…¥è¿›è¡Œæµå¼ batch ç»„è£… (batching)
*   æµå¼ç”Ÿæˆè¯ï¼Œä¸»è¦åŸºäºŽ SSE åè®® (Server-Sent Eventsï¼ŒSSE)
*   æŽ¨ç†æ—¶æ”¯æŒå¤š GPU å¼ é‡å¹¶è¡Œ (Tensor Parallelism )ï¼ŒæŽ¨ç†é€Ÿåº¦æ›´å¿«
*   transformers æ¨¡åž‹ä»£ç ç”±å®šåˆ¶ CUDA æ ¸å‡½æ•°æ·±åº¦ä¼˜åŒ–
*   åŸºäºŽ Prometheus å’Œ Open Telemetry çš„äº§å“çº§æ—¥å¿—è®°å½•ã€ç›‘æŽ§å’Œè·Ÿè¸ªæœºåˆ¶

ä»Ž v0.8.2 èµ·ï¼ŒText Generation Inference åŽŸç”Ÿæ”¯æŒ Falcon 7b å’Œ 40b æ¨¡åž‹ï¼Œè€Œæ— éœ€ä¾èµ– transformers çš„ `â€œä¿¡ä»»è¿œç¨‹ä»£ç  (trust remote code)â€` åŠŸèƒ½ã€‚å› æ­¤ï¼ŒText Generation Inference å¯ä»¥æ”¯æŒå¯†é—­éƒ¨ç½²åŠå®‰å…¨å®¡è®¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ Falcon æ¨¡åž‹çš„å®žçŽ°ä¸­åŠ å…¥äº†å®šåˆ¶ CUDA æ ¸å‡½æ•°ä¼˜åŒ–ï¼Œè¿™å¯æ˜¾è‘—é™ä½ŽæŽ¨ç†çš„ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚

![tgi-hfe-screenshot.png](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202306131321255.png)

**Hugging Face Inference Endpoint çŽ°å·²æ”¯æŒ Text Generation Inferenceã€‚ä½ å¯ä»¥åœ¨å•å¼  A100 ä¸Šè½»æ¾éƒ¨ç½² `falcon-40b-instruct` çš„ Int8 é‡åŒ–æ¨¡åž‹ã€‚**

Text Generation Inference çŽ°å·²é›†æˆè‡³ Hugging Face çš„ [Inference Endpoint](https://huggingface.co/inference-endpoints)ã€‚æƒ³è¦éƒ¨ç½² Falcon æ¨¡åž‹ï¼Œå¯è‡³ [æ¨¡åž‹é¡µé¢](https://huggingface.co/tiiuae/falcon-7b-instruct) å¹¶ç‚¹å‡» [Deploy -> Inference Endpoints](https://ui.endpoints.huggingface.co/new?repository=tiiuae/falcon-7b-instruct) æŒ‰é’®ã€‚

å¦‚éœ€éƒ¨ç½² 7B æ¨¡åž‹ï¼Œå»ºè®®é€‰æ‹© â€œGPU \[medium\] - 1x Nvidia A10Gâ€ã€‚

å¦‚éœ€éƒ¨ç½² 40B æ¨¡åž‹ï¼Œä½ éœ€è¦åœ¨ â€œGPU \[xlarge\] - 1x Nvidia A100â€ ä¸Šéƒ¨ç½²ä¸”éœ€è¦å¼€å¯é‡åŒ–åŠŸèƒ½ï¼Œè·¯å¾„å¦‚ä¸‹:  
`Advanced configuration -> Serving Container -> Int-8 Quantization`

_æ³¨æ„: åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œå¦‚æžœä½ éœ€è¦å‡çº§é…é¢ï¼Œå¯ç›´æŽ¥å‘ç”µå­é‚®ä»¶è‡³ api-enterprise@huggingface.co ç”³è¯·ã€‚_

è¯„ä¼°
--

é‚£ä¹ˆ Falcon æ¨¡åž‹ç©¶ç«Ÿæ•ˆæžœå¦‚ä½•ï¼Ÿ Falcon çš„ä½œè€…ä»¬é©¬ä¸Šå°†ä¼šå‘å¸ƒä¸€ä¸ªæ·±å…¥çš„è¯„ä¼°æ•°æ®ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä»…åœ¨æˆ‘ä»¬çš„ [Open LLM æŽ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) ä¸Šå¯¹ Falcon åŸºç¡€æ¨¡åž‹å’ŒæŒ‡ä»¤æ¨¡åž‹è¿›è¡Œä¸€ä¸ªåˆæ­¥è¯„ä¼°ã€‚ `Open LLM æŽ’è¡Œæ¦œ`ä¸»è¦è¡¡é‡ LLM çš„æŽ¨ç†èƒ½åŠ›åŠå…¶å›žç­”ä»¥ä¸‹å‡ ä¸ªé¢†åŸŸçš„é—®é¢˜çš„èƒ½åŠ›:

*   [AI2 æŽ¨ç†æŒ‘æˆ˜](https://allenai.org/data/arc) (ARC): å°å­¦ç¨‹åº¦æœ‰å…³ç§‘å­¦çš„é€‰æ‹©é¢˜ã€‚
*   [HellaSwag](https://arxiv.org/abs/1905.07830): å›´ç»•æ—¥å¸¸äº‹ä»¶çš„å¸¸è¯†æ€§é—®é¢˜ã€‚
*   [MMLU](https://github.com/hendrycks/test): 57 ä¸ªç§‘ç›® (åŒ…å«èŒä¸šç§‘ç›®åŠå­¦æœ¯ç§‘ç›®) çš„é€‰æ‹©é¢˜ã€‚
*   [TruthfulQA](https://arxiv.org/abs/2109.07958): æµ‹è¯•æ¨¡åž‹ä»Žä¸€ç»„é”™è¯¯é™ˆè¿°ä¸­æ‰¾å‡ºäº‹å®žæ€§é™ˆè¿°çš„èƒ½åŠ›ã€‚

ç»“æžœæ˜¾ç¤ºï¼Œ40B åŸºç¡€æ¨¡åž‹å’ŒæŒ‡ä»¤æ¨¡åž‹éƒ½éžå¸¸å¼ºï¼Œç›®å‰åœ¨ [Open LLM æŽ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) ä¸Šåˆ†åˆ—ç¬¬ä¸€å’Œç¬¬äºŒðŸ†ï¼

![leaderboard.png](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202306131321218.png)

æ­£å¦‚ [Thomas Wolf](https://www.linkedin.com/posts/thom-wolf_open-llm-leaderboard-a-hugging-face-space-activity-7070334210116329472-x6ek?utm_source=share&utm_medium=member_desktop) æ‰€è¿°ï¼Œæˆ‘ä»¬æƒŠå–œåœ°å‘çŽ°ï¼Œç›®å‰é¢„è®­ç»ƒ 40B æ¨¡åž‹æ‰€ç”¨çš„è®¡ç®—é‡å¤§çº¦åªæœ‰ LLaMa 65B æ‰€ç”¨è®¡ç®—é‡çš„ä¸€åŠ (Falcon 40B ç”¨äº† 2800 petaflop- å¤©ï¼Œè€Œ LLaMa 65B ç”¨äº† 6300 petaflop- å¤©)ï¼Œè¿™è¡¨æ˜Žè¯¥æ¨¡åž‹ç”šè‡³å°šæœªå®Œå…¨é¢„è®­ç»ƒè‡³ LLM çš„â€œæœ€ä½³â€æžé™ã€‚

å¯¹ 7B æ¨¡åž‹è€Œè¨€ï¼Œæˆ‘ä»¬å‘çŽ°å…¶åŸºç¡€æ¨¡åž‹è¡¨çŽ°ä¼˜äºŽ `llama-7b`ï¼Œå¹¶è¶…â€‹â€‹è¿‡äº† MosaicML çš„ `mpt-7b`ï¼Œæˆä¸ºå½“å‰è¯¥è§„æ¨¡ä¸Šæœ€å¥½çš„é¢„è®­ç»ƒ LLMã€‚ä¸‹é¢æ‘˜å½•äº†æŽ’è¡Œæ¦œä¸­ä¸€äº›çƒ­é—¨æ¨¡åž‹çš„æŽ’åæƒ…å†µï¼Œä»¥ä¾›æ¯”è¾ƒ:

æ¨¡åž‹

ç±»åž‹

æŽ’è¡Œæ¦œå¹³å‡å¾—åˆ†

[tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)

instruct

63.2

[tiiuae/falcon-40b](https://huggingface.co/tiiuae/falcon-40b)

base

60.4

[llama-65b](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)

base

58.3

[TheBloke/dromedary-65b-lora-HF](https://huggingface.co/TheBloke/dromedary-65b-lora-HF)

instruct

57

[stable-vicuna-13b](https://huggingface.co/CarperAI/stable-vicuna-13b-delta)

rlhf

52.4

[llama-13b](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)

base

51.8

[TheBloke/wizardLM-7B-HF](https://huggingface.co/TheBloke/wizardLM-7B-HF)

instruct

50.1

[tiiuae/falcon-7b](https://huggingface.co/tiiuae/falcon-7b)

base

48.8

[mosaicml/mpt-7b](https://huggingface.co/mosaicml/mpt-7b)

base

48.6

[tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)

instruct

48.4

[llama-7b](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)

base

47.6

å°½ç®¡ `Open LLM æŽ’è¡Œæ¦œ` ä¸èƒ½è¡¡é‡èŠå¤©èƒ½åŠ› (è¿™æ–¹é¢ç›®å‰ä¸»è¦è¿˜æ˜¯ä¾èµ–äººç±»è¯„ä¼°)ï¼Œä½†æˆªè‡³ç›®å‰ Falcon æ¨¡åž‹è¡¨çŽ°å‡ºçš„è¿™äº›åˆæ­¥æ•ˆæžœä¾ç„¶éžå¸¸é¼“èˆžäººå¿ƒï¼

çŽ°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•å¾®è°ƒä¸€ä¸ªä½ è‡ªå·±çš„ Falcon æ¨¡åž‹ â€”â€” æˆ–è®¸ä½ å¾®è°ƒå‡ºæ¥çš„æŸä¸€ä¸ªæ¨¡åž‹æœ€ç»ˆä¼šç™»ä¸Šæ¦œé¦–ðŸ¤—ã€‚

ç”¨ PEFT å¾®è°ƒ
---------

è®­ç»ƒ 10B+ å¤§å°çš„æ¨¡åž‹åœ¨æŠ€æœ¯å’Œè®¡ç®—ä¸Šéƒ½é¢‡å…·æŒ‘æˆ˜ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£å¦‚ä½•ä½¿ç”¨ Hugging Face ç”Ÿæ€ä¸­è½¯ä»¶å·¥å…·åœ¨ç®€å•çš„ç¡¬ä»¶ä¸Šé«˜æ•ˆåœ°å¾®è°ƒè¶…å¤§æ¨¡åž‹ï¼Œå¹¶å±•ç¤ºå¦‚ä½•åœ¨å•å¼ è‹±ä¼Ÿè¾¾ T4 å¡ (16GB - Google Colab) ä¸Šå¾®è°ƒ `falcon-7b`ã€‚

æˆ‘ä»¬ä»¥åœ¨ [Guanaco æ•°æ®é›†](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) ä¸Šå¾®è°ƒ Falcon ä¸ºä¾‹ã€‚Guanaco æ•°æ®é›†æ˜¯ [Open Assistant æ•°æ®é›†](https://huggingface.co/datasets/OpenAssistant/oasst1) çš„ä¸€ä¸ªé«˜è´¨é‡å­é›†ï¼Œå…¶ä¸­åŒ…å«å¤§çº¦ 1 ä¸‡ä¸ªå¯¹è¯ã€‚é€šè¿‡ [PEFT åº“](https://github.com/huggingface/peft)ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æœ€æ–°çš„ [QLoRA](https://arxiv.org/abs/2305.14314) æ–¹æ³•ç”¨ 4 æ¯”ç‰¹æ¥è¡¨ç¤ºæ¨¡åž‹ï¼Œå¹¶å†»ç»“å®ƒï¼Œå†åœ¨å…¶ä¸ŠåŠ ä¸€ä¸ªé€‚é…å­æ¨¡åž‹ (adapter)ï¼Œå¹¶å¾®è°ƒè¯¥é€‚é…å­æ¨¡åž‹ã€‚ä½ å¯ä»¥ [ä»Žè¿™ç¯‡åšæ–‡ä¸­](https://huggingface.co/blog/4bit-transformers-bitsandbytes) äº†è§£æœ‰å…³ 4 æ¯”ç‰¹é‡åŒ–æ¨¡åž‹çš„æ›´å¤šä¿¡æ¯ã€‚

å› ä¸ºåœ¨ä½¿ç”¨ä½Žé˜¶é€‚é…å™¨ (Low Rank Adaptersï¼ŒLoRA) æ—¶åªæœ‰ä¸€å°éƒ¨åˆ†æ¨¡åž‹æƒé‡æ˜¯å¯è®­ç»ƒçš„ï¼Œæ‰€ä»¥å¯è®­ç»ƒå‚æ•°çš„æ•°é‡å’Œè®­å¾—æ¨¡åž‹çš„å°ºå¯¸éƒ½ä¼šæ˜¾è‘—å‡å°ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæœ€ç»ˆçš„è®­ç»ƒäº§ç‰© (trained artifact) ä¸ŽåŽŸå§‹çš„ 7B æ¨¡åž‹ (æ•°æ®ç±»åž‹ä¸º bfloat16 æ—¶å  15GB å­˜å‚¨ç©ºé—´) ç›¸æ¯”ï¼Œåªå  65MB å­˜å‚¨ç©ºé—´ã€‚

![repo-screenshot.png](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202306131322899.png)

**ä¸Žå¤§çº¦ 15GB çš„åŽŸå§‹æ¨¡åž‹ï¼ˆåŠç²¾åº¦ï¼‰ç›¸æ¯”ï¼Œæœ€ç»ˆçš„è®­ç»ƒäº§ç‰©åªéœ€å­˜å‚¨ 65MB çš„æƒé‡**

æ›´å…·ä½“åœ°è¯´ï¼Œåœ¨é€‰å®šéœ€è¦å¾®è°ƒçš„æ¨¡å— (å³æ³¨æ„åŠ›æ¨¡å—çš„æŸ¥è¯¢æ˜ å°„å±‚å’Œé”®æ˜ å°„å±‚) ä¹‹åŽï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªç›®æ ‡æ¨¡å—æ—è¾¹æ·»åŠ ä¸¤ä¸ªå°çš„å¯è®­ç»ƒçº¿æ€§å±‚ (å¦‚ä¸‹å›¾æ‰€ç¤º) ä½œä¸ºé€‚é…å­æ¨¡åž‹ã€‚ç„¶åŽï¼Œå°†é€‚é…å­æ¨¡åž‹è¾“å‡ºçš„éšå«çŠ¶æ€ä¸ŽåŽŸå§‹æ¨¡åž‹çš„éšå«çŠ¶æ€ç›¸åŠ ä»¥èŽ·å¾—æœ€ç»ˆéšå«çŠ¶æ€ã€‚

![lora-gif](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202306131322253.gif)

**ç”¨ç”±æƒé‡çŸ©é˜µ A å’Œ B ç»„æˆçš„ä½Žç§©é€‚é…å™¨ï¼ˆå³ï¼‰çš„è¾“å‡ºæ¿€æ´»æ¥å¢žå¼ºåŽŸå§‹ï¼ˆå†»ç»“ï¼‰é¢„è®­ç»ƒæ¨¡åž‹ï¼ˆå·¦ï¼‰çš„è¾“å‡ºæ¿€æ´»ã€‚**

ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæ— é¡»ä¿å­˜æ•´ä¸ªæ¨¡åž‹ï¼Œå› ä¸ºåŸºç¡€æ¨¡åž‹ä¸€ç›´å¤„äºŽå†»ç»“çŠ¶æ€ã€‚æ­¤å¤–ï¼ŒåŽŸå§‹æ¨¡åž‹å¯ä»¥è¡¨ç¤ºä¸ºä»»æ„æ•°æ®ç±»åž‹ (int8ã€fp4ã€fp16 ç­‰)ï¼Œåªè¦åœ¨ä¸Žé€‚é…å™¨çš„è¾“å‡ºéšå«çŠ¶æ€ç›¸åŠ å‰ï¼Œå°†å…¶è¾“å‡ºéšå«çŠ¶æ€çš„æ•°æ®ç±»åž‹è½¬æ¢æˆä¸Žé€‚é…å™¨ç›¸åŒçš„æ•°æ®ç±»åž‹å³å¯ â€”â€” bitsandbytes çš„æ¨¡å— ( `Linear8bitLt` å’Œ `Linear4bit` ) å°±æ˜¯è¿™ä¹ˆåšçš„ï¼Œ `Linear8bitLt` å’Œ `Linear4bit` è¿™ä¸¤ä¸ªæ¨¡å—çš„è¾“å‡ºæ•°æ®ç±»åž‹ä¸ŽåŽŸæœªé‡åŒ–æ¨¡åž‹çš„è¾“å‡ºæ•°æ®ç±»åž‹ç›¸åŒã€‚

æˆ‘ä»¬åœ¨ Guanaco æ•°æ®é›†ä¸Šå¾®è°ƒäº† Falcon æ¨¡åž‹çš„ä¸¤ä¸ªå˜ä½“ (7B å’Œ 40B)ã€‚å…¶ä¸­ï¼Œ7B æ¨¡åž‹æ˜¯åœ¨å• NVIDIA-T4 16GB ä¸Šå¾®è°ƒçš„ï¼Œè€Œ 40B æ¨¡åž‹æ˜¯åœ¨å• NVIDIA A100 80GB ä¸Šå¾®è°ƒçš„ã€‚åœ¨å¾®è°ƒæ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† 4 æ¯”ç‰¹é‡åŒ–çš„åŸºç¡€æ¨¡åž‹ä»¥åŠ QLoRA æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨äº† [æ¥è‡ª TRL åº“çš„æœ€æ–°çš„ `SFTTrainer`](https://huggingface.co/docs/trl/main/en/sft_trainer)ã€‚

[æ­¤å¤„](https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14) æä¾›äº†ä½¿ç”¨ PEFT é‡çŽ°æˆ‘ä»¬å®žéªŒçš„å®Œæ•´è„šæœ¬ã€‚ä½†æ˜¯å¦‚æžœä½ æƒ³å¿«é€Ÿè¿è¡Œ `SFTTrainer` (è€Œæ— éœ€ PEFT) çš„è¯ï¼Œåªéœ€ä¸‹é¢å‡ è¡Œä»£ç å³å¯:

    from datasets import load_dataset
    from trl import SFTTrainer
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    dataset = load_dataset("imdb", split="train")
    
    model_id = "tiiuae/falcon-7b"
    
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)
    
    trainer = SFTTrainer(
        model,
        tokenizer=tokenizer
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=512,
    )
    trainer.train()
    

ä½ è¿˜å¯ä»¥æŸ¥çœ‹ [åŽŸå§‹ QLoRA ä»£ç åº“](https://github.com/artidoro/qlora/)ï¼Œä»¥äº†è§£æœ‰å…³å¦‚ä½•è¯„ä¼°è®­ç»ƒæ¨¡åž‹çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

### å…³äºŽå¾®è°ƒçš„èµ„æº

*   **[ä½¿ç”¨ 4 æ¯”ç‰¹é‡åŒ–å’Œ PEFT åœ¨ Guanaco æ•°æ®é›†ä¸Šå¾®è°ƒ Falcon-7B çš„ Colab notebook](https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing)**
*   **[è®­ç»ƒä»£ç ](https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14)**
*   **[40B æ¨¡åž‹çš„ LoRA æ¨¡åž‹](https://huggingface.co/smangrul/falcon-40B-int4-peft-lora-sfttrainer)** ([æ—¥å¿—](https://wandb.ai/smangrul/huggingface/runs/3hpqq08s/workspace?workspace=user-younesbelkada))
*   **[7B æ¨¡åž‹çš„ LoRA æ¨¡åž‹](https://huggingface.co/ybelkada/falcon-7b-guanaco-lora)** ([æ—¥å¿—](https://wandb.ai/younesbelkada/huggingface/runs/2x4zi72j?workspace=user-younesbelkada))

æ€»ç»“
--

Falcon æ˜¯æœ€æ–°çš„ã€ä»¤äººå…´å¥‹çš„ã€å¯å•†ç”¨çš„å¤§è¯­è¨€æ¨¡åž‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº† Falcon æ¨¡åž‹çš„åŠŸèƒ½ã€å¦‚ä½•åœ¨ä½ è‡ªå·±çš„çŽ¯å¢ƒä¸­è¿è¡Œ Falcon æ¨¡åž‹ä»¥åŠåœ¨ Hugging Face ç”Ÿæ€ä¸­å¦‚ä½•è½»æ¾åœ°ç”¨è‡ªæœ‰æ•°æ®å¾®è°ƒå®ƒä»¬ã€‚æˆ‘ä»¬æœŸå¾…çœ‹åˆ°ç¤¾åŒºå¦‚ä½•ä½¿ç”¨ Falcon æ¨¡åž‹ï¼

* * *

> è‹±æ–‡åŽŸæ–‡: [https://hf.co/blog/falcon](https://hf.co/blog/falcon)
> 
> åŽŸæ–‡ä½œè€…ï¼šLeandro von Werraï¼ŒYounes Belkadaï¼ŒSourab Mangrulkarï¼ŒLewis Tunstallï¼ŒOlivier Dehaeneï¼ŒPedro Cuencaï¼ŒPhilipp Schmid
> 
> è¯‘è€…: Matrix Yao (å§šä¼Ÿå³°)ï¼Œè‹±ç‰¹å°”æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œå·¥ä½œæ–¹å‘ä¸º transformer-family æ¨¡åž‹åœ¨å„æ¨¡æ€æ•°æ®ä¸Šçš„åº”ç”¨åŠå¤§è§„æ¨¡æ¨¡åž‹çš„è®­ç»ƒæŽ¨ç†ã€‚
> 
> æŽ’ç‰ˆ/å®¡æ ¡: zhongdongy (é˜¿ä¸œ)