---
layout: post
title: "Prompt工程师指南[从基础到进阶篇]：用于开发和优化提示，以有效地使用语言模型（LMs）进行各种应用和研究主题"
date: "2023-05-15T01:09:18.912Z"
---
Prompt工程师指南\[从基础到进阶篇\]：用于开发和优化提示，以有效地使用语言模型（LMs）进行各种应用和研究主题
===========================================================

Prompt工程师指南\[从基础到进阶篇\]：用于开发和优化提示，以有效地使用语言模型（LMs）进行各种应用和研究主题

Prompt工程师指南\[从基础到进阶篇\]：用于开发和优化提示，以有效地使用语言模型（LMs）进行各种应用和研究主题
===========================================================

Prompt工程是一种相对较新的学科，用于开发和优化提示，以有效地使用语言模型（LMs）进行各种应用和研究主题。Prompt工程技能有助于更好地理解大型语言模型（LLMs）的能力和局限性。研究人员使用Prompt工程来改善LLMs在各种常见和复杂任务上的能力，

1.Prompting介绍
=============

Prompt engineering(提示工程)是一门相对较新的学科，旨在开发和优化提示以有效地利用语言模型 (LMs) 进行各种应用和研究主题。提示工程技能有助于更好地理解大型语言模型 (LLMs) 的能力和局限性。研究人员使用提示工程来提高LLMs在各种常见和复杂任务上的能力，例如问答和算术推理。开发人员使用提示工程来设计与LLMs和其他工具接口的稳健和有效的提示技术。

本指南介绍了标准提示的基础知识，以提供如何使用提示与大型语言模型 (LLMs) 进行交互和指导的大致思路。

除非另有说明，所有示例均使用 text-davinci-003 (在 OpenAI 的 playground 上使用)，使用默认配置，例如 temperature=0.7 和 top-p=1。

提纲:

*   [基本提示](#%E5%9F%BA%E6%9C%AC%E6%8F%90%E7%A4%BA)
*   [关于LLM设置的一些说明](#%E5%85%B3%E4%BA%8ELLM%E8%AE%BE%E7%BD%AE%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AF%B4%E6%98%8E)
*   [标准提示](#%E6%A0%87%E5%87%86%E6%8F%90%E7%A4%BA)
*   [提示的元素](#%E6%8F%90%E7%A4%BA%E7%9A%84%E5%85%83%E7%B4%A0)
*   [设计提示的一些技巧](#%E8%AE%BE%E8%AE%A1%E6%8F%90%E7%A4%BA%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%80%E5%B7%A7)

* * *

1.1基本提示
-------

使用提示已经可以实现很多功能，但结果的质量取决于您提供的信息量。提示可以包含诸如您传递给模型的“指令”或“问题”以及其他详细信息，例如“输入”或“示例”。

以下是一个简单提示的基本示例：

_Prompt_

    The sky is
    

_Output:_

    blue
    
    The sky is blue on a clear day. On a cloudy day, the sky may be gray or white.
    

如您所见，语言模型输出了一系列在上下文“天空是”下有意义的字符串。输出可能出乎意料，或与我们想要完成的任务相去甚远。

这个基本示例也突出了提供更多上下文或具体说明我们想要实现什么的必要性。

让我们试着稍微改进一下：

_Prompt:_

    Complete the sentence: 
    
    The sky is
    

_Output:_

     so  beautiful today.
    

这样是不是好多了？我们告诉模型要完成句子，所以结果看起来更好，因为它恰好遵循了我们要求它做的事情（“完成句子”）。这种设计最佳提示以指导模型执行任务的方法就是所谓的 提示工程。

上面的例子是现今LLMs能够实现的基本演示。如今的LLMs能够执行各种先进任务，从文本摘要到数学推理再到代码生成。

* * *

1.2关于LLM设置的一些说明
---------------

在使用提示时，您将通过API或直接与LLM进行交互。您可以配置一些参数来获取不同的提示结果。

**Temperature** - 温度,简而言之，温度越低，结果越确定，意思是始终选择最可能的下一个标记。增加温度可能会导致更多的随机性，从而鼓励更多多样化或有创造性的输出。我们实际上是增加了其他可能标记的权重。在应用方面，对于基于事实的问答之类的任务，我们可能希望使用较低的温度来鼓励更准确和简明的回答。对于诗歌生成或其他有创造性的任务，增加温度可能会更有益。  
**Top\_p** - 同样，使用温度的一种抽样技术称为核采样（nucleus sampling）时，可以通过top\_p来控制模型在生成响应时的确定性。如果您想要精确和客观的答案，请将其保持较低。如果您想要更多样化的回答，请将其增加到较高的值。

一般建议是只更改一个参数而非同时更改两个参数。

在开始一些基本示例之前，请记住您的结果可能因您使用的LLM版本而异。

* * *

1.3 标准提示
--------

我们上面尝试了一个非常简单的提示。标准提示具有以下格式：

    <Question>?
    

这可以按照很多问答数据集的标准格式进行格式化，如下所示：

    Q: <Question>?
    A: 
    

根据上面的标准格式，一种流行且有效的提示技术被称为few-shot提示，其中我们提供示例。  
Few-shot提示可以格式化如下：

    <Question>?
    <Answer>
    
    <Question>?
    <Answer>
    
    <Question>?
    <Answer>
    
    <Question>?
    
    

您可以猜测它的问答（QA）格式版本看起来像这样：

    Q: <Question>?
    A: <Answer>
    
    Q: <Question>?
    A: <Answer>
    
    Q: <Question>?
    A: <Answer>
    
    Q: <Question>?
    A:
    

请注意，不需要使用QA格式。格式取决于手头的任务。例如，您可以执行一个简单的分类任务并提供演示该任务的示例，如下所示：  
_Prompt:_

    This is awesome! // Positive
    This is bad! // Negative
    Wow that movie was rad! // Positive
    What a horrible show! //
    

_Output:_

    Negative
    

Few-shot prompts可以实现上下文学习，即语言模型仅仅通过少量的样本就能够学习任务。在即将发布的指南中，我们将看到更多这方面的实际应用。

* * *

1.4 提示的元素
---------

随着我们涵盖越来越多的提示工程的实例和应用，您会发现提示由以下特定元素组成：

**指令** - 您希望模型执行的特定任务或指令

**上下文** - 可涉及外部信息或额外的上下文，可以引导模型产生更好的响应

**输入数据** - 我们感兴趣的输入或问题，希望找到对应的回答

**输出指标** - 指示输出的类型或格式。

并不是所有的组件都是提示所必需的，格式取决于手头的任务。我们将在即将发布的指南中提供更多具体的例子。

* * *

1.5 设计提示的一些技巧
-------------

以下是设计提示时需要记住的一些技巧：

*   从简单开始  
    在开始设计提示时，您应该记住，这是一个需要大量实验和迭代才能得到最佳结果的过程。使用像OpenAI或Cohere这样的简单 playground 是一个很好的起点。

您可以从简单的提示开始，随着不断添加元素和上下文，以获得更好的结果。在这个过程中对提示进行版本控制非常重要。在我们阅读指南的过程中，您将看到许多例子，其中具体、简单和简明的提示通常会给您带来更好的结果。

当您有一个涉及许多不同子任务的大任务时，可以尝试将任务分解成更简单的子任务，并随着获得更好的结果逐步构建。这可以避免在提示设计过程中添加过多的复杂性。

*   指令  
    您可以使用命令指导模型执行各种简单任务来设计有效的提示，例如“写入”，“分类”，“摘要”，“翻译”，“排序”等。

请记住，您也需要进行大量实验，以查看哪种方式效果最佳。尝试使用不同的关键字、上下文和数据来尝试不同的指令，以查看哪种方法最适合您的特定用例和任务。通常情况下，上下文与您要执行的任务越具体和相关，效果越好。在即将发布的指南中，我们将涉及到采样和添加更多上下文的重要性。

其他人建议将指令放置在提示的开头。同时，建议使用清晰的分隔符比如“###”来分隔指令和上下文。

例如：

_Prompt:_

    ###nstruction ###
    Translate the text below to Spanish:
    
    Text: "hello!"
    

_Output:_

    ¡Hola!
    

*   具体方式  
    在设计提示时，要非常具体，明确要求模型执行的任务。提示越具体、详细，结果就越好。这在你需要特定的生成结果或风格时尤为重要。并没有特定的令牌或关键字可以带来更好的结果，更重要的是有一个良好的格式和描述性提示。实际上，在提示中提供示例非常有效，可以获得特定格式的期望输出。  
    在设计提示时，您还应考虑提示的长度，因为提示的长度有限制。思考提示应该多具体、详细是需要考虑的问题。太多不必要的细节并不一定是一个好方法。细节应该是相关的，对任务有所贡献。这是你需要进行大量实验和迭代来优化提示的应用程序。

作为一个例子，让我们尝试一个简单的提示，从一段文本中提取特定信息。

_Prompt:_

    Extract the name of places in the following text. 
    
    Desired format:
    Place: <comma_separated_list_of_company_names>
    
    Input: "Although these developments are encouraging to researchers, much is still a mystery. “We often have a black box between the brain and the effect we see in the periphery,” says Henrique Veiga-Fernandes, a neuroimmunologist at the Champalimaud Centre for the Unknown in Lisbon. “If we want to use it in the therapeutic context, we actually need to understand the mechanism.""
    

_Output:_

    Place: Champalimaud Centre for the Unknown, Lisbon
    

输入文本内容来自于这篇Nature的文章 [this Nature article](https://www.nature.com/articles/d41586-023-00509-z).

*   避免不准确性

根据上面关于提供详细信息和改善格式的提示，很容易陷入想要过于聪明的提示陷阱，从而可能创建不准确的描述。通常更直接和具体的描述更为有效。这里的类比非常类似于有效沟通，越直接，信息传递就越有效。

例如，您可能有兴趣了解提示工程的概念。您可以尝试像这样的提示：

    解释提示工程的概念。保持解释简短，只有几句话，不要过于描述。
    

从上面的提示中并不清楚要使用多少句话以及什么样的风格。使用上面的提示仍然可能得到良好的回复，但更好的提示是非常具体，简洁明了，直截了当的。例如：

    用 2-3 句话向一名高中学生解释提示工程的概念。
    

*   什么该做什么不该做  
    在设计提示时，另一个常见的技巧是避免说出什么不要做，而是说出应该做什么。这鼓励更加具体化和专注于细节，从而产生良好的模型响应。

以下是一个电影推荐聊天机器人失败的示例，因为我写的指示语里侧重于要避免什么。

_Prompt:_

    以下是一个向客户推荐电影的代理。不要询问兴趣爱好。不要询问个人信息。
    
    顾客：请根据我的兴趣爱好推荐一部电影。
    代理：
    

_Output:_

    当然，我可以根据您的兴趣推荐电影。您想看什么类型的电影？您喜欢动作片、喜剧片、爱情片还是其他类型的电影？
    

这里是更好的Prompt：

_Prompt:_

    以下是一个向客户推荐电影的代理。该代理应该从全球热门电影中推荐电影，不要询问用户的喜好，避免询问个人信息。如果代理无法推荐电影，则应回复“对不起，今天无法找到推荐的电影。”
    
    顾客：请根据我的兴趣爱好推荐一部电影。
    代理：
    

_Output:_

    对不起，我没有关于您的兴趣爱好的信息。然而，以下是当前全球热门电影的列表：[电影列表]。希望您能找到自己喜欢的电影！
    

上述示例部分摘自["OpenAI API的提示工程最佳实践"](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)文章。

2.基础提示 Basic Prompting
======================

在上一篇指南中，我们介绍了并且提供了一个基本的 prompt 示例。

在这篇指南中，我们将提供更多的 prompt 使用示例，并介绍在更高级的指南中将会很重要的关键概念。

通常，学习概念的最佳方法是通过示例进行学习。下面我们将涵盖一些精心制作的 prompt 的示例，这些 prompt 可以用于执行各种有趣和不同的任务。

主题：

*   [文本摘要](#%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81)
*   [信息提取](#%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96)
*   [问答](#%E9%97%AE%E7%AD%94)
*   [文本分类](#%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB)
*   [对话](#%E5%AF%B9%E8%AF%9D)
*   [代码生成](#%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90)
*   [推理](#%E6%8E%A8%E7%90%86)
*   \[Python 笔记本\](#Python 笔记本)

* * *

2.1 文本摘要
--------

自然语言生成中的一项标准任务是文本摘要。文本摘要可以包含许多不同的风格和领域。语言模型最有前途的应用之一是将文章和概念总结成快速易读的摘要。让我们尝试使用提示来进行基本的摘要任务。

假设我有兴趣了解抗生素，我可以尝试使用以下提示：

_Prompt:_

    解释什么是抗生素
    
    A:
    

_Output:_

    抗生素是一种用于治疗细菌感染的药物。它们通过杀死细菌或防止其繁殖来使机体免疫系统对抗感染。抗生素通常以口服药片、胶囊或液体溶液的形式服用，有时也会静脉注射。它们对病毒感染无效，不当使用会导致抗生素耐药性。
    

在这里，"A:" 是一个显式的提示格式，用于问题回答。我在这里使用它来告诉模型存在预期的后续内容。在这个例子中，这并不清楚有什么用处，但我们会在后面的例子中详细讲解。让我们假设这个输出信息太多，我们希望进一步摘要。我们可以指示模型进行一句话总结，如下所示：  
_Prompt:_

    抗生素是一种用于治疗细菌感染的药物。它们通过杀死细菌或防止其繁殖来使机体免疫系统对抗感染。抗生素通常以口服药片、胶囊或液体溶液的形式服用，有时也会静脉注射。它们对病毒感染无效，不当使用会导致抗生素耐药性。
    
    请用一句话解释上面的内容：
    

_Output:_

    抗生素是一种药物，用于通过杀死或防止细菌繁殖来治疗细菌感染，但不对病毒有效，不当使用会导致抗生素耐药性。
    

在不过多关注上述输出的准确性的情况下，我们可以看到模型尝试将段落总结为一句话。您可以聪明地使用指示，但我们将在以后的章节中介绍。随时暂停并进行实验，看看是否可以得到更好的结果。

* * *

2.2 信息提取
--------

尽管语言模型是被训练用于执行自然语言生成及相关任务的，但它也非常擅长执行分类和一系列其他的自然语言处理（NLP）任务。

以下是一个从给定段落中提取信息的提示示例。

_Prompt:_

    研究论文中的作者贡献声明和致谢应明确并具体说明作者在撰写和分析手稿过程中是否以及在什么程度上使用了诸如ChatGPT这样的AI技术。它们还应指出使用了哪些LLMs。这将提醒编辑和审稿人更仔细地审查手稿，以寻找潜在的偏见、不准确性和不正确的来源证明。同样，科学期刊应在选择提交的手稿时透明地公开使用LLMs的情况。
    
    提及该段落中提到的大型语言模型产品是什么？
    

_Output:_

    该段落中提到的大型语言模型产品是ChatGPT。
    

我们可以通过许多方法来改进上述结果，但这已经非常有用了。

现在显然可以通过简单地指示它要做什么来要求模型执行不同的任务。这是AI产品构建者已经在使用的强大功能，以构建强大的产品和体验。

段落来源: [ChatGPT: five priorities for research](https://www.nature.com/articles/d41586-023-00288-7)

* * *

2.3 问答
------

获取模型对特定答案的响应的最佳方法之一是改进提示的格式。如前所述，提示可以结合指令、上下文、输入和输出指示符来获得更好的结果。虽然这些组件并非必需，但随着您指令越具体，您将获得更好的结果。以下是如何在更具结构的提示中进行的示例。  
_Prompt:_

    根据以下内容回答问题。保持答案简短。如果不确定答案，请回答“不确定答案”。
    
    背景：Teplizumab可以追溯到新泽西州的一家名为Ortho Pharmaceutical的药品公司。在那里，科学家们生成了一种名为OKT3的抗体的早期版本。该分子最初是从小鼠中提取的，它能够结合到T细胞的表面，并限制它们的细胞杀伤潜力。1986年，它获得批准，用于帮助预防肾脏移植后的器官排斥，成为首个允许人类使用的治疗性抗体。
    
    问题：OKT3最初是从哪里提取的？
    
    答案：
    

_Output:_

    OKT3最初是从小鼠中提取的。
    

内容来源: [Nature](https://www.nature.com/articles/d41586-023-00400-x).

* * *

2.4 文本分类
--------

到目前为止，我们已经使用简单的指令执行任务。作为一个 Prompt 工程师，您需要提高提供更好指令的能力。但这还不够！您还会发现，在更难的用例中，仅提供指令是不够的。这就是您需要更多考虑上下文和可以在提示中使用的不同元素的地方。您可以提供的其他元素包括输入数据或示例。

让我们尝试通过提供文本分类的示例来演示这一点。

_Prompt:_

    Classify the text into neutral, negative or positive. 
    
    Text: I think the food was okay. 
    Sentiment:
    

_Output:_

    Neutral 
    

这里的回答是Neutral(首字母大写)，本例子是想让回答变成小写的neutral

我们给出了分类文本的指令，模型以'Neutral'响应，这是正确的。没有什么错的，但是假设我们真正需要的是让模型以我们想要的确切格式返回标签。所以，我们不想要Neutral，而是想要它返回全小写的neutral。我们如何做到这一点。有不同的方法来做到这一点。我们关心的是具体性，在提示中提供的信息越多，结果就越好。我们可以尝试提供示例来指定正确的行为。让我们再试一次：  
_Prompt:_

    Classify the text into neutral, negative or positive. 
    
    Text: I think the vacation is okay.
    Sentiment: neutral 
    
    Text: I think the food was okay. 
    Sentiment:
    

_Output:_

    neutral
    

完美！这次模型返回了neutral，这正是我要寻找的具体标签。似乎提示中提供的示例帮助模型在其输出中变得更加具体。为了强调为什么有时候具体性很重要，请看这个例子并找出问题：  
_Prompt:_

    Classify the text into nutral, negative or positive. 
    
    Text: I think the vacation is okay.
    Sentiment:
    

_Output:_

    Neutral
    

这里的问题是什么？

* * *

2.5 对话
------

通过prompt engineering，您可以告诉LLM系统如何行事，其意图和身份，这在构建客户服务聊天机器人等对话系统时特别有用。

例如，让我们创建一个对问题生成更多技术和科学响应的对话系统。请注意，我们通过指令明确告诉它如何行事。这有时被称为角色提示。

_Prompt:_

    下面是与一名技术和科学气氛的AI研究助理进行的对话。
    
    人类：你好，你是谁？
    AI：问候！我是一个AI研究助手。今天我能帮你什么忙吗？
    人类：你能告诉我黑洞的形成吗？
    AI：
    

_Output:_

    当然可以！黑洞是时空中引力极强的区域，甚至连光也无法逃脱。当一个非常质量巨大的恒星死亡并且其核心坍塌时，就会形成一个无限密度的奇点，从而形成黑洞。奇点的强烈引力将周围的所有物质和辐射吸引进来，从而形成黑洞。
    

我们的AI研究助手听起来有点太技术了，对吧？好的，让我们改变这种行为，指示系统提供更易于理解的答案。

_Prompt:_

    下面是与一名AI研究助手的对话。助手的回答应该易于被小学生理解。
    人类：你好，你是谁？
    AI：问候！我是一个AI研究助手。今天我能帮你什么忙吗？
    人类：你能告诉我黑洞的形成吗？
    AI：
    

_Output:_

    当然可以！黑洞是由于非常质量巨大的恒星耗尽燃料而坍塌形成的。恒星的引力非常强大，甚至连光也无法逃离其吸引。这会在时空中创造一个引力极强的点，称为黑洞。
    

我认为我们取得了一些进展。你可以继续改进。我相信如果你添加更多的例子，你可能会得到更好的结果。

* * *

2.6 代码生成
--------

代码生成是LLM非常有效的应用之一，Copilot是一个很好的例子。通过巧妙的提示，你可以执行大量的代码生成任务。让我们看一些下面的例子。

首先，让我们尝试一个简单的程序来问候用户。

_Prompt:_

    /*
    询问用户姓名并说“Hello”
    */
    

_Output:_

    let name = prompt("What is your name?");
    console.log(`Hello, ${name}!`);
    

你可以看到，我们甚至不需要指定要使用的语言。

让我们稍微提高一下难度。我想向你展示在设计提示时LLM可以有多强大。

_Prompt:_

    """
    Table departments, columns = [DepartmentId, DepartmentName]
    Table students, columns = [DepartmentId, StudentId, StudentName]
    创建一个MySQL查询，查询计算机科学系的所有学生
    """
    

_Output:_

    SELECT StudentId, StudentName 
    FROM students 
    WHERE DepartmentId IN (SELECT DepartmentId FROM departments WHERE DepartmentName = 'Computer Science');
    

这非常令人印象深刻。在这种情况下，我们提供了关于数据库模式的数据，并要求它生成一个有效的MySQL查询。

2.7 推理
------

也许目前对于LLM来说最具挑战性的任务之一是需要一定推理能力的任务。推理是我最为期待的领域之一，因为LLM可以从中得到各种复杂的应用。

在涉及到数学能力的任务方面已经有了一些改进。尽管如此，需要注意的是目前的LLM在执行推理任务方面仍然存在困难，因此需要更高级的提示工程技术。我们将在下一篇指南中介绍这些高级技术。现在，我们将介绍一些基本示例，以展示LLM的算术能力。

_Prompt:_

    9000 * 9000等于多少？
    

_Output:_

    81,000,000
    

让我们试试更难的题目。

_Prompt:_

    这组数字中的奇数加起来是一个偶数：15, 32, 5, 13, 82, 7, 1. 
    
    A: 
    

_Output_

    不，这组数字中的奇数加起来是一个奇数：119。
    

这是错误的！让我们通过改进提示来改进结果。

_Prompt:_

    这组数字中的奇数加起来是一个偶数：15, 32, 5, 13, 82, 7, 1. 
    
    通过分步解决问题。首先，识别出奇数，将它们相加，并指示结果是奇数还是偶数。
    

_Output:_

    奇数：15, 5, 13, 7, 1
    总和：41
    41是一个奇数。
    

好多了，对吧？顺便说一句，我尝试了几次，有时系统会失败。如果您提供更好的指示结合示例，可能有助于获得更准确的结果。  
另外GPT4在这方面有很大的改进和提升！

我们将继续在本节中包含更多常见应用的示例。

在接下来的指南中，我们将介绍更高级的提示工程概念，以提高在所有这些和更难的任务上的性能。

* * *

2.7 Python 笔记本
--------------

描述

笔记本

学习如何使用 openai 和 LangChain 库执行许多不同类型的常见任务

[开始使用提示工程](../notebooks/pe-lecture.ipynb)

3.高阶Prompting
=============

到这一步，应该很明显，改进提示有助于在不同任务上获得更好的结果。这就是Prompt工程背后的整个理念。

虽然之前的例子很有趣，但在我们深入了解更高级的概念之前，让我们先正式地介绍一些概念。

Topics:

*   [零样本Prompting](#%E9%9B%B6%E6%A0%B7%E6%9C%ACPrompting)
*   [少样本Prompting](#%E5%B0%91%E6%A0%B7%E6%9C%ACPrompting)
*   [思维链Prompting](#%E6%80%9D%E7%BB%B4%E9%93%BEPrompting)
*   [零样本思维链zero-shot-cot](#%E9%9B%B6%E6%A0%B7%E6%9C%AC%E6%80%9D%E7%BB%B4%E9%93%BEzero-shot-cot)
*   [自洽性Self-Consistency](#%E8%87%AA%E6%B4%BD%E6%80%A7Self-Consistency)
*   [生成知识Prompting](#%E7%94%9F%E6%88%90%E7%9F%A5%E8%AF%86Prompting)
*   [自动提示工程师AutomaticPromptEngineer](#%E8%87%AA%E5%8A%A8%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%B8%88AutomaticPromptEngineer)

* * *

3.1 零样本Prompting
----------------

目前，通过大量数据训练并根据指示进行调整的LLM能够在零样本情况下执行任务。  
我们在前面的章节中尝试了一些零样本的例子。这是我们使用过的其中一个例子：

_Prompt:_

    Classify the text into neutral, negative, or positive. 
    
    Text: I think the vacation is okay.
    Sentiment:
    

_输出:_

    Neutral
    

请注意，上面的提示中我们没有向模型提供任何示例——这就是零样本能力的工作方式。当零样本不起作用时，建议在提示中提供演示或示例。  
下面我们将讨论少样本Prompting这种方法。

* * *

3.2 少样本Prompting
----------------

尽管大型语言模型已经展示出了非凡的零样本能力，但在使用零样本设置时，它们在更复杂的任务上仍然表现不足。为了改进这一点，少样本Prompting被用作一种技术，以实现上下文学习，即在提示中提供演示以引导模型取得更好的表现。这些演示作为后续例子的条件，我们希望模型能够生成一个响应。

让我们通过一个由 [Brown et al. 2020](https://arxiv.org/abs/2005.14165).  
提出的例子来演示少样本Prompting。在这个例子中，任务是在句子中正确使用一个新词。

_Prompt:_

    A "whatpu" is a small, furry animal native to Tanzania. An example of a sentence that uses
    the word whatpu is:
    We were traveling in Africa and we saw these very cute whatpus.
    To do a "farduddle" means to jump up and down really fast. An example of a sentence that uses
    the word farduddle is:
    

_Output:_

    When we won the game, we all started to farduddle in celebration.
    

我们可以观察到，仅提供一个示例（即1-shot）即可让模型学会执行任务。对于更难的任务，我们可以尝试增加示例（例如3-shot，5-shot，10-shot等）来进行实验。

根据Min等人[Min et al. (2022)](https://arxiv.org/abs/2202.12837),的研究结果，以下是一些关于少样本演示/示例的提示：

\-“标签空间和演示文本分布对于性能都很重要（无论个别输入的标签是否正确）”  
\-无论您是否只使用随机标签，所使用的格式也对性能起关键作用，这要比没有标签好得多。  
\-其他结果显示，从真实标签分布（而不是均匀分布）中选择随机标签也有所帮助。

让我们尝试一些例子。首先，让我们尝试一个具有随机标签的示例（意味着将“Negative”和“Positive”标签随机分配给输入）：

_Prompt:_

    This is awesome! // Negative
    This is bad! // Positive
    Wow that movie was rad! // Positive
    What a horrible show! //
    

_Output:_

    Negative
    

我们仍然得到了正确的答案，即使标签已经被随机化了。请注意，我们也保留了格式，这也有助于结果。  
实际上，通过进一步的实验，我们发现我们正在尝试的新GPT模型甚至变得更加强大，即使格式是随机的。例如：

_Prompt:_

    Positive This is awesome! 
    This is bad! Negative
    Wow that movie was rad!
    Positive
    What a horrible show! --
    

_Output:_

    Negative
    

正如你所说，我们需要进行更全面的分析以确认这是否适用于不同和更复杂的任务，包括不同变体的提示。

### 少样本(Few-shot) Prompting的限制

标准的少样本提示法对许多任务来说效果良好，但在处理更复杂的推理任务时仍然不是完美的技术，让我们来演示一下为什么会出现这种情况。  
你还记得我们之前提供的以下任务吗：

    The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. 
    
    A: 
    

如果我们重试，模型会有下面的输出

    Yes, the odd numbers in this group add up to 107, which is an even number.
    

这不是正确的回答，不仅凸显了这些系统的局限性，还表明需要更先进的提示工程。  
让我们尝试添加一些示例，看看few-shot prompting是否可以改善结果。

_Prompt:_

    The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
    A: The answer is False.
    
    The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.
    A: The answer is True.
    
    The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.
    A: The answer is True.
    
    The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.
    A: The answer is False.
    
    The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. 
    A: 
    

_Output:_

    The answer is True.
    

这不是正确的回答，这不仅突出了这些系统的局限性，而且需要更先进的提示工程。让我们尝试添加一些示例，看看少量提示是否会改善结果。

标准的少量提示在许多任务中表现良好，但在处理更复杂的推理任务时仍然不是完美的技术。让我们演示一下为什么会出现这种情况。你还记得我们之前提供的任务吗？

“请计算以下数学问题：24 ÷ 3”

答案是8，但如果我们将问题更改为“请计算以下数学问题：3 ÷ 24”，模型的答案将会是0.这表明，对于某些更复杂的推理问题，少量提示是不足以得到可靠的响应的。上面的示例提供了任务的基本信息。如果你仔细观察，就会发现我们引入的任务类型涉及更多的推理步骤。换句话说，如果我们将问题分解成步骤并向模型演示，这可能会有所帮助。最近，思维链 [chain-of-thought (CoT) prompting](https://arxiv.org/abs/2201.11903)提示已经流行起来，以解决更复杂的算术、常识和符号推理任务。

总的来说，提供示例对于解决一些任务是有用的。当零量提示和少量提示不足以满足需求时，这可能意味着模型所学到的知识不足以在任务中表现良好。从这里开始，建议考虑微调模型或尝试更高级的提示技术。接下来，我们将介绍一种流行的提示技术，即思维链提示，它已经广受欢迎。

* * *

3.3思维链Prompting
---------------

在[Wei et al. (2022)](https://arxiv.org/abs/2201.11903)等人的论文中引入的Chain-of-Thought（CoT）prompting，通过中间推理步骤实现复杂的推理能力。  
您可以将其与few-shot prompting结合使用，以获得更好的结果，用于需要在回答之前进行推理的更复杂任务。

_Prompt:_

    The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
    A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.
    
    The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.
    A: Adding all the odd numbers (17, 19) gives 36. The answer is True.
    
    The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.
    A: Adding all the odd numbers (11, 13) gives 24. The answer is True.
    
    The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.
    A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.
    
    The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. 
    A:
    

_Output:_

    Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.
    

哇！我们可以看到当我们提供了推理步骤时，得到了完美的结果。我们可以通过提供更少的示例来解决这个任务，即，只需要一个示例似乎就足够了：

_Prompt:_

    The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
    A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.
    
    The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. 
    A:
    

_Output:_

    Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.
    

请注意，作者声称这是足够大的语言模型出现的新兴能力。

* * *

3.4零样本思维链zero-shot-cot
----------------------

最近提出的一个想法是“零样本思维链”（Zero-Shot CoT） [zero-shot CoT](https://arxiv.org/abs/2205.11916) (Kojima et al. 2022)。它的基本思想是在原始提示中添加“一步一步地思考”。让我们尝试一个简单的问题，并看看这个模型的表现：

_Prompt:_

    I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?
    

_Output:_

    11 apples
    

答案是不正确的！现在让我们尝试一下特殊提示。

_Prompt:_

    I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?
    
    Let's think step by step.
    

_Output:_

    First, you started with 10 apples.
    You gave away 2 apples to the neighbor and 2 to the repairman, so you had 6 apples left.
    Then you bought 5 more apples, so now you had 11 apples.
    Finally, you ate 1 apple, so you would remain with 10 apples.
    

这个简单的提示在这个任务中非常有效，令人印象深刻。特别是在你没有太多示例可用于提示时，它非常有用。

* * *

3.5自洽性Self-Consistency
----------------------

也许目前用于提示工程的更高级的技术之一是Self-Consistency。由 [Wang et al. (2022)](https://arxiv.org/pdf/2203.11171.pdf)等人提出，Self-Consistency的目标是“替代链式思维提示中使用的朴素贪婪解码方法”。其思想是通过few-shot CoT采样多个不同的推理路径，并使用生成的结果选择最一致的答案。这有助于提高CoT提示在涉及算术和常识推理的任务中的性能。

让我们尝试以下算术推理示例：

_Prompt:_

    When I was 6 my sister was half my age. Now
    I’m 70 how old is my sister?
    

_Output:_

    35
    

输出是错误的！我们如何使用Self-Consistency来改善它呢？让我们试一试。我们将使用Wang等人（2022年）的few-shot示例（表17）：

_Prompt:_

    Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,
    there will be 21 trees. How many trees did the grove workers plant today?
    A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.
    So, they must have planted 21 - 15 = 6 trees. The answer is 6.
    
    Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
    A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.
    
    Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
    A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74
    chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.
    
    Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops
    did Jason give to Denny?
    A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of
    lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.
    
    Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does
    he have now?
    A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so
    in total he has 7 + 2 = 9 toys. The answer is 9.
    
    Q: There were nine computers in the server room. Five more computers were installed each day, from
    monday to thursday. How many computers are now in the server room?
    A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =
    20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.
    The answer is 29.
    
    Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many
    golf balls did he have at the end of wednesday?
    A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On
    Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.
    
    Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?
    A: She bought 5 bagels for $3 each. This means she spent 5
    
    Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?
    A:
    

_Output 1:_

    When I was 6 my sister was half my age, so she was 3. Now I am 70, so she is 70 - 3 = 67. The answer is 67.
    

_Output 2:_

    When the narrator was 6, his sister was half his age, which is 3. Now that the narrator is 70, his sister would be 70 - 3 = 67 years old. The answer is 67.
    

_Output 3:_

    When I was 6 my sister was half my age, so she was 3. Now I am 70, so she is 70/2 = 35. The answer is 35.
    

将最终答案计算出来需要几个步骤（详见论文），但为简单起见，我们可以看到已经出现了绝大多数的答案，因此这实质上将成为最终答案。

* * *

3.6生成知识Prompting
----------------

LLM（大型语言模型）不断得到改进，其中一种流行的技术是能够整合知识或信息来帮助模型进行更准确的预测。

使用类似的思路，模型是否也可以在做出预测之前生成知识？这正是Liu等人 [Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf)在论文中所尝试的——生成用作提示的知识。特别是，这对于常识推理等任务有多么有帮助？

让我们尝试一个简单的提示：

_Prompt:_

    Part of golf is trying to get a higher point total than others. Yes or No?
    

_Output:_

    Yes.
    

这种错误揭示了LLMs在执行需要更多关于世界的知识的任务时的局限性。我们如何通过知识生成来改善这一点呢？

首先，我们生成一些“知识”：

_Prompt:_

    Input: Greece is larger than mexico.
    Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.
    
    Input: Glasses always fog up.
    Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.
    
    Input: A fish is capable of thinking.
    Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of ’higher’ vertebrates including non-human primates. Fish’s long-term memories help them keep track of complex social relationships.
    
    Input: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.
    Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.
    
    Input: A rock is the same size as a pebble.
    Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).
    
    Input: Part of golf is trying to get a higher point total than others.
    Knowledge:
    

_Knowledge 1:_

    The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.
    

_Knowledge 2:_

    Golf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into a series of holes on a course using the fewest number of strokes. The goal is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.
    

我们正在使用[Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf)论文中提供的提示。

接下来的步骤是整合知识并得出预测。我将问题转换成了问答格式，以指导回答的格式。

_Prompt:_

    Question: Part of golf is trying to get a higher point total than others. Yes or No?
    
    Knowledge: The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.
    
    Explain and Answer: 
    

_回答 1 (非常有信心):_

    No, the objective of golf is not to get a higher point total than others. Rather, the objective is to play a set of holes in the least number of strokes. The total number of strokes is used to determine the winner of the game, not the total number of points.
    

    Question: Part of golf is trying to get a higher point total than others. Yes or No?
    
    Knowledge: Golf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into a series of holes on a course using the fewest number of strokes. The goal is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.
    
    Explain and Answer:
    

_回答 2 (信心要低得多):_

    Yes, part of golf is trying to get a higher point total than others. Each player tries to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.
    

这个例子中发生了一些非常有趣的事情。在第一个回答中，模型非常有信心，但在第二个回答中却不是那么有信心。我为了演示的目的简化了过程，但在得出最终答案时还有一些更多的细节需要考虑。

3.7 自动提示工程师AutomaticPromptEngineer
----------------------------------

![](https://ai-studio-static-online.cdn.bcebos.com/5f273980aa1d445ba9e907c7ae3a1dcda5ed953d802445c4b96e9fee39b55ed1)

[Zhou et al., (2022)](https://arxiv.org/abs/2211.01910) 提出了自动提示工程师（APE）框架，用于自动生成和选择指令。指令生成问题被构建为自然语言合成问题，使用LLMs作为黑盒优化问题来生成和搜索候选解。

第一步涉及一个大型语言模型（作为推理模型），该模型会获得输出演示，以生成任务的指令候选方案。这些候选方案将指导搜索过程。指令使用目标模型执行，然后基于计算出的评估分数选择最适合的指令。  
APE发现了一个更好的零-shot CoT提示，比人工设计的“让我们逐步思考”提示（Kojima等人，2022）更好。

提示“让我们逐步地工作，以确保我们有正确的答案。”引发了思考链，提高了MultiArith和GSM8K基准测试的性能：

![](https://ai-studio-static-online.cdn.bcebos.com/d9ef2b83773149518fa3a24ada35e3f12c88062b30ce4a8785072c23b6f71c3a)

本文涉及到与提示工程相关的一个重要主题，即自动优化提示的想法。虽然本指南不深入探讨这个话题，但如果您对此感兴趣，以下是一些关键论文：

*   [AutoPrompt](https://arxiv.org/abs/2010.15980) - 提出了一种基于梯度引导搜索的自动创建各种任务提示的方法。
*   [Prefix Tuning](https://arxiv.org/abs/2101.00190) - 一种轻量级的fine-tuning替代方案，为NLG任务准备可训练的连续前缀。
*   [Prompt Tuning](https://arxiv.org/abs/2104.08691) - 提出了一种通过反向传播学习软提示的机制。