---
layout: post
title: "自然语言处理（NLP） - 前预训练时代的自监督学习"
date: "2023-06-08T01:18:21.194Z"
---
自然语言处理（NLP） - 前预训练时代的自监督学习
==========================

前预训练时代的自监督学习自回归、自编码预训练的前世  
![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602112233170-1798796623.png)  
神经网络(Neural Network, NN)  
![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602112451955-1103931181.png)  
![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602112603353-1028580314.png)

损失函数，度量神经网络的_预测结果_和_真实结果_相差多少

*   平方差损失（欧式距离角度）预测概率分部和实际标签概率的欧式距离
*   交叉熵损失（信息量角度）预测概率分部和真实概率分部的差异，指导神经网络学习时，更加稳定  
    ![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602112649537-278531153.png)  
    对参数W更新损失的负梯度  
    ![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602113136884-455258139.png)  
    One-hot 人为规定，不需要学习，在推荐里有非常多的用处，（可以理解成完全命中）  
    ![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602113751412-1186366375.png)  
    词向量需要学习，可以很好的泛化结果，泛化性能比 one-hot 更好（可以理解成泛化关系的建模）  
    评估模型的好坏：有全体指标，以及一些公开的数据集，去评估词向量的相关性  
    ![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602113944643-674327936.png)

![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602114209288-1388767152.png)

Skip-gram： 给定一个中间值，预测上下文窗口中的一个词  
![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602114231979-2046813122.png)  
![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602114722216-117910640.png)  
CBoW：给定一个上下文词，预测中间值  
![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602114732981-715643428.png)

![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602115151672-2072129378.png)

![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602115357749-532479113.png)

RNN 抛开马尔科夫假设，  
![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602120011882-1744613110.png)

Self-Attention：每个单词和整句所有话进行匹配，来获取当前单词对每个单词的重视程度，利用这个重视程序，对整句话的每个单词进行加权，加权的结果用于表示当前这个单词  
![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602120227741-30226337.png)  
Self-Attention：也是非常流行的 Transformer 的核心模块，  
Seft-Attention 没有考虑单词的顺序，所以为了更精装的表示位置信息，需要对句子的输入加个位置的序号 Positional Embedding  
![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602131053845-870346209.png)

残差连接，很好的缓解梯度消失的问题，包括映射和直连接部分  
![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602133457228-542517037.png)  
![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602141609242-1628451283.png)  
![image](https://img2023.cnblogs.com/blog/80824/202306/80824-20230602141643278-144050732.png)

[https://aistudio.baidu.com/aistudio/education/lessonvideo/1451160](https://aistudio.baidu.com/aistudio/education/lessonvideo/1451160)