---
layout: post
title: "es笔记四之中文分词插件安装与使用"
date: "2023-07-12T01:22:43.182Z"
---
es笔记四之中文分词插件安装与使用
=================

> 本文首发于公众号：Hunter后端  
> 原文链接：[es笔记四之中文分词插件安装与使用](https://mp.weixin.qq.com/s/aQuwrUzLZDKLv_K8dKeVzw)

前面我们介绍的操作及演示都是基于英语单词的分词，但我们大部分使用的肯定都是中文，所以如果需要使用分词的操作肯定也是需要使用中分分词。

这里我们介绍一下如何安装中文分词插件。

在介绍安装之前，我们可以先来测试一下没有中文分词的分词效果，我们先来插入一条数据：

    PUT /exam/_doc/16
    {
        "name" : "张三丰",
        "address": "一个苹果"
    }
    

如果是中文分词，我们可以想到 '一个苹果' 应该会被分为 '一个' 和 '苹果' 这两个词，但是默认的分词是无法这样分词的，我们可以尝试搜索一下：

    GET /exam/_search
    {
      "query": {
        "term": {
          "address": "一个"
        }
      }
    }
    

可以发现是无法搜索到结果的。

我们可以查看一下这条数据的 address 字段被分词的结果：

    GET /exam/_doc/16/_termvectors?fields=address
    

其中，\_doc 后面的 16 是我们要查询的数据的 id，fields 后面跟着的要查看的分词结果的字段名。

可以看到 一个苹果 这个字符串，被挨个分成了四个。

然后我们来安装一下中文分词的插件。

#### 安装中文分词插件

在 elasticsearch 安装的目录下，执行下面的语句：

    ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.6.0/elasticsearch-analysis-ik-7.6.0.zip
    

这里我们安装的是 7.6.0 版本的 es，所以安装对应的分词插件。

然后我们重新建立这个 index，为 address 字段加上指定的分词插件：

挨个执行下面的命令：

    DELETE /exam/
    
    PUT /exam
    
    PUT /exam/_mapping
    {
      "properties": {
        "address": {
          "type": "text",
          "analyzer": "ik_max_word",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "name": {
          "type": "keyword"
        }
      }
    }
    

这里，和第一次创建 mapping 的时候比，多了一行 analyzer 的定义。

然后我们再次重复上面的插入和查询操作：

    PUT /exam/_doc/16
    {
        "name" : "张三丰",
        "address": "一个苹果"
    }
    
    GET /exam/_search
    {
      "query": {
        "term": {
          "address": "一个"
        }
      }
    }
    

可以看到，这次我们搜索 一个，就可以查询到数据了。

然后我们看下这条数据的 address 的分词结果：

    GET /exam/_doc/16/_termvectors?fields=address
    

可以看到，这次返回的结果除了单独的 '一' 和 '个' 之外，还有被分成整体的 '一个' 和 '苹果'。

#### 查看分词结果

这里额外介绍一下如何查看字符串的分词。

如果我们有一个字符串，想要知道它会被如何分词，可以直接使用下面的命令：

    POST /_analyze
    {
      "analyzer": "ik_smart",
      "text": "一个苹果"
    }
    

其中，analyzer 是指定的分词的插件，如果不指定就会使用默认的分词功能。

中文分词插件的github 地址如下：[https://github.com/medcl/elasticsearch-analysis-ik](https://github.com/medcl/elasticsearch-analysis-ik)

如果想获取更多后端相关文章，可扫码关注阅读：  
![image](https://img2023.cnblogs.com/blog/1298097/202307/1298097-20230711202933898-1893441005.png)