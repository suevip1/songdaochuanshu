---
layout: post
title: "è§£ç å™¨ | åŸºäº Transformers çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹"
date: "2023-06-07T01:21:28.070Z"
---
è§£ç å™¨ | åŸºäº Transformers çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹
================================

åŸºäº transformer çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹æ˜¯ _è¡¨å¾å­¦ä¹ _ å’Œ _æ¨¡å‹æ¶æ„_ è¿™ä¸¤ä¸ªé¢†åŸŸå¤šå¹´ç ”ç©¶æˆæœçš„ç»“æ™¶ã€‚æœ¬æ–‡ç®€è¦ä»‹ç»äº†ç¥ç»ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„å†å²ï¼Œæ›´å¤šèƒŒæ™¯çŸ¥è¯†ï¼Œå»ºè®®è¯»è€…é˜…è¯»ç”± Sebastion Ruder æ’°å†™çš„è¿™ç¯‡ç²¾å½© [åšæ–‡](https://ruder.io/a-review-of-the-recent-history-of-nlp/)ã€‚æ­¤å¤–ï¼Œå»ºè®®è¯»è€…å¯¹ _è‡ªæ³¨æ„åŠ› (self-attention) æ¶æ„_ æœ‰ä¸€ä¸ªåŸºæœ¬äº†è§£ï¼Œå¯ä»¥é˜…è¯» Jay Alammar çš„ [è¿™ç¯‡åšæ–‡](http://jalammar.github.io/illustrated-transformer/) å¤ä¹ ä¸€ä¸‹åŸå§‹ transformer æ¨¡å‹ã€‚

æœ¬æ–‡åˆ† 4 ä¸ªéƒ¨åˆ†:

*   èƒŒæ™¯ - _ç®€è¦å›é¡¾äº†ç¥ç»ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„å†å²ï¼Œé‡ç‚¹å…³æ³¨åŸºäº RNN çš„æ¨¡å‹ã€‚_
*   ç¼–ç å™¨-è§£ç å™¨ - _é˜è¿°åŸºäº transformer çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œå¹¶é˜è¿°å¦‚ä½•ä½¿ç”¨è¯¥æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚_
*   ç¼–ç å™¨ - _é˜è¿°æ¨¡å‹çš„ç¼–ç å™¨éƒ¨åˆ†ã€‚_
*   è§£ç å™¨ - _é˜è¿°æ¨¡å‹çš„è§£ç å™¨éƒ¨åˆ†ã€‚_

æ¯ä¸ªéƒ¨åˆ†éƒ½å»ºç«‹åœ¨å‰ä¸€éƒ¨åˆ†çš„åŸºç¡€ä¸Šï¼Œä½†ä¹Ÿå¯ä»¥å•ç‹¬é˜…è¯»ã€‚è¿™ç¯‡åˆ†äº«æ˜¯æœ€åä¸€éƒ¨åˆ† **è§£ç å™¨**ã€‚

è§£ç å™¨
---

å¦‚ _ç¼–ç å™¨-è§£ç å™¨_ éƒ¨åˆ†æ‰€è¿°ï¼Œ _åŸºäº transformer_ çš„è§£ç å™¨å®šä¹‰äº†ç»™å®šä¸Šä¸‹æ–‡ç¼–ç åºåˆ—æ¡ä»¶ä¸‹ç›®æ ‡åºåˆ—çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ:

\\\[p\_{\\theta\_{dec}}(\\mathbf{Y}\_{1: m} | \\mathbf{\\overline{X}}\_{1:n}) \\\]

æ ¹æ®è´å¶æ–¯æ³•åˆ™ï¼Œåœ¨ç»™å®šä¸Šä¸‹æ–‡ç¼–ç åºåˆ—å’Œæ¯ä¸ªç›®æ ‡å˜é‡çš„æ‰€æœ‰å‰é©±ç›®æ ‡å‘é‡çš„æ¡ä»¶ä¸‹ï¼Œå¯å°†ä¸Šè¿°åˆ†å¸ƒåˆ†è§£ä¸ºæ¯ä¸ªç›®æ ‡å‘é‡çš„æ¡ä»¶åˆ†å¸ƒçš„ä¹˜ç§¯:

\\\[p\_{\\theta\_{dec}}(\\mathbf{Y}\_{1:m} | \\mathbf{\\overline{X}}\_{1:n}) = \\prod\_{i=1}^{m} p\_{\\theta\_{dec}}(\\mathbf{y}\_i | \\mathbf{Y}\_{0: i-1}, \\mathbf{\\overline{X}}\_{1:n}) \\\]

æˆ‘ä»¬é¦–å…ˆäº†è§£ä¸€ä¸‹åŸºäº transformer çš„è§£ç å™¨å¦‚ä½•å®šä¹‰æ¦‚ç‡åˆ†å¸ƒã€‚åŸºäº transformer çš„è§£ç å™¨ç”±å¾ˆå¤š _è§£ç å™¨æ¨¡å—_ å †å è€Œæˆï¼Œæœ€åå†åŠ ä¸€ä¸ªçº¿æ€§å±‚ (å³ â€œLM å¤´â€)ã€‚è¿™äº›è§£ç å™¨æ¨¡å—çš„å †å å°†ä¸Šä¸‹æ–‡ç›¸å…³çš„ç¼–ç åºåˆ— \\(\\mathbf{\\overline{X}}\_{1:n}\\) å’Œæ¯ä¸ªç›®æ ‡å‘é‡çš„å‰é©±è¾“å…¥ \\(\\mathbf{Y}\_{0:i-1}\\) (è¿™é‡Œ \\(\\mathbf{y}\_0\\) ä¸º BOS) æ˜ å°„ä¸ºç›®æ ‡å‘é‡çš„ç¼–ç åºåˆ— \\(\\mathbf{\\overline{Y} }\_{0:i-1}\\)ã€‚ç„¶åï¼Œâ€œLM å¤´â€å°†ç›®æ ‡å‘é‡çš„ç¼–ç åºåˆ— \\(\\mathbf{\\overline{Y}}\_{0:i-1}\\) æ˜ å°„åˆ° logit å‘é‡åºåˆ— \\(\\mathbf {L}\_{1:n} = \\mathbf{l}\_1, \\ldots, \\mathbf{l}\_n\\), è€Œæ¯ä¸ª logit å‘é‡\\(\\mathbf{l}\_i\\) çš„ç»´åº¦å³ä¸ºè¯è¡¨çš„è¯æ±‡é‡ã€‚è¿™æ ·ï¼Œå¯¹äºæ¯ä¸ª \\(i \\in {1, \\ldots, n}\\)ï¼Œå…¶åœ¨æ•´ä¸ªè¯æ±‡è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒå¯ä»¥é€šè¿‡å¯¹ \\(\\mathbf{l}\_i\\) å– softmax è·å¾—ã€‚å…¬å¼å¦‚ä¸‹:

\\\[p\_{\\theta\_{dec}}(\\mathbf{y}\_i | \\mathbf{Y}\_{0: i-1}, \\mathbf{\\overline{X}}\_{1:n}), \\forall i \\in {1, \\ldots, n} \\\]

â€œLM å¤´â€ å³ä¸ºè¯åµŒå…¥çŸ©é˜µçš„è½¬ç½®ï¼Œ _å³_ \\(\\mathbf{W}\_{\\text{emb}}^{\\intercal} = \\left\[\\mathbf{ y}^1, \\ldots, \\mathbf{y}^{\\text{vocab}}\\right\]^{â€‹â€‹T}\\) \\({}^1\\)ã€‚ç›´è§‚ä¸Šæ¥è®²ï¼Œè¿™æ„å‘³ç€å¯¹äºæ‰€æœ‰ \\(i \\in {0, \\ldots, n - 1}\\) â€œLM å¤´â€ å±‚ä¼šå°† \\(\\mathbf{\\overline{y }}\_i\\) ä¸è¯æ±‡è¡¨ \\(\\mathbf{y}^1, \\ldots, \\mathbf{y}^{\\text{vocab}}\\) ä¸­çš„æ‰€æœ‰è¯åµŒå…¥ä¸€ä¸€æ¯”è¾ƒï¼Œè¾“å‡ºçš„ logit å‘é‡ \\(\\mathbf{l}\_{i+1}\\) å³è¡¨ç¤º \\(\\mathbf{\\overline{y }}\_i\\) ä¸æ¯ä¸ªè¯åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚Softmax æ“ä½œåªæ˜¯å°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚å¯¹äºæ¯ä¸ª \\(i \\in {1, \\ldots, n}\\)ï¼Œä»¥ä¸‹ç­‰å¼æˆç«‹:

\\\[p\_{\\theta\_{dec}}(\\mathbf{y} | \\mathbf{\\overline{X}}\_{1:n}, \\mathbf{Y}\_{0:i-1}) \\\]

\\\[= \\text{Softmax}(f\_{\\theta\_{\\text{dec}}}(\\mathbf{\\overline{X}}\_{1:n}, \\mathbf{Y}\_{0:i-1})) \\\]

\\\[= \\text{Softmax}(\\mathbf{W}\_{\\text{emb}}^{\\intercal} \\mathbf{\\overline{y}}\_{i-1}) \\\]

\\\[= \\text{Softmax}(\\mathbf{l}\_i) \\\]

æ€»ç»“ä¸€ä¸‹ï¼Œä¸ºäº†å¯¹ç›®æ ‡å‘é‡åºåˆ— \\(\\mathbf{Y}\_{1: m}\\) çš„æ¡ä»¶åˆ†å¸ƒå»ºæ¨¡ï¼Œå…ˆåœ¨ç›®æ ‡å‘é‡ \\(\\mathbf{Y}\_{1: m-1}\\) å‰é¢åŠ ä¸Šç‰¹æ®Šçš„ \\(\\text{BOS}\\) å‘é‡ ( _å³_ \\(\\mathbf{y}\_0\\))ï¼Œå¹¶å°†å…¶ä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„ç¼–ç åºåˆ— \\(\\mathbf{\\overline{X}}\_{1:n}\\) ä¸€èµ·æ˜ å°„åˆ° logit å‘é‡åºåˆ— \\(\\mathbf{L}\_{1:m}\\)ã€‚ç„¶åï¼Œä½¿ç”¨ softmax æ“ä½œå°†æ¯ä¸ª logit ç›®æ ‡å‘é‡ \\(\\mathbf{l}\_i\\) è½¬æ¢ä¸ºç›®æ ‡å‘é‡ \\(\\mathbf{y}\_i\\) çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚æœ€åï¼Œå°†æ‰€æœ‰ç›®æ ‡å‘é‡çš„æ¡ä»¶æ¦‚ç‡ \\(\\mathbf{y}\_1, \\ldots, \\mathbf{y}\_m\\) ç›¸ä¹˜å¾—åˆ°å®Œæ•´ç›®æ ‡å‘é‡åºåˆ—çš„æ¡ä»¶æ¦‚ç‡:

\\\[p\_{\\theta\_{dec}}(\\mathbf{Y}\_{1:m} | \\mathbf{\\overline{X}}\_{1:n}) = \\prod\_{i=1}^{m} p\_{\\theta\_{dec}}(\\mathbf{y}\_i | \\mathbf{Y}\_{0: i-1}, \\mathbf{\\overline{X}}\_{1:n}). \\\]

ä¸åŸºäº transformer çš„ç¼–ç å™¨ä¸åŒï¼Œåœ¨åŸºäº transformer çš„è§£ç å™¨ä¸­ï¼Œå…¶è¾“å‡ºå‘é‡ \\(\\mathbf{\\overline{y}}\_{i-1}\\) åº”è¯¥èƒ½å¾ˆå¥½åœ°è¡¨å¾ _ä¸‹ä¸€ä¸ª_ ç›®æ ‡å‘é‡ (å³ \\(\\mathbf{y}\_i\\))ï¼Œè€Œä¸æ˜¯è¾“å…¥å‘é‡æœ¬èº« (å³ \\(\\mathbf{y}\_{i-1}\\))ã€‚æ­¤å¤–ï¼Œè¾“å‡ºå‘é‡ \\(\\mathbf{\\overline{y}}\_{i-1}\\) åº”åŸºäºç¼–ç å™¨çš„æ•´ä¸ªè¾“å‡ºåºåˆ— \\(\\mathbf{\\overline{X}}\_{1:n}\\)ã€‚ä¸ºäº†æ»¡è¶³è¿™äº›è¦æ±‚ï¼Œæ¯ä¸ªè§£ç å™¨å—éƒ½åŒ…å«ä¸€ä¸ª **å•å‘**è‡ªæ³¨æ„å±‚ï¼Œç´§æ¥ç€æ˜¯ä¸€ä¸ª **äº¤å‰æ³¨æ„**å±‚ï¼Œæœ€åæ˜¯ä¸¤ä¸ªå‰é¦ˆå±‚\\({}^2\\)ã€‚å•å‘è‡ªæ³¨æ„å±‚å°†å…¶æ¯ä¸ªè¾“å…¥å‘é‡ \\(\\mathbf{y'}\_j\\) ä»…ä¸å…¶å‰é©±è¾“å…¥å‘é‡ \\(\\mathbf{y'}\_i\\) (å…¶ä¸­ \\(i \\le j\\)ï¼Œä¸” \\(j \\in {1, \\ldots, n}\\)) ç›¸å…³è”ï¼Œæ¥æ¨¡æ‹Ÿä¸‹ä¸€ä¸ªç›®æ ‡å‘é‡çš„æ¦‚ç‡åˆ†å¸ƒã€‚äº¤å‰æ³¨æ„å±‚å°†å…¶æ¯ä¸ªè¾“å…¥å‘é‡ \\(\\mathbf{y''}\_j\\) ä¸ç¼–ç å™¨è¾“å‡ºçš„æ‰€æœ‰å‘é‡ \\(\\mathbf{\\overline{X}}\_{1:n}\\) ç›¸å…³è”ï¼Œæ¥æ ¹æ®ç¼–ç å™¨è¾“å…¥é¢„æµ‹ä¸‹ä¸€ä¸ªç›®æ ‡å‘é‡çš„æ¦‚ç‡åˆ†å¸ƒã€‚

å¥½ï¼Œæˆ‘ä»¬ä»ä»¥è‹±è¯­åˆ°å¾·è¯­ç¿»è¯‘ä¸ºä¾‹å¯è§†åŒ–ä¸€ä¸‹ _åŸºäº transformer_ çš„è§£ç å™¨ã€‚

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/encoder_decoder_detail.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è§£ç å™¨å°† \\(\\mathbf{Y}\_{0:5}\\): â€œBOSâ€ã€â€œIchâ€ã€â€œwillâ€ã€â€œeinâ€ã€â€œAutoâ€ã€â€œkaufenâ€ (å›¾ä¸­ä»¥æµ…çº¢è‰²æ˜¾ç¤º) å’Œ â€œIâ€ã€â€œwantâ€ã€â€œtoâ€ã€â€œbuyâ€ã€â€œaâ€ã€â€œcarâ€ã€â€œEOSâ€ ( _å³_ \\(\\mathbf{\\overline{X}}\_{1:7}\\) (å›¾ä¸­ä»¥æ·±ç»¿è‰²æ˜¾ç¤º)) æ˜ å°„åˆ° logit å‘é‡ \\(\\mathbf{L}\_{1:6}\\) (å›¾ä¸­ä»¥æ·±çº¢è‰²æ˜¾ç¤º)ã€‚

å› æ­¤ï¼Œå¯¹æ¯ä¸ª \\(\\mathbf{l}\_1ã€\\mathbf{l}\_2ã€\\ldotsã€\\mathbf{l}\_6\\) ä½¿ç”¨ softmax æ“ä½œå¯ä»¥å®šä¹‰ä¸‹åˆ—æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ:

\\\[p\_{\\theta\_{dec}}(\\mathbf{y} | \\text{BOS}, \\mathbf{\\overline{X}}\_{1:7}), \\\]

> \\\[p\_{\\theta\_{dec}}(\\mathbf{y} | \\text{BOS Ich}, \\mathbf{\\overline{X}}\_{1:7}), \\\]
> 
> \\\[\\ldots, \\\]
> 
> \\\[p\_{\\theta\_{dec}}(\\mathbf{y} | \\text{BOS Ich will ein Auto kaufen}, \\mathbf{\\overline{X}}\_{1:7}) \\\]

æ€»æ¡ä»¶æ¦‚ç‡å¦‚ä¸‹:

\\\[p\_{\\theta\_{dec}}(\\text{Ich will ein Auto kaufen EOS} | \\mathbf{\\overline{X}}\_{1:n}) \\\]

å…¶å¯è¡¨ç¤ºä¸ºä»¥ä¸‹ä¹˜ç§¯å½¢å¼:

\\\[p\_{\\theta\_{dec}}(\\text{Ich} | \\text{BOS}, \\mathbf{\\overline{X}}\_{1:7}) \\times \\ldots \\times p\_{\\theta\_{dec}}(\\text{EOS} | \\text{BOS Ich will ein Auto kaufen}, \\mathbf{\\overline{X}}\_{1:7}) \\\]

å›¾å³ä¾§çš„çº¢æ¡†æ˜¾ç¤ºäº†å‰ä¸‰ä¸ªç›®æ ‡å‘é‡ \\(\\mathbf{y}\_0\\)ã€\\(\\mathbf{y}\_1\\)ã€ \\(\\mathbf{y}\_2\\) åœ¨ä¸€ä¸ªè§£ç å™¨æ¨¡å—ä¸­çš„è¡Œä¸ºã€‚ä¸‹åŠéƒ¨åˆ†è¯´æ˜äº†å•å‘è‡ªæ³¨æ„æœºåˆ¶ï¼Œä¸­é—´è¯´æ˜äº†äº¤å‰æ³¨æ„æœºåˆ¶ã€‚æˆ‘ä»¬é¦–å…ˆå…³æ³¨å•å‘è‡ªæ³¨æ„åŠ›ã€‚

ä¸åŒå‘è‡ªæ³¨æ„ä¸€æ ·ï¼Œåœ¨å•å‘è‡ªæ³¨æ„ä¸­ï¼Œ `query` å‘é‡ \\(\\mathbf{q}\_0, \\ldots, \\mathbf{q}\_{m-1}\\) (å¦‚ä¸‹å›¾ç´«è‰²æ‰€ç¤º)ï¼Œ `key` å‘é‡ \\(\\mathbf{k}\_0, \\ldots, \\mathbf{k}\_{m-1}\\) (å¦‚ä¸‹å›¾æ©™è‰²æ‰€ç¤º)ï¼Œå’Œ `value` å‘é‡ \\(\\mathbf{v }\_0, \\ldots, \\mathbf{v}\_{m-1}\\) (å¦‚ä¸‹å›¾è“è‰²æ‰€ç¤º) å‡ç”±è¾“å…¥å‘é‡ \\(\\mathbf{y'}\_0, \\ldots, \\mathbf{ y'}\_{m-1}\\) (å¦‚ä¸‹å›¾æµ…çº¢è‰²æ‰€ç¤º) æ˜ å°„è€Œæ¥ã€‚ç„¶è€Œï¼Œåœ¨å•å‘è‡ªæ³¨æ„åŠ›ä¸­ï¼Œæ¯ä¸ª `query` å‘é‡ \\(\\mathbf{q}\_i\\) _ä»…_ ä¸å½“å‰åŠä¹‹å‰çš„ `key` å‘é‡è¿›è¡Œæ¯”è¾ƒ (å³ \\(\\mathbf{k}\_0 , \\ldots, \\mathbf{k}\_i\\)) å¹¶ç”Ÿæˆå„è‡ªçš„ _æ³¨æ„åŠ›æƒé‡_ ã€‚è¿™å¯ä»¥é˜²æ­¢è¾“å‡ºå‘é‡ \\(\\mathbf{y''}\_j\\) (å¦‚ä¸‹å›¾æ·±çº¢è‰²æ‰€ç¤º) åŒ…å«æœªæ¥å‘é‡ (\\(\\mathbf{y}\_i\\)ï¼Œå…¶ä¸­ \\(i > j\\) ä¸” \\(j \\in {0, \\ldots, m - 1 }\\)) çš„ä»»ä½•ä¿¡æ¯ ã€‚ä¸åŒå‘è‡ªæ³¨æ„åŠ›çš„æƒ…å†µä¸€æ ·ï¼Œå¾—åˆ°çš„æ³¨æ„åŠ›æƒé‡ä¼šä¹˜ä»¥å®ƒä»¬å„è‡ªçš„ `value` å‘é‡å¹¶åŠ æƒæ±‚å’Œã€‚

æˆ‘ä»¬å°†å•å‘è‡ªæ³¨æ„åŠ›æ€»ç»“å¦‚ä¸‹:

\\\[\\mathbf{y''}\_i = \\mathbf{V}\_{0: i} \\textbf{Softmax}(\\mathbf{K}\_{0: i}^\\intercal \\mathbf{q}\_i) + \\mathbf{y'}\_i \\\]

è¯·æ³¨æ„ï¼Œ `key` å’Œ `value` å‘é‡çš„ç´¢å¼•èŒƒå›´éƒ½æ˜¯ \\(0:i\\) è€Œä¸æ˜¯ \\(0: m-1\\)ï¼Œ\\(0: m-1\\) æ˜¯åŒå‘è‡ªæ³¨æ„åŠ›ä¸­ `key` å‘é‡çš„ç´¢å¼•èŒƒå›´ã€‚

ä¸‹å›¾æ˜¾ç¤ºäº†ä¸Šä¾‹ä¸­è¾“å…¥å‘é‡ \\(\\mathbf{y'}\_1\\) çš„å•å‘è‡ªæ³¨æ„åŠ›ã€‚

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/causal_attn.png)

å¯ä»¥çœ‹å‡º \\(\\mathbf{y''}\_1\\) åªä¾èµ–äº \\(\\mathbf{y'}\_0\\) å’Œ \\(\\mathbf{y'}\_1\\)ã€‚å› æ­¤ï¼Œå•è¯ â€œIchâ€ çš„å‘é‡è¡¨å¾ ( _å³_ \\(\\mathbf{y'}\_1\\)) ä»…ä¸å…¶è‡ªèº«åŠ â€œBOSâ€ ç›®æ ‡å‘é‡ ( _å³_ \\(\\mathbf{y'}\_0\\)) ç›¸å…³è”ï¼Œè€Œ **ä¸** ä¸ â€œwillâ€ çš„å‘é‡è¡¨å¾ ( _å³_ \\(\\mathbf{y'}\_2\\)) ç›¸å…³è”ã€‚

é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆè§£ç å™¨ä½¿ç”¨å•å‘è‡ªæ³¨æ„åŠ›è€Œä¸æ˜¯åŒå‘è‡ªæ³¨æ„åŠ›è¿™ä»¶äº‹å¾ˆé‡è¦å‘¢ï¼Ÿå¦‚å‰æ‰€è¿°ï¼ŒåŸºäº transformer çš„è§£ç å™¨å®šä¹‰äº†ä»è¾“å…¥å‘é‡åºåˆ— \\(\\mathbf{Y}\_{0: m-1}\\) åˆ°å…¶ **ä¸‹ä¸€ä¸ª** è§£ç å™¨è¾“å…¥çš„ logit å‘é‡çš„æ˜ å°„ï¼Œå³ \\(\\mathbf{L}\_{1:m}\\)ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œè¾“å…¥å‘é‡ \\(\\mathbf{y}\_1\\) = â€œIchâ€ ä¼šæ˜ å°„åˆ° logit å‘é‡ \\(\\mathbf{l}\_2\\)ï¼Œå¹¶ç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªè¾“å…¥å‘é‡ \\(\\mathbf{y}\_2\\)ã€‚å› æ­¤ï¼Œå¦‚æœ \\(\\mathbf{y'}\_1\\) å¯ä»¥è·å–åç»­è¾“å…¥å‘é‡ \\(\\mathbf{Y'}\_{2:5}\\)çš„ä¿¡æ¯ï¼Œè§£ç å™¨å°†ä¼šç®€å•åœ°å¤åˆ¶å‘é‡ â€œwillâ€ çš„å‘é‡è¡¨å¾ ( _å³_ \\(\\mathbf{y'}\_2\\)) ä½œä¸ºå…¶è¾“å‡º \\(\\mathbf{y''}\_1\\)ï¼Œå¹¶å°±è¿™æ ·ä¸€ç›´ä¼ æ’­åˆ°æœ€åä¸€å±‚ï¼Œæ‰€ä»¥æœ€ç»ˆçš„è¾“å‡ºå‘é‡ \\(\\mathbf{\\overline{y}}\_1\\) åŸºæœ¬ä¸Šå°±åªå¯¹åº”äº \\(\\mathbf{y}\_2\\) çš„å‘é‡è¡¨å¾ï¼Œå¹¶æ²¡æœ‰èµ·åˆ°é¢„æµ‹çš„ä½œç”¨ã€‚

è¿™æ˜¾ç„¶æ˜¯ä¸å¯¹çš„ï¼Œå› ä¸ºè¿™æ ·çš„è¯ï¼ŒåŸºäº transformer çš„è§£ç å™¨æ°¸è¿œä¸ä¼šå­¦åˆ°åœ¨ç»™å®šæ‰€æœ‰å‰é©±è¯çš„æƒ…å†µä¸‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œè€Œåªæ˜¯å¯¹æ‰€æœ‰ \\(i \\in {1, \\ldots, m }\\)ï¼Œé€šè¿‡ç½‘ç»œå°†ç›®æ ‡å‘é‡ \\(\\mathbf{y}\_i\\) å¤åˆ¶åˆ° \\(\\mathbf {\\overline{y}}\_{i-1}\\)ã€‚ä»¥ä¸‹ä¸€ä¸ªç›®æ ‡å˜é‡æœ¬èº«ä¸ºæ¡ä»¶å»å®šä¹‰ä¸‹ä¸€ä¸ªç›®æ ‡å‘é‡ï¼Œå³ä» \\(p(\\mathbf{y} | \\mathbf{Y}\_{0:i}, \\mathbf{\\overline{ X}})\\) ä¸­é¢„æµ‹ \\(\\mathbf{y}\_i\\)ï¼Œ æ˜¾ç„¶æ˜¯ä¸å¯¹çš„ã€‚å› æ­¤ï¼Œå•å‘è‡ªæ³¨æ„åŠ›æ¶æ„å…è®¸æˆ‘ä»¬å®šä¹‰ä¸€ä¸ª _å› æœçš„_ æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™å¯¹æœ‰æ•ˆå»ºæ¨¡ä¸‹ä¸€ä¸ªç›®æ ‡å‘é‡çš„æ¡ä»¶åˆ†å¸ƒè€Œè¨€æ˜¯å¿…è¦çš„ã€‚

å¤ªæ£’äº†ï¼ç°åœ¨æˆ‘ä»¬å¯ä»¥è½¬åˆ°è¿æ¥ç¼–ç å™¨å’Œè§£ç å™¨çš„å±‚ - _äº¤å‰æ³¨æ„åŠ›_ æœºåˆ¶ï¼

äº¤å‰æ³¨æ„å±‚å°†ä¸¤ä¸ªå‘é‡åºåˆ—ä½œä¸ºè¾“å…¥: å•å‘è‡ªæ³¨æ„å±‚çš„è¾“å‡º \\(\\mathbf{Y''}\_{0: m-1}\\) å’Œç¼–ç å™¨çš„è¾“å‡º \\(\\mathbf{\\overline{X}}\_{1:n}\\)ã€‚ä¸è‡ªæ³¨æ„åŠ›å±‚ä¸€æ ·ï¼Œ `query` å‘é‡ \\(\\mathbf{q}\_0, \\ldots, \\mathbf{q}\_{m-1}\\) æ˜¯ä¸Šä¸€å±‚è¾“å‡ºå‘é‡ \\(\\mathbf{Y''}\_{0: m-1}\\) çš„æŠ•å½±ã€‚è€Œ `key` å’Œ `value` å‘é‡ \\(\\mathbf{k}\_0, \\ldots, \\mathbf{k}\_{n-1}\\)ã€\\(\\mathbf{v}\_0, \\ldots, \\mathbf {v}\_{n-1}\\) æ˜¯ç¼–ç å™¨è¾“å‡ºå‘é‡ \\(\\mathbf{\\overline{X}}\_{1:n}\\) çš„æŠ•å½±ã€‚å®šä¹‰å®Œ `key` ã€`value` å’Œ `query` å‘é‡åï¼Œå°† `query` å‘é‡ \\(\\mathbf{q}\_i\\) ä¸ _æ‰€æœ‰_ `key` å‘é‡è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶ç”¨å„è‡ªçš„å¾—åˆ†å¯¹ç›¸åº”çš„ `value` å‘é‡è¿›è¡ŒåŠ æƒæ±‚å’Œã€‚è¿™ä¸ªè¿‡ç¨‹ä¸ _åŒå‘_ è‡ªæ³¨æ„åŠ›å¯¹æ‰€æœ‰ \\(i \\in {0, \\ldots, m-1}\\) æ±‚ \\(\\mathbf{y'''}\_i\\) æ˜¯ä¸€æ ·çš„ã€‚äº¤å‰æ³¨æ„åŠ›å¯ä»¥æ¦‚æ‹¬å¦‚ä¸‹:

\\\[\\mathbf{y'''}\_i = \\mathbf{V}\_{1:n} \\textbf{Softmax}(\\mathbf{K}\_{1: n}^\\intercal \\mathbf{q}\_i) + \\mathbf{y''}\_i \\\]

æ³¨æ„ï¼Œ`key` å’Œ `value` å‘é‡çš„ç´¢å¼•èŒƒå›´æ˜¯ \\(1:n\\)ï¼Œå¯¹åº”äºç¼–ç å™¨è¾“å…¥å‘é‡çš„æ•°ç›®ã€‚

æˆ‘ä»¬ç”¨ä¸Šä¾‹ä¸­è¾“å…¥å‘é‡ \\(\\mathbf{y''}\_1\\) æ¥å›¾è§£ä¸€ä¸‹äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ã€‚

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/cross_attention.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ° `query` å‘é‡ \\(\\mathbf{q}\_1\\)ï¼ˆç´«è‰²ï¼‰æºè‡ª \\(\\mathbf{y''}\_1\\)ï¼ˆçº¢è‰²ï¼‰ï¼Œå› æ­¤å…¶ä¾èµ–äºå•è¯ "Ich" çš„å‘é‡è¡¨å¾ã€‚ç„¶åå°† `query` å‘é‡ \\(\\mathbf{q}\_1\\) ä¸å¯¹åº”çš„ `key` å‘é‡ \\(\\mathbf{k}\_1, \\ldots, \\mathbf{k}\_7\\)ï¼ˆé»„è‰²ï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œè¿™é‡Œçš„ `key` å‘é‡å¯¹åº”äºç¼–ç å™¨å¯¹å…¶è¾“å…¥ \\(\\mathbf{X}\_{1:n}\\) = "I want to buy a car EOS" çš„ä¸Šä¸‹æ–‡ç›¸å…³å‘é‡è¡¨å¾ã€‚è¿™å°† "Ich" çš„å‘é‡è¡¨å¾ä¸æ‰€æœ‰ç¼–ç å™¨è¾“å…¥å‘é‡ç›´æ¥å…³è”èµ·æ¥ã€‚æœ€åï¼Œå°†æ³¨æ„åŠ›æƒé‡ä¹˜ä»¥ `value` å‘é‡ \\(\\mathbf{v}\_1, \\ldots, \\mathbf{v}\_7\\)ï¼ˆé’ç»¿è‰²ï¼‰å¹¶åŠ ä¸Šè¾“å…¥å‘é‡ \\(\\mathbf{y''}\_1\\) æœ€ç»ˆå¾—åˆ°è¾“å‡ºå‘é‡ \\(\\mathbf{y'''}\_1\\)ï¼ˆæ·±çº¢è‰²ï¼‰ã€‚

æ‰€ä»¥ï¼Œç›´è§‚è€Œè¨€ï¼Œåˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿæ¯ä¸ªè¾“å‡ºå‘é‡ \\(\\mathbf{y'''}\_i\\) æ˜¯ç”±æ‰€æœ‰ä»ç¼–ç å™¨æ¥çš„ `value` å‘é‡ï¼ˆ\\(\\mathbf{v}\_{1}, \\ldots, \\mathbf{v}\_7\\) ï¼‰çš„åŠ æƒå’Œä¸è¾“å…¥å‘é‡æœ¬èº« \\(\\mathbf{y''}\_i\\) ç›¸åŠ è€Œå¾—ï¼ˆå‚è§ä¸Šå›¾æ‰€ç¤ºçš„å…¬å¼ï¼‰ã€‚å…¶å…³é”®æ€æƒ³æ˜¯ï¼š _æ¥è‡ªè§£ç å™¨çš„_ \\(\\mathbf{q}\_i\\) çš„ `query` æŠ•å½±ä¸ _æ¥è‡ªç¼–ç å™¨çš„ \\(\\mathbf{k}\_j\\)_ è¶Šç›¸å…³ï¼Œå…¶å¯¹åº”çš„ \\(\\mathbf{v}\_j\\) å¯¹è¾“å‡ºçš„å½±å“è¶Šå¤§ã€‚

é…·ï¼ç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ç§æ¶æ„çš„æ¯ä¸ªè¾“å‡ºå‘é‡ \\(\\mathbf{y'''}\_i\\) å–å†³äºå…¶æ¥è‡ªç¼–ç å™¨çš„è¾“å…¥å‘é‡ \\(\\mathbf{\\overline{X}}\_{1 :n}\\) åŠå…¶è‡ªèº«çš„è¾“å…¥å‘é‡ \\(\\mathbf{y''}\_i\\)ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªé‡è¦çš„ç‚¹ï¼Œåœ¨è¯¥æ¶æ„ä¸­ï¼Œè™½ç„¶è¾“å‡ºå‘é‡ \\(\\mathbf{y'''}\_i\\) ä¾èµ–æ¥è‡ªç¼–ç å™¨çš„è¾“å…¥å‘é‡ \\(\\mathbf{\\overline{X}}\_{1:n}\\)ï¼Œä½†å…¶å®Œå…¨ç‹¬ç«‹äºè¯¥å‘é‡çš„æ•°é‡ \\(n\\)ã€‚æ‰€æœ‰ç”Ÿæˆ `key` å‘é‡ \\(\\mathbf{k}\_1, \\ldots, \\mathbf{k}\_n\\) å’Œ `value` å‘é‡ $\\mathbf{v}\_1, \\ldots, \\mathbf{v}\_n $ çš„æŠ•å½±çŸ©é˜µ \\(\\mathbf{W}^{\\text{cross}}\_{k}\\) å’Œ \\(\\mathbf{W}^{\\text{cross}}\_{v}\\) éƒ½æ˜¯ä¸ \\(n\\) æ— å…³çš„ï¼Œæ‰€æœ‰ \\(n\\) å…±äº«åŒä¸€ä¸ªæŠ•å½±çŸ©é˜µã€‚ä¸”å¯¹æ¯ä¸ª \\(\\mathbf{y'''}\_i\\)ï¼Œæ‰€æœ‰ `value` å‘é‡ \\(\\mathbf{v}\_1, \\ldots, \\mathbf{v}\_n\\) è¢«åŠ æƒæ±‚å’Œè‡³ä¸€ä¸ªå‘é‡ã€‚è‡³æ­¤ï¼Œå…³äº`ä¸ºä»€ä¹ˆåŸºäº transformer çš„è§£ç å™¨æ²¡æœ‰è¿œç¨‹ä¾èµ–é—®é¢˜è€ŒåŸºäº RNN çš„è§£ç å™¨æœ‰`è¿™ä¸€é—®é¢˜çš„ç­”æ¡ˆå·²ç»å¾ˆæ˜¾ç„¶äº†ã€‚å› ä¸ºæ¯ä¸ªè§£ç å™¨ logit å‘é‡ _ç›´æ¥_ ä¾èµ–äºæ¯ä¸ªç¼–ç åçš„è¾“å‡ºå‘é‡ï¼Œå› æ­¤æ¯”è¾ƒç¬¬ä¸€ä¸ªç¼–ç è¾“å‡ºå‘é‡å’Œæœ€åä¸€ä¸ªè§£ç å™¨ logit å‘é‡åªéœ€ä¸€æ¬¡æ“ä½œï¼Œè€Œä¸åƒ RNN éœ€è¦å¾ˆå¤šæ¬¡ã€‚

æ€»è€Œè¨€ä¹‹ï¼Œå•å‘è‡ªæ³¨æ„åŠ›å±‚è´Ÿè´£åŸºäºå½“å‰åŠä¹‹å‰çš„æ‰€æœ‰è§£ç å™¨è¾“å…¥å‘é‡å»ºæ¨¡æ¯ä¸ªè¾“å‡ºå‘é‡ï¼Œè€Œäº¤å‰æ³¨æ„åŠ›å±‚åˆ™è´Ÿè´£è¿›ä¸€æ­¥åŸºäºç¼–ç å™¨çš„æ‰€æœ‰è¾“å…¥å‘é‡å»ºæ¨¡æ¯ä¸ªè¾“å‡ºå‘é‡ã€‚

ä¸ºäº†éªŒè¯æˆ‘ä»¬å¯¹è¯¥ç†è®ºçš„ç†è§£ï¼Œæˆ‘ä»¬ç»§ç»­ä¸Šé¢ç¼–ç å™¨éƒ¨åˆ†çš„ä»£ç ï¼Œå®Œæˆè§£ç å™¨éƒ¨åˆ†ã€‚

* * *

\\({}^1\\) è¯åµŒå…¥çŸ©é˜µ \\(\\mathbf{W}\_{\\text{emb}}\\) ä¸ºæ¯ä¸ªè¾“å…¥è¯æä¾›å”¯ä¸€çš„ _ä¸Šä¸‹æ–‡æ— å…³_ å‘é‡è¡¨ç¤ºã€‚è¿™ä¸ªçŸ©é˜µé€šå¸¸ä¹Ÿè¢«ç”¨ä½œ â€œLM å¤´â€ï¼Œæ­¤æ—¶ â€œLM å¤´â€å¯ä»¥å¾ˆå¥½åœ°å®Œæˆâ€œç¼–ç å‘é‡åˆ° logitâ€ çš„æ˜ å°„ã€‚

\\({}^2\\) ä¸ç¼–ç å™¨éƒ¨åˆ†ä¸€æ ·ï¼Œæœ¬æ–‡ä¸ä¼šè¯¦ç»†è§£é‡Šå‰é¦ˆå±‚åœ¨åŸºäº transformer çš„æ¨¡å‹ä¸­çš„ä½œç”¨ã€‚[Yun ç­‰ (2017)](https://arxiv.org/pdf/1912.10077.pdf) çš„å·¥ä½œè®¤ä¸ºå‰é¦ˆå±‚å¯¹äºå°†æ¯ä¸ªä¸Šä¸‹æ–‡ç›¸å…³å‘é‡ \\(\\mathbf{x'}\_i\\) æ˜ å°„åˆ°æ‰€éœ€çš„è¾“å‡ºç©ºé—´è‡³å…³é‡è¦ï¼Œä»…é è‡ªæ³¨æ„åŠ›å±‚æ— æ³•å®Œæˆã€‚è¿™é‡Œåº”è¯¥æ³¨æ„ï¼Œæ¯ä¸ªè¾“å‡ºè¯å…ƒ \\(\\mathbf{x'}\\) å¯¹åº”çš„å‰é¦ˆå±‚æ˜¯ç›¸åŒçš„ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œå»ºè®®è¯»è€…é˜…è¯»è®ºæ–‡ã€‚

    from transformers import MarianMTModel, MarianTokenizer
    import torch
    
    tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-de")
    model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-de")
    embeddings = model.get_input_embeddings()
    
    # create token ids for encoder input
    input_ids = tokenizer("I want to buy a car", return_tensors="pt").input_ids
    
    # pass input token ids to encoder
    encoder_output_vectors = model.base_model.encoder(input_ids, return_dict=True).last_hidden_state
    
    # create token ids for decoder input
    decoder_input_ids = tokenizer("<pad> Ich will ein", return_tensors="pt", add_special_tokens=False).input_ids
    
    # pass decoder input ids and encoded input vectors to decoder
    decoder_output_vectors = model.base_model.decoder(decoder_input_ids, encoder_hidden_states=encoder_output_vectors).last_hidden_state
    
    # derive embeddings by multiplying decoder outputs with embedding weights
    lm_logits = torch.nn.functional.linear(decoder_output_vectors, embeddings.weight, bias=model.final_logits_bias)
    
    # change the decoder input slightly
    decoder_input_ids_perturbed = tokenizer("<pad> Ich will das", return_tensors="pt", add_special_tokens=False).input_ids
    decoder_output_vectors_perturbed = model.base_model.decoder(decoder_input_ids_perturbed, encoder_hidden_states=encoder_output_vectors).last_hidden_state
    lm_logits_perturbed = torch.nn.functional.linear(decoder_output_vectors_perturbed, embeddings.weight, bias=model.final_logits_bias)
    
    # compare shape and encoding of first vector
    print(f"Shape of decoder input vectors {embeddings(decoder_input_ids).shape}. Shape of decoder logits {lm_logits.shape}")
    
    # compare values of word embedding of "I" for input_ids and perturbed input_ids
    print("Is encoding for `Ich` equal to its perturbed version?: ", torch.allclose(lm_logits[0, 0], lm_logits_perturbed[0, 0], atol=1e-3))
    

_è¾“å‡º:_

        Shape of decoder input vectors torch.Size([1, 5, 512]). Shape of decoder logits torch.Size([1, 5, 58101])
        Is encoding for `Ich` equal to its perturbed version?: True
    

æˆ‘ä»¬é¦–å…ˆæ¯”è¾ƒè§£ç å™¨è¯åµŒå…¥å±‚çš„è¾“å‡ºç»´åº¦ `embeddings(decoder_input_ids)` (å¯¹åº”äº \\(\\mathbf{Y}\_{0: 4}\\)ï¼Œè¿™é‡Œ `<pad>` å¯¹åº”äº BOS ä¸” "Ich will das" è¢«åˆ†ä¸º 4 ä¸ªè¯) å’Œ `lm_logits` (å¯¹åº”äº \\(\\mathbf{L}\_{1:5}\\)) çš„ç»´åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡è§£ç å™¨å°†å•è¯åºåˆ— â€œ`<pad>` Ich will einâ€ å’Œå…¶è½»å¾®æ”¹ç¼–ç‰ˆ â€œ`<pad>` Ich will dasâ€ ä¸ `encoder_output_vectors` ä¸€èµ·ä¼ é€’ç»™è§£ç å™¨ï¼Œä»¥æ£€æŸ¥å¯¹åº”äº â€œIchâ€ çš„ç¬¬äºŒä¸ª lm\_logit åœ¨ä»…æ”¹å˜è¾“å…¥åºåˆ—ä¸­çš„æœ€åä¸€ä¸ªå•è¯ (â€œeinâ€ -> â€œdasâ€) æ—¶æ˜¯å¦ä¼šæœ‰æ‰€ä¸åŒã€‚

æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œè§£ç å™¨è¾“å…¥è¯åµŒå…¥å’Œ lm\_logits çš„è¾“å‡ºï¼Œ _å³_ \\(\\mathbf{Y}\_{0: 4}\\) å’Œ \\(\\mathbf{L}\_{ 1:5}\\) çš„æœ€åä¸€ä¸ªç»´åº¦ä¸åŒã€‚è™½ç„¶åºåˆ—é•¿åº¦ç›¸åŒ (=5)ï¼Œä½†è§£ç å™¨è¾“å…¥è¯åµŒå…¥çš„ç»´åº¦å¯¹åº”äº `model.config.hidden_â€‹â€‹size`ï¼Œè€Œ `lm_logit` çš„ç»´æ•°å¯¹åº”äºè¯æ±‡è¡¨å¤§å° `model.config.vocab_size`ã€‚å…¶æ¬¡ï¼Œå¯ä»¥æ³¨æ„åˆ°ï¼Œå½“å°†æœ€åä¸€ä¸ªå•è¯ä» â€œeinâ€ å˜ä¸º â€œdasâ€ï¼Œ\\(\\mathbf{l}\_1 = \\text{â€œIchâ€}\\) çš„è¾“å‡ºå‘é‡çš„å€¼ä¸å˜ã€‚é‰´äºæˆ‘ä»¬å·²ç»ç†è§£äº†å•å‘è‡ªæ³¨æ„åŠ›ï¼Œè¿™å°±ä¸è¶³ä¸ºå¥‡äº†ã€‚

æœ€åä¸€ç‚¹ï¼Œ _è‡ªå›å½’_ æ¨¡å‹ï¼Œå¦‚ GPT2ï¼Œä¸åˆ é™¤äº†äº¤å‰æ³¨æ„åŠ›å±‚çš„ _åŸºäº transformer_ çš„è§£ç å™¨æ¨¡å‹æ¶æ„æ˜¯ç›¸åŒçš„ï¼Œå› ä¸ºçº¯è‡ªå›å½’æ¨¡å‹ä¸ä¾èµ–ä»»ä½•ç¼–ç å™¨çš„è¾“å‡ºã€‚å› æ­¤ï¼Œè‡ªå›å½’æ¨¡å‹æœ¬è´¨ä¸Šä¸ _è‡ªç¼–ç _ æ¨¡å‹ç›¸åŒï¼Œåªæ˜¯ç”¨å•å‘æ³¨æ„åŠ›ä»£æ›¿äº†åŒå‘æ³¨æ„åŠ›ã€‚è¿™äº›æ¨¡å‹è¿˜å¯ä»¥åœ¨å¤§é‡å¼€æ”¾åŸŸæ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥åœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆ (NLG) ä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚åœ¨ [Radford ç­‰ (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) çš„å·¥ä½œä¸­ï¼Œä½œè€…è¡¨æ˜é¢„è®­ç»ƒçš„ GPT2 æ¨¡å‹æ— éœ€å¤ªå¤šå¾®è°ƒå³å¯åœ¨å¤šç§ NLG ä»»åŠ¡ä¸Šå–å¾—è¾¾åˆ° SOTA æˆ–æ¥è¿‘ SOTA çš„ç»“æœã€‚ä½ å¯ä»¥åœ¨ [æ­¤å¤„](https://huggingface.co/transformers/model_summary.html#autoregressive-models) è·å–æ‰€æœ‰ ğŸ¤— transformers æ”¯æŒçš„ _è‡ªå›å½’_ æ¨¡å‹çš„ä¿¡æ¯ã€‚

å¥½äº†ï¼è‡³æ­¤ï¼Œä½ åº”è¯¥å·²ç»å¾ˆå¥½åœ°ç†è§£äº† _åŸºäº transforemr_ çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ä»¥åŠå¦‚ä½•åœ¨ ğŸ¤— transformers åº“ä¸­ä½¿ç”¨å®ƒä»¬ã€‚

éå¸¸æ„Ÿè°¢ Victor Sanhã€Sasha Rushã€Sam Shleiferã€Oliver Ã…strandã€Ted Moskovitz å’Œ Kristian Kyvik æä¾›çš„å®è´µåé¦ˆã€‚

**é™„å½•**
------

å¦‚ä¸Šæ‰€è¿°ï¼Œä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº†å¦‚ä½•ä¸º _åŸºäº transformer_ çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ç¼–å†™ä¸€ä¸ªç®€å•çš„ç”Ÿæˆæ–¹æ³•ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ `torch.argmax` å®ç°äº†ä¸€ä¸ªç®€å•çš„ _è´ªå¿ƒ_ è§£ç æ³•æ¥å¯¹ç›®æ ‡å‘é‡è¿›è¡Œé‡‡æ ·ã€‚

    from transformers import MarianMTModel, MarianTokenizer
    import torch
    
    tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-de")
    model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-de")
    
    # create ids of encoded input vectors
    input_ids = tokenizer("I want to buy a car", return_tensors="pt").input_ids
    
    # create BOS token
    decoder_input_ids = tokenizer("<pad>", add_special_tokens=False, return_tensors="pt").input_ids
    
    assert decoder_input_ids[0, 0].item() == model.config.decoder_start_token_id, "`decoder_input_ids` should correspond to `model.config.decoder_start_token_id`"
    
    # STEP 1
    
    # pass input_ids to encoder and to decoder and pass BOS token to decoder to retrieve first logit
    outputs = model(input_ids, decoder_input_ids=decoder_input_ids, return_dict=True)
    
    # get encoded sequence
    encoded_sequence = (outputs.encoder_last_hidden_state,)
    # get logits
    lm_logits = outputs.logits
    
    # sample last token with highest prob
    next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)
    
    # concat
    decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)
    
    # STEP 2
    
    # reuse encoded_inputs and pass BOS + "Ich" to decoder to second logit
    lm_logits = model(None, encoder_outputs=encoded_sequence, decoder_input_ids=decoder_input_ids, return_dict=True).logits
    
    # sample last token with highest prob again
    next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)
    
    # concat again
    decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)
    
    # STEP 3
    lm_logits = model(None, encoder_outputs=encoded_sequence, decoder_input_ids=decoder_input_ids, return_dict=True).logits
    next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)
    decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)
    
    # let's see what we have generated so far!
    print(f"Generated so far: {tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)}")
    
    # This can be written in a loop as well.
    

_è¾“å‡º:_

        Generated so far: Ich will ein
    

åœ¨è¿™ä¸ªç¤ºä¾‹ä»£ç ä¸­ï¼Œæˆ‘ä»¬å‡†ç¡®åœ°å±•ç¤ºäº†æ­£æ–‡ä¸­æè¿°çš„å†…å®¹ã€‚æˆ‘ä»¬åœ¨è¾“å…¥ â€œI want to buy a carâ€ å‰é¢åŠ ä¸Š \\(\\text{BOS}\\) ï¼Œç„¶åä¸€èµ·ä¼ ç»™ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œå¹¶å¯¹ç¬¬ä¸€ä¸ª logit $\\mathbf{l}\_1 $ (å¯¹åº”ä»£ç ä¸­ç¬¬ä¸€æ¬¡å‡ºç° lm\_logits çš„éƒ¨åˆ†) è¿›è¡Œé‡‡æ ·ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬çš„é‡‡æ ·ç­–ç•¥å¾ˆç®€å•: è´ªå¿ƒåœ°é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯ä½œä¸ºä¸‹ä¸€ä¸ªè§£ç å™¨è¾“å…¥å‘é‡ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»¥è‡ªå›å½’æ–¹å¼å°†é‡‡æ ·å¾—çš„è§£ç å™¨è¾“å…¥å‘é‡ä¸å…ˆå‰çš„è¾“å…¥ä¸€èµ·ä¼ é€’ç»™ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹å¹¶å†æ¬¡é‡‡æ ·ã€‚é‡å¤ 3 æ¬¡åï¼Œè¯¥æ¨¡å‹ç”Ÿæˆäº† â€œIch will einâ€ã€‚ç»“æœæ²¡é—®é¢˜ï¼Œå¼€äº†ä¸ªå¥½å¤´ã€‚

åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨æ›´å¤æ‚çš„è§£ç æ–¹æ³•æ¥é‡‡æ · `lm_logits`ã€‚ä½ å¯ä»¥å‚è€ƒ [è¿™ç¯‡åšæ–‡](https://huggingface.co/blog/zh/how-to-generate) äº†è§£æ›´å¤šçš„è§£ç æ–¹æ³•ã€‚

è‡³æ­¤ï¼Œã€ŠåŸºäº Transformers çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ã€‹çš„å››ä¸ªéƒ¨åˆ†å°±å…¨éƒ¨åˆ†äº«å®Œå•¦ï¼Œæ¬¢è¿å¤§å®¶é˜…è¯»å…¶ä»–åˆ†äº« ğŸ¤—ï¼

* * *

> è‹±æ–‡åŸæ–‡: [https://hf.co/blog/encoder-decoder](https://hf.co/blog/encoder-decoder)
> 
> åŸæ–‡ä½œè€…: Patrick von Platen
> 
> è¯‘è€…: Matrix Yao (å§šä¼Ÿå³°)ï¼Œè‹±ç‰¹å°”æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œå·¥ä½œæ–¹å‘ä¸º transformer-family æ¨¡å‹åœ¨å„æ¨¡æ€æ•°æ®ä¸Šçš„åº”ç”¨åŠå¤§è§„æ¨¡æ¨¡å‹çš„è®­ç»ƒæ¨ç†ã€‚
> 
> å®¡æ ¡/æ’ç‰ˆ: zhongdongy (é˜¿ä¸œ)