---
layout: post
title: "人工智能AI图像风格迁移(StyleTransfer),基于双层ControlNet(Python3.10)"
date: "2023-04-21T01:05:18.636Z"
---
人工智能AI图像风格迁移(StyleTransfer),基于双层ControlNet(Python3.10)
======================================================

图像风格迁移（Style Transfer）是一种计算机视觉技术，旨在将一幅图像的风格应用到另一幅图像上，从而生成一幅新图像，该新图像结合了两幅原始图像的特点，目的是达到一种风格化叠加的效果，本次我们使用Stable-Diffusion结合ControlNet来实现图像风格迁移效果。

安装ControlNet插件
--------------

首先确保本地已经安装并且配置好了Stable-Diffusion-Webui服务，关于Stable-Diffusion-Webui，请参见：[人工智能,丹青圣手,全平台(原生/Docker)构建Stable-Diffusion-Webui的AI绘画库教程(Python3.10/Pytorch1.13.0)](https://v3u.cn/a_id_283)，这里不再赘述。

随后进入项目目录，启动Stable-Diffusion-Webui服务：

    python3 launch.py
    

如果是没有N卡的电脑，就使用cpu模式启动：

    python3 launch.py --skip-torch-cuda-test --upcast-sampling --use-cpu interrogate
    

接着访问 [http://localhost:7860](http://localhost:7860)

选择插件(Extensions)选项卡

点击从url安装，输入插件地址：github.com/Mikubill/sd-webui-controlnet.git

安装成功后，重启WebUI界面。

![](https://v3u.cn/v3u/Public/js/editor/attached/20230420140458_43880.png)

由于ControlNet默认是一层网络，风格化操作我们需要两层，所以在设置选单(Settings)中，将多层网络设置为2。

![](https://v3u.cn/v3u/Public/js/editor/attached/20230420140453_82480.png)

设置好之后，下载模型文件：huggingface.co/webui/ControlNet-modules-safetensors/tree/main

将模型放入 stable-diffusion-webui/extensions/sd-webui-controlnet/models目录

这里还需要单独下载一个风格迁移模型，地址是：huggingface.co/TencentARC/T2I-Adapter/blob/main/models/t2iadapter\_style\_sd14v1.pth

同样放入stable-diffusion-webui/extensions/sd-webui-controlnet/models目录

至此，Stable-Diffusion-Webui服务的ControlNet插件就配置好了。

风格迁移
----

现在，我们打开ControlNet的第一个图层，将原始图像的轮廓渲染出来，因为需要保证原始图像的基本形状。

![](https://v3u.cn/v3u/Public/js/editor/attached/20230420140414_45249.png)

这里预处理器选择head，模型使用ControlNet的head模型即可。

可以看到基本轮廓已经得到了保留，风格化只负责颜色和线条。

随后配置第二个ControlNet图层，预处理器选择t2ia\_style-clipvison，模型选择刚刚下载的t2iadapter\_style\_sd14v1.pth，默认图像权重为1，先不要动。

接着上传一张目标风格的图片，这里我们选择文森特梵高的表现主义作品《星空》：

![](https://v3u.cn/v3u/Public/js/editor/attached/20230420150401_65947.jpg)

随后点击Generate按钮做图生图（img2img）操作即可。

过拟合问题（Overfitting）
------------------

经过一段时间的本地推理，生成结果如下：

![](https://v3u.cn/v3u/Public/js/editor/attached/20230420150431_20902.png)

效果并不尽如人意，这也是大多数深度学习入门者会遇到的问题，也就是过拟合问题。

过拟合（Overfitting）是指在训练模型时，模型过度地学习了训练数据的特征和噪声，从而导致模型在新数据上表现不佳的问题。

通俗地讲，过拟合就像是一名学生背诵考试答案，但是他只是死记硬背了考试题目的答案，没有真正理解题目的本质和解题思路。当他遇到新的考试题目时，由于没有理解题目的本质和解题思路，他就无法正确回答。

在机器学习中，过拟合的原因是模型复杂度过高，导致模型对训练数据中的噪声和特征都过度追求，并且忽略了数据背后的本质规律和特征。因此，当模型面对新的数据时，由于没有真正理解数据的本质规律和特征，它就无法正确地对新数据进行预测。

说白了，就是对于原始图的特征过分追求，从而淡化了目标图的风格，还记得ControlNet默认权重是1吗？这里我们只需要将权重往下调整，比如调成0.8，再次尝试生成：

![](https://v3u.cn/v3u/Public/js/editor/attached/20230420150451_80241.png)

效果不错，既保留了原始图的大部分细节，又增加了梵高的表现主义风格。

当然了，权重也不能一味地往下调整，否则也会出现欠拟合（Underfitting）问题，整个风格化迁移的过程也可以理解为是一种“调参”的过程。

结语
--

通过Stable-Diffusion结合ControlNet插件，我们可以得到一幅新的图像，该图像结合了两幅原始图像的特点，既具有内容图像的内容，又具有风格图像的风格。图像风格迁移也可以应用于其他的领域，比如电影、游戏、虚拟现实和动画创作等等。