---
layout: post
title: "论文解读（TAT）《 Transferable Adversarial Training: A General Approach to Adapting Deep Classifiers》"
date: "2023-08-13T00:57:53.616Z"
---
论文解读（TAT）《 Transferable Adversarial Training: A General Approach to Adapting Deep Classifiers》
==============================================================================================

Note：\[ wechat：Y466551 | 可加勿骚扰，付费咨询 \]

论文信息
====

> 论文标题：Transferable Adversarial Training: A General Approach to Adapting Deep Classifiers  
> 论文作者：Hong Liu, Mingsheng Long, Jianmin Wang, Michael I. Jordan  
> 论文来源：ICML 2019  
> 论文地址：[download](https://www.semanticscholar.org/paper/Transferable-Adversarial-Training%3A-A-General-to-Liu-Long/6df47f4eb667df691c71dbab60bce6585baf247c)   
> 论文代码：download 

1 Introduction
==============

　　出发点：当使用对抗性训练的时候，因为抑制领域特定的变化时，会扭曲原始的特征分布；

　　事实：

　　![](https://img2023.cnblogs.com/blog/1664108/202308/1664108-20230812102347904-594746164.png)

　　Figure2(b)：

*   *   使用源域和目标域的标记数据做测试，对比了使用对抗性训练（DANN、MCD）和监督训练（EestNet50）的测试误差；
    *   结论：使用对抗性训练，减少特定领域的变化不可避免地打破了原始表示的判别结构；

　　Figure2(c)：

*   *   计算特征表示层模型权重的奇异值分布；
    *   结论：使用对抗性训练的奇异值分布更加重尾，表示产生了扭曲的特征表示；

2 方法
====

**2.1 模型框架**
------------

　　![](https://img2023.cnblogs.com/blog/1664108/202308/1664108-20230812151917190-1781945533.png)

2.2 Adversarial Generation of Transferable Examples
---------------------------------------------------

　　现有的对抗性域适应方法，用 $f = F (x)$ 表示特征提取器，用 $d = D (f)$ 表示域鉴别器，用 $c =C (x)$ 表示分类器。

　　$D$ 和 $F$ 形成一个双人极大极小博弈：

*   *   $D$ 训练区分源和目标域；
    *   $F$ 训练混淆 $D$ ，并训练 $C$；

　　然而，这可能会恶化适应性。为保证适应性，本文提出生成可转移的例子来弥合域差距。

　　具体地说，仍训练域鉴别器 $D$ 通过以下损失函数来区分源域和目标域：

　　　　$\\begin{aligned}\\ell\_{d}\\left(\\theta\_{D}, \\mathbf{f}\\right)= & -\\frac{1}{n\_{s}} \\sum\_{i=1}^{n\_{s}} \\log \\left\[D\\left(\\mathbf{f}\_{s}^{(i)}\\right)\\right\] \\\\& -\\frac{1}{n\_{t}} \\sum\_{i=1}^{n\_{t}} \\log \\left\[1-D\\left(\\mathbf{f}\_{t}^{(i)}\\right)\\right\] .\\end{aligned}   \\quad\\quad(1)$

　　分类器 $C$ 通过源域样本监督训练：

　　　　$\\ell\_{c}\\left(\\theta\_{C}, \\mathbf{f}\\right)=\\frac{1}{n\_{s}} \\sum\_{i=1}^{n\_{s}} \\ell\_{c e}\\left(C\\left(\\mathbf{f}\_{s}^{(i)}\\right), \\mathbf{y}\_{s}^{(i)}\\right) \\quad\\quad(2)$

　　本文通过一种新的对抗性训练范式生成的可转移样本来填补源域和目标域之间的差距，从而减少分布变化。

　　生成的可转移样本需要满足两个条件：

*   *   首先，可转移的样本应该有效地混淆域鉴别器 $D$，从而填补域间隙，桥接源域和目标域；
    *   其次，可转移的样本应该能够欺骗类别分类器 $C$，这样它们就可以推动决策边界远离数据点；

　　因此，可转移的样本是通过 $\\ell\_{c}$ 和 $\\ell\_{d}$ 的联合损失而反向生成的：

　　　　$\\begin{aligned}\\mathbf{f}\_{t^{k+1}} \\leftarrow \\mathbf{f}\_{t^{k}} & +\\beta \\nabla\_{\\mathbf{f}\_{t^{k}}} \\ell\_{d}\\left(\\theta\_{D}, \\mathbf{f}\_{t^{k}}\\right) \\\\& -\\gamma \\nabla\_{\\mathbf{f}\_{t^{k}}} \\ell\_{2}\\left(\\mathbf{f}\_{t^{k}}, \\mathbf{f}\_{t^{0}}\\right) \\\\\\end{aligned} \\quad\\quad(3)$

　　　　$\\begin{aligned}\\mathbf{f}\_{s^{k+1}} \\leftarrow \\mathbf{f}\_{s^{k}} & +\\beta \\nabla\_{\\mathbf{f}\_{s}} \\ell\_{d}\\left(\\theta\_{D}, \\mathbf{f}\_{s^{k}}\\right) \\\\& -\\gamma \\nabla\_{\\mathbf{f}\_{s}} \\ell\_{2}\\left(\\mathbf{f}\_{s^{k}}, \\mathbf{f}\_{s^{0}}\\right) \\\\& +\\beta \\nabla\_{\\mathbf{f}\_{s k}} \\ell\_{c}\\left(\\theta\_{C}, \\mathbf{f}\_{s^{k}}\\right)\\end{aligned} \\quad\\quad(4)$

　　其中，$\\mathbf{f}\_{t^{0}}=\\mathbf{f}\_{t}, \\mathbf{f}\_{s^{0}}=\\mathbf{f}\_{s}, \\mathbf{f}\_{t \*}=\\mathbf{f}\_{t^{K}}, \\mathbf{f}\_{s \*}=\\mathbf{f}\_{s^{K}}$。

　　此外，为避免生成的样本的发散，控制生成的样本与原始样本之间的 $\\ell\_{2}$-距离。

2.3 Adversarial Training with Transferable Examples
---------------------------------------------------

　　用可转移的样本训练类分类器可以解释为提高了分类器的先验分布对对抗性扰动和域变化的鲁棒性。

　　因此，对类别分类器 $C$ 的对抗性训练的损失函数表述如下：  
　　　　$\\begin{aligned}\\ell\_{c, a d v}\\left(\\theta\_{C}, \\mathbf{f}\_{\*}\\right) & =\\frac{1}{n\_{s}} \\sum\_{i=1}^{n\_{s}} \\ell\_{c e}\\left(C\\left(\\mathbf{f}\_{s \*}^{(i)}\\right), \\mathbf{y}\_{s \*}^{(i)}\\right) \\\\& +\\frac{1}{n\_{t}} \\sum\_{i=1}^{n\_{t}}\\left|C\\left(\\left(\\mathbf{f}\_{t \*}^{(i)}\\right)\\right)-C\\left(\\left(\\mathbf{f}\_{t}^{(i)}\\right)\\right)\\right|\\end{aligned} \\quad\\quad(5)$

　　与训练类别分类器类似，也用生成的可转移的例子来训练域鉴别器。这对于稳定对抗性训练过程很重要，否则生成的可转移的例子就会出现分歧。另一个关键的观点是利用这些可转移的例子来弥合领域上的差异。简单地在原始数据上欺骗域鉴别器并不能保证生成的示例可以从一个域转移到另一个域。因此，建议反向训练域鉴别器，以进一步区分可转移的例子从源和目标，使用以下损失：

　　　　$\\begin{aligned}\\ell\_{d, a d v}\\left(\\theta\_{D}, \\mathbf{f}\_{\*}\\right)= & -\\frac{1}{n\_{s}} \\sum\_{i=1}^{n\_{s}} \\log \\left\[D\\left(\\mathbf{f}\_{s \*}^{(i)}\\right)\\right\] \\\\& -\\frac{1}{n\_{t}} \\sum\_{i=1}^{n\_{t}} \\log \\left\[1-D\\left(\\mathbf{f}\_{t \*}^{(i)}\\right)\\right\]\\end{aligned} \\quad\\quad(6)$

　　我们共同最小化误差(1)和误差(6)来训练 $D$，最小化误差(2)和误差(5) 来训练 $C$，训练目标：

　　　　$\\begin{array}{l}\\underset{\\theta\_{D}, \\theta\_{C}}{\\text{min}}\\;\\;\\ell\_{d}\\left(\\theta\_{D}, \\mathbf{f}\\right)+\\ell\_{c}\\left(\\theta\_{C}, \\mathbf{f}\\right) +\\ell\_{d, a d v}\\left(\\theta\_{D}, \\mathbf{f}\_{\*}\\right)+\\ell\_{c, a d v}\\left(\\theta\_{C}, \\mathbf{f}\_{\*}\\right) \\end{array} \\quad\\quad(7)$

3 实验
====

　　![](https://img2023.cnblogs.com/blog/1664108/202308/1664108-20230812155052459-621827944.png)

　　![](https://img2023.cnblogs.com/blog/1664108/202308/1664108-20230812155308132-1491973287.png)

因上求缘，果上努力~~~~ 作者：[Wechat~Y466551](https://www.cnblogs.com/BlairGrowing/)，转载请注明原文链接：[https://www.cnblogs.com/BlairGrowing/p/17624387.html](https://www.cnblogs.com/BlairGrowing/p/17624387.html)