---
layout: post
title: "Cilium系列-10-启用 IPv6 BIG TCP和启用巨帧"
date: "2023-08-03T01:06:29.118Z"
---
Cilium系列-10-启用 IPv6 BIG TCP和启用巨帧
================================

系列文章
----

*   [Cilium 系列文章](https://ewhisper.cn/tags/Cilium/)

前言
--

将 Kubernetes 的 CNI 从其他组件切换为 Cilium, 已经可以有效地提升网络的性能. 但是通过对 Cilium 不同模式的切换/功能的启用, 可以进一步提升 Cilium 的网络性能. 具体调优项包括不限于:

*   启用本地路由(Native Routing)
*   完全替换 KubeProxy
*   IP 地址伪装(Masquerading)切换为基于 eBPF 的模式
*   Kubernetes NodePort 实现在 DSR(Direct Server Return) 模式下运行
*   绕过 iptables 连接跟踪(Bypass iptables Connection Tracking)
*   主机路由(Host Routing)切换为基于 BPF 的模式 (需要 Linux Kernel >= 5.10)
*   启用 IPv6 BIG TCP (需要 Linux Kernel >= 5.19)
*   禁用 Hubble(但是不建议, 可观察性比一点点的性能提升更重要)
*   修改 MTU 为巨型帧(jumbo frames) (需要网络条件允许)
*   启用带宽管理器(Bandwidth Manager) (需要 Kernel >= 5.1)
*   启用 Pod 的 BBR 拥塞控制 (需要 Kernel >= 5.18)
*   启用 XDP 加速 (需要 支持本地 XDP 驱动程序)
*   (高级用户可选)调整 eBPF Map Size
*   Linux Kernel 优化和升级
    *   `CONFIG_PREEMPT_NONE=y`
*   其他:
    *   tuned network-\* profiles, 如: `tuned-adm profile network-latency` 或 `network-throughput`
    *   CPU 调为性能模式
    *   停止 `irqbalance`，将网卡中断引脚指向特定 CPU

在网络/网卡设备/OS等条件满足的情况下, 我们尽可能多地启用这些调优选项, 相关优化项会在后续文章逐一更新. 敬请期待.

今天我们来调优 Cilium, 启用 IPv6 BIG TCP 允许网络协议栈准备更大的 GSO（发送）和 GRO（接收）数据包，以减少协议栈的遍历次数，从而提高性能和延迟.

### 测试环境

*   Cilium 1.13.4
*   K3s v1.26.6+k3s1
*   OS
    *   3 台 Ubuntu 23.04 VM, Kernel 6.2, x86

IPv6 BIG TCP
------------

IPv6 BIG TCP 允许网络协议栈准备更大的 GSO（发送）和 GRO（接收）数据包，以减少协议栈的遍历次数，从而提高性能和延迟。它可减少 CPU 负载，有助于实现更高的速度（即 100Gbit/s 及以上）。为了让这些数据包通过协议栈，BIG TCP 在 IPv6 头之后添加了一个临时的 "逐跳"（Hop-By-Hop）头，并在通过线路传输数据包之前将其剥离。BIG TCP 可在双协议栈设置中运行，IPv4 数据包将使用旧的下限（64k），IPv6 数据包将使用新的较大下限（192k）。请注意，Cilium 假定 GSO 和 GRO 的默认内核值为 64k，只有在必要时才会进行调整，也就是说，如果启用了 BIG TCP，而当前的 GSO/GRO 最大值小于 192k，那么 Cilium 会尝试增加这些值；如果禁用了 BIG TCP，而当前的最大值大于 64k，那么 Cilium 会尝试减少这些值。BIG TCP 不需要更改网络接口 MTU。

### 需求

*   ✔️ Kernel >= 5.19
*   ✔️ eBPF Host-Routing(主机路由)
*   ✔️ 基于 eBPF 的 kube-proxy 替换
*   ✔️ 基于 eBPF masquerading(伪装)
*   ✔️ 禁用隧道(Tunnel)和加密
*   ❌ 支持的 NICs: mlx4, mlx5

由于我这里没有 mlx4, mlx5 型号的网卡, 所以本次无法实战测试了.

要启用 IPv6 BIG TCP:

    helm install cilium cilium/cilium --version 1.13.4 \
      --namespace kube-system \
      --set tunnel=disabled \
      --set bpf.masquerade=true \
      --set ipv6.enabled=true \
      --set enableIPv6Masquerade=false \
      --set enableIPv6BIGTCP=true \
      --set kubeProxyReplacement=strict
    

请注意，切换 IPv6 BIG TCP 选项后，必须重新启动 Kubernetes Pod 才能使更改生效。

要验证您的安装是否使用 IPv6 BIG TCP 运行，请在任何一个 Cilium pod 中运行 `cilium status`，并查找报告 "IPv6 BIG TCP "状态的行，其状态应为 "enabled"。

修改 MTU 为巨型帧
-----------

这里也顺便提一下"修改 MTU 为巨型帧".

最大传输单位（MTU）会对配置的网络吞吐量产生重大影响。Cilium 将自动检测底层网络设备的 MTU。因此，如果系统配置为使用巨型帧，Cilium 将自动使用巨型帧。

要从中受益，请确保您的系统配置为使用巨型帧（如果您的网络允许）。

因为我的网络设备无法修改为巨型帧, 所以本次也无法实战测试.

总结
--

本文调优 Cilium, 启用 IPv6 BIG TCP 允许网络协议栈准备更大的 GSO（发送）和 GRO（接收）数据包，以减少协议栈的遍历次数，从而提高性能和延迟.

但是前提条件是 Kernel >= 5.19, 且需要特定网卡支持. 所以本次无法实际验证.

另一项调优为: 修改 MTU 为巨型帧, 以提升网络吞吐量. 但是前提条件是网络允许.

至此，性能调优已完成实战验证：

*   ✔️ 启用本地路由 (Native Routing)
*   ✔️ 完全替换 KubeProxy
*   ✔️ IP 地址伪装 (Masquerading) 切换为基于 eBPF 的模式
*   ✔️ Kubernetes NodePort 实现在 DSR(Direct Server Return) 模式下运行
*   ✔️ 绕过 iptables 连接跟踪 (Bypass iptables Connection Tracking)
*   ✔️ 主机路由 (Host Routing) 切换为基于 BPF 的模式 (需要 Linux Kernel >= 5.10)
*   ❌ 启用 IPv6 BIG TCP (需要 Linux Kernel >= 5.19, 支持的 NICs: mlx4, mlx5)
    *   由于没有支持的网卡, 无法完成验证
*   ❌ 修改 MTU 为巨型帧 (jumbo frames) （需要网络条件允许）
*   启用带宽管理器 (Bandwidth Manager) (需要 Kernel >= 5.1)
*   启用 Pod 的 BBR 拥塞控制 (需要 Kernel >= 5.18)
*   启用 XDP 加速 （需要 支持本地 XDP 驱动程序）

📚️参考文档
-------

*   [IPv6 BIG TCP - Tuning Guide — Cilium 1.13.4 documentation](https://docs.cilium.io/en/stable/operations/performance/tuning/#ipv6-big-tcp)

> _三人行, 必有我师; 知识共享, 天下为公._ 本文由东风微鸣技术博客 [EWhisper.cn](https://EWhisper.cn) 编写.