---
layout: post
title: "扩展的多曝光图像合成算法及其在单幅图像增强中的应用。"
date: "2023-09-18T00:57:28.904Z"
---
扩展的多曝光图像合成算法及其在单幅图像增强中的应用。
==========================

针对Exposure fusion算法存在的Out-of-range Artifact和low frequency halo两个瑕疵，分析了Extended Exposure Fusion的改进过程，并进一步借助有关方法实现了单幅图像的Simulated Exposure Fusion过程。

　　在[拉普拉斯金字塔在多图HDR算法中的应用以及多曝光图像的融合算法简介](https://www.cnblogs.com/Imageshop/p/17691517.html)一文中提高的Exposure Fusion算法，是一种非常优秀的多曝光图片合成算法，对于大部分测试图都能获取到较为满意的结果，但是也存在着两个局限性：

　　1、存在着Out-of-range Artifact；

        2、存在着low frequency halo；

　　为了解决这两个问题，Charles Hessel在2019年发表了一篇名为《Extended Exposure Fusion》的论文，基本上有效的避免了《Exposure Fusion》的这两个缺陷，并且以此为基础，将Exposure Fusion扩展到了单幅图像的增强中。

　　在IPOL网站中，有对这两篇文章的详细资料和在线测试程序，详见：

　　[http://www.ipol.im/pub/art/2019/278/](http://www.ipol.im/pub/art/2019/278/　)　　　　　　Extended Exposure Fusion

　　[https://www.ipol.im/pub/art/2019/279/](https://www.ipol.im/pub/art/2019/279/)　　　　　Simulated Exposure Fusion

　　我们不去过多的分析他们的原理，只是大概的描述下论文的细节吧，更为详细的可以直接阅读论文本身。

　　**一、Extended Exposure Fusion**

　　这个文章虽然篇幅有十几页，但是实际上核心的东西就是一个：无中生有，即我们从原始的图像数据序列中fu在继续创造更多的图像，然后利用Exposure Fusion合成，这些新创造出的图像相对于原始的图具有更低的动态范围，具体的过程为：

　　确定一个参数Beta，有效范围是\[0,1\]，然后根据Ceil(1.0 / Beta)向上取整得到我们需要重新创建的图像的数量M，这个Beta我们称之为动态范围。新创建的M个图像的生产方法如下：

　　 对于序列![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916132448308-1243783784.png)中的每一个值，我们计算一个参数：　　　　　　　 

    　　                                    ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916132606330-1544585613.png)

 　　作为需要压缩的动态的范围的中心，当原始的像素值t在![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916132716192-1706342514.png)范围内时，线性映射，即t不变化，当不在此范围时，按以下公式计算新的t值。

　　                             ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916132850064-2123116803.png)

　　其中   ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916132922635-1162813341.png) , ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916132933429-1659716972.png)以及 ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916133028606-1177678393.png)，这样就保证像素不在我们设定的动态范围时，像素值不会断崖式的裁剪，而是平滑的予以变化，从而使得融合的结果不会太突兀。

　　以下时Beta = 0.5/0.34/0.25时对应的重新映射的曲线图，可以看到随着Beta的值的变小，新创建的图像数量M不断地增加，但是不管如何，所有图像组合在一起，都覆盖了原图的所有的动态范围（即合并后的映射图总会有一条45倾斜的直线），而Beta值的含义页可以从曲线总可以看出就是直线段的长度，即每幅图动态范围保持不变的部分。

![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916133323041-1059420540.png)

 　　　　　　Beta = 0.5, M = 2　　　　　　　　　　　　　　Beta = 0.34, M = 3　　　　　　　　　　　　　　　　　Beta = 0.25, M = 4

 　　注意，以上的映射等行为的公式都是针对归一化后的图像数据的，即要将图像由\[0，255\]先映射为\[0,1\]。

　　如果原始图像序列由N幅图像，则这样处理就增加为了N\*M幅图，后续就是对这N\*M幅进行标准的比曝光融合了。因此可以明显的看出，这个算法的速度要比Exposure Fusion至少慢M倍。

　　以下C++代码简答的解释了上述新图像的生成过程：

![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916134531846-910235812.png)

 　　当Beta = 0.5，M = 2时（注意到上述曲线），下述图像清晰的表达了这个扩展的过程：

![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916134825576-1770699974.png)

 　   原始的图像序列只有4幅图，扩展后的为8幅图，而且我们注意到扩展后的图和源图没有一个是相同的，通过组合这新生产的8幅图，最终得到扩展的融合结果。

　　就以上述图为例，Beta设置为0.5，我们金字塔层数都设置为8，Exposure Fusion和Extended Exposure Fusion的融合效果分别如下所示：

   ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916135143892-1350961910.jpg)  ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916135155003-1237812839.png)

　　　　　　　　　　Exposure Fusion　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　Extended Exposure Fusion

　　我们注意观察图中最左侧窗户上部的小方格内，左图里该方格里基本是纯白色，看不到什么信息，而右图则把原始图input 1里的有效信息带入了，增加了信息量。

　　在比如下图，左图是标准的Exposure Fusion，右图为扩展后的，可以看到右侧图台灯里以及窗户外面的地面有着更为细腻的细节，而左图基本是纯色。

  ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916135556288-2072725119.jpg)  ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916135608657-871189383.jpg)

　　在《Extended Exposure Fusion》还提到了对各图像权重的一个改进，虽然那个东西有一定作用，但是个人认为是锦上添花的一些东西吧，实际上也没有啥大的作用，因此，本人未做具体的研究。

　　另外，Extended Exposure Fusion里还提到了一个叫Robust Normalization的东西，这个确实还是有点用的，他是对曝光融合的图像简单的做了一下增强，我们确实也发现普通的融合后图像不是很鲜艳，而使用Robust Normalization则可以比较为明显的改善他，这个过程有点类似于Photoshop的自动色阶或自动对比度，但是有别于他们的是他不是对单通道进行统计直方图，而是低裁剪值所依赖的直方图由每个像素的最小分量(Min(R/G/B))决定，高裁剪值所依赖的直方图由每个像素的最大分量决定。

　　其他的这篇文章也没有啥好东西了。 

　　**二、Simulated Exposure Fusion**

　　那么这篇文章的作者和Extended Exposure Fusion其实是一个人，所以他们文章的思路其实是想通的。只不过这篇文章是这对单幅图像进行的处理。

　　那么很明显，如果要想借用多曝光融合算法来增强单幅图像，一个很自然的想法就是在原图的基础上使用不同曝光值进行映射（增强或降低对比度），然后融合就可以了，但是这里就涉及到了几个问题，第一，如果确定需要的曝光的图像的数量，二是如何确定每幅图像的曝光值。

　　我们先来看看第二个问题，假定我们已经确定了需要曝光的图像的数量为M，那么首先这样：

　　我们计算出整幅图像的中间值Median，这个计算很明显可以用直方图搞定，至于彩色图像，可以直接把R/G/B所有通道的直方图全部累加后即可。一般来说，这个中间值很好的反应了整幅的明暗程度，因此，用他作为一个参考数据。利用下式计算出需要降低对比度（under-exposed image）:

　　　　　　　　　　Ns = int((M - 1) \* Median / 255.

　　则需要增加对比度的图像数量为 N =  M - 1 - Ns。

　　比如Median的结果为50，则说明图像整体比较暗，如果M=10，则Ns = 1, N = 8;

　　给定一个用户输入的最大对比度参数Alpha，该参数的意义上容许图像最多的增强或降低的对比度是Alpha（可过曝或欠曝），可如果在这M个序列的图像里，要能达到整个Alpah值，必须有某个参数和N或Ns有关，明显要取他们的最大值 Nx = Max(N, Ns)。论文里作者折腾了半天提出了下面这个计算式：　　　　　　

                                         ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916151407694-199674342.png)

　　这个公式其实是个线性的公式，即图像整体变量或整体变暗，如果直接把这样生成的M幅图像合成，不用想，没有什么意义的。  
　　那么我们考虑到Extended Exposure Fusion文章里的动态范围压缩，如果把他们两个结合起来，那么是不是能获得更好的结果呢。

　　因为在前面调节曝光度的时候，我们的调节公式必然会造成部分像素过曝或欠曝，如果简单的把他们裁剪掉，就造成了信息的丢失，所以我们暂时不裁剪，而是直接使用扩展的曝光融合里的动态范围压缩技术，把他们压缩，压缩完成后再次进行裁剪，这样就可以很好的控制结果了。

　　这样这个过程就涉及到了前面所说的参数Beta了，那么由Alpha和Beta实际上又可以共同确定前面所说的第一个参数M，因为我们只要取某一个M使得所有的动态范围都至少又一次得到了增强。这个论文里提出了一个do while不断的去测试合理的M值：

 %%% Compute optimal number of images (smallest N that ensure every part of
    %%% the input range is enhanced)
    Mp \= 1;              % Mp = M-1; M is the total number of images
    Ns \= floor(Mp\*cval); % number of images generated with fs
    N  \= Mp-Ns;          % number of images generated with f
    Nx \= max(N,Ns);      % used to compute maximal factor
    tmax1  \= (+1    + (Ns+1)\*(beta-1)/Mp)/(alpha^(1/Nx));     % t\_max k=+1
    tmin1s \= (-beta + (Ns-1)\*(beta-1)/Mp)/(alpha^(1/Nx)) + 1; % t\_min k=-1
    tmax0  \= 1      + Ns\*(beta-1)/Mp;                         % t\_max k=0
    tmin0  \= 1\-beta + Ns\*(beta-1)/Mp;                         % t\_min k=0
    while tmax1 < tmin0 || tmax0 < tmin1s
        %%% add an image to the sequence
        Mp \= Mp+1;
        %%% update values
        Ns \= floor(Mp\*cval);
        N  \= Mp-Ns;
        Nx \= max(N,Ns);
        tmax1  \= (+1    + (Ns+1)\*(beta-1)/Mp)/(alpha^(1/Nx));     % t\_max k=+1
        tmin1s \= (-beta + (Ns-1)\*(beta-1)/Mp)/(alpha^(1/Nx)) + 1; % t\_min k=-1
        tmax0  \= 1      + Ns\*(beta-1)/Mp;                         % t\_max k=0
        tmin0  \= 1\-beta + Ns\*(beta-1)/Mp;                         % t\_min k=0
        if Mp > 49 % break if no solution
            warning(\['The estimation of the number of image required in '...
                     'the sequence stopped because it reached M>50. ' ...
                     'Check the parameters.'\]);
            break
        end
    end

　　根据不同的Alpha和Beta，已经不同的图像特性（决定了中值），可以得到不同的映射曲线，如下所示：

![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916153301538-438369904.png)

 　　注意上面的m就是指归一化的中值。

　　有了这些曲线，在原有图像的基础上进行映射得到一个序列的图像，然后再用Exposure Fusion就可以了。

　　论文里还提到了用HSV颜色空间的V分量来做这些工作，而不是用RGB颜色空间，这个时候由于是V是个灰度的信息，因此，在权重的计算时，饱和度那个指标就么有意义了，这个转换我觉得除了能加快下程度的速度，其他的可能并没有什么特殊的需要，而且HSV颜色空间和RGB之间的转换还是很耗时的，如果写的不好，可能还会降低速度，还不如直接在RGB颜色空间中做。

　　我们测试一些图像，这个算法获得的结果却是还是相当不错和稳定的：　

  ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916154419941-1175599899.jpg)  ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916154427801-1116598264.jpg)

  ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916154503134-537927966.jpg)  ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916154514821-609505417.jpg)

   ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916154523538-1217428963.jpg)   ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916154531213-660199788.jpg)

   ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916154618094-1809686371.jpg)   ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916154634341-692893205.jpg)

　　和其他的一些增强算法相比，这个方法的比较明显的特征是不会过分的引入瑕疵（比如突出的斑点或者块状），但是耗时方面也还是比较可观（和图像的内容和参数有关）。

　　在原始的论文中，作者还进行了大量的比较，有兴趣的可以去看看。

       提供一个测试DEMO供有兴趣的朋友玩玩：[https://files.cnblogs.com/files/Imageshop/Exposure\_Fusion.rar?t=1694501148&download=true](https://files.cnblogs.com/files/Imageshop/Exposure_Fusion.rar?t=1694501148&download=true)

       如果我们选择的是一个图像，则调用的即为Simulated Exposure Fusion算法。

![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916155212028-1908365835.png)

       补充： 在上一篇Exposure fusion里，也简单的提到了不做权重融合，只对拉普拉斯金字塔取最大值、平均值等操作的多图融合方式，虽然这种融合对于多曝光融合的效果很一般，但是在对于多焦段融合时确有着较为明显的优势，这个时候权重融合反而效果很差，如下图所示 ：

   ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916163019440-1685059177.jpg)   ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916163027647-1839787696.jpg)

　　　　　　　　　　　　　　　　原图1　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　原图2 　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　

   ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916163215426-1547074647.jpg)   ![](https://img2023.cnblogs.com/blog/349293/202309/349293-20230916163233894-370493447.jpg)

　　　　　　　　　　基于特征权重的融合　　　　　　　　　　　　　　　　　　　　　　　　　　　　基于最大值/平均值的融合

　　其实想一想，道理也很简单，多焦距的图像，在非焦点区域，图像是模糊的，焦点区域图像是清晰的，因为模糊区域的拉普拉斯金字塔必然数较小，而焦点区域因为有很多图像细节，拉普拉斯数据丰富，因此，如果是用特征权重融合，模糊区域对结果就一定有影响，而如果用最大值，则只取那些清晰的区域，所以合成后的结果就更为清晰。

　　所以说，不同的算法还是有不同的应用场景，必须找到合适的对象予以体现他们的价值。

　　如果想时刻关注本人的最新文章，也可关注公众号或者添加本人微信：  laviewpbt

                             ![](https://img2020.cnblogs.com/blog/349293/202104/349293-20210407161055729-1331889910.jpg)

翻译

搜索

复制