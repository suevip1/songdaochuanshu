---
layout: post
title: "Cilium系列-9-主机路由切换为基于 BPF 的模式"
date: "2023-08-01T01:12:57.052Z"
---
Cilium系列-9-主机路由切换为基于 BPF 的模式
============================

系列文章
----

*   [Cilium 系列文章](https://ewhisper.cn/tags/Cilium/)

前言
--

将 Kubernetes 的 CNI 从其他组件切换为 Cilium, 已经可以有效地提升网络的性能. 但是通过对 Cilium 不同模式的切换/功能的启用, 可以进一步提升 Cilium 的网络性能. 具体调优项包括不限于:

*   启用本地路由(Native Routing)
*   完全替换 KubeProxy
*   IP 地址伪装(Masquerading)切换为基于 eBPF 的模式
*   Kubernetes NodePort 实现在 DSR(Direct Server Return) 模式下运行
*   绕过 iptables 连接跟踪(Bypass iptables Connection Tracking)
*   主机路由(Host Routing)切换为基于 BPF 的模式 (需要 Linux Kernel >= 5.10)
*   启用 IPv6 BIG TCP (需要 Linux Kernel >= 5.19)
*   禁用 Hubble(但是不建议, 可观察性比一点点的性能提升更重要)
*   修改 MTU 为巨型帧(jumbo frames) (需要网络条件允许)
*   启用带宽管理器(Bandwidth Manager) (需要 Kernel >= 5.1)
*   启用 Pod 的 BBR 拥塞控制 (需要 Kernel >= 5.18)
*   启用 XDP 加速 (需要 支持本地 XDP 驱动程序)
*   (高级用户可选)调整 eBPF Map Size
*   Linux Kernel 优化和升级
    *   `CONFIG_PREEMPT_NONE=y`
*   其他:
    *   tuned network-\* profiles, 如: `tuned-adm profile network-latency` 或 `network-throughput`
    *   CPU 调为性能模式
    *   停止 `irqbalance`，将网卡中断引脚指向特定 CPU

在网络/网卡设备/OS等条件满足的情况下, 我们尽可能多地启用这些调优选项, 相关优化项会在后续文章逐一更新. 敬请期待.

今天我们来调优 Cilium, 启用 Host-Routing(主机路由) 以完全绕过 iptables 和上层主机堆栈，并实现比常规 veth 设备操作更快的网络命名空间切换。

### 测试环境

*   Cilium 1.13.4
*   K3s v1.26.6+k3s1
*   OS
    *   3 台 Ubuntu 23.04 VM, Kernel 6.2, x86
    *   3 台 Debian 10 开发板, Kernel 4.19, arm64

eBPF Host-Routing
-----------------

即使 Cilium 使用 eBPF 执行网络路由，默认情况下，网络数据包仍会穿越节点常规网络堆栈的某些部分。这就导致了所有数据包仍能通过所有 iptables 钩子。不过，这些钩子会增加大量开销。有关测试环境的确切数据，请参阅 [TCP 吞吐量 (TCP\_STREAM)](https://docs.cilium.io/en/stable/operations/performance/benchmark/#benchmark-throughput)，并比较 "Cilium "和 "Cilium（传统主机路由）"的结果。

具体如下:

Single-Stream:

![TCP Throuthput(Single-Stream)](https://img2023.cnblogs.com/other/3034537/202307/3034537-20230731163808848-1521054444.png)

![TCP Throuthput(Single-Stream) - CPU](https://img2023.cnblogs.com/other/3034537/202307/3034537-20230731163809044-1742248603.png)

Multi-Stream:

![TCP Throughput(32 Streams)](https://img2023.cnblogs.com/other/3034537/202307/3034537-20230731163809225-1479325723.png)

![TCP Throughput(32 Streams) - CPU](https://img2023.cnblogs.com/other/3034537/202307/3034537-20230731163809390-440578213.png)

在 Cilium 1.9 中引入了基于 eBPF 的主机路由，以**完全绕过 iptables 和上层主机堆栈**，并实现比常规 veth 设备操作更快的网络命名空间切换。**如果内核支持该选项，它将自动启用**。要验证您的安装是否使用了 eBPF 主机路由，请在任何 Cilium pod 中运行 `cilium status`，并查找报告 "Host Routing（主机路由）"状态的行，其中应显示 "BPF"。

如下, 在 Cilium 1.9 中引入了基于 eBPF 的主机路由后的性能提升:

![Pod to Pod Performance, Kernel 5.10, Cilium 1.9](https://img2023.cnblogs.com/other/3034537/202307/3034537-20230731163809584-1329741430.png)

从初始结果可以看出，当在 v5.10 内核上使用 Cilium 1.9(及更新版本) 的 eBPF 扩展时，直接路由下 Pod 到远程 Pod 会话的单流 TCP 吞吐量会翻倍，而不是由主机堆栈转发处理两个方向。同样，在避开主机堆栈时，测试中的 Pod 的 TCP 请求/响应事务性能提高了近 3 倍。

### 要求

*   Kernel >= 5.10
*   直接路由(Direct-routing)配置或隧道
*   基于 eBPF 的 kube-proxy 替换
*   基于 eBPF 的伪装(masquerading)

实施
--

如上所述, "如果内核支持该选项，它将自动启用".

我们查看 Kernel >= 5.10 的情况:

### Kernel >= 5.10

    $ kubectl -n kube-system exec ds/cilium -- cilium status |grep "Host Routing"
    Host Routing:            BPF
    

如上所示, 在 Kernel >= 5.10 的环境: "3 台 Ubuntu 23.04 VM, Kernel 6.2, x86" 中, 已经自动启用 Host-Routing 基于 BPF 的功能

> 📝**Notes**
> 
> 根据[上一篇文章 - 绕过 IPTables 连接跟踪](https://ewhisper.cn/posts/58823/): 在无法使用 eBPF 主机路由 (Host-Routing) 的情况下，网络数据包仍需在主机命名空间中穿越常规网络堆栈，iptables 会增加大量成本。  
> 所以, 在"3 台 Ubuntu 23.04 VM, Kernel 6.2, x86" 中, 其实是没必要设置"绕过 IPTables 连接跟踪" 的.

### Kernel < 5.10

    $ kubectl -n kube-system exec ds/cilium -- cilium status |grep "Host Routing"
    Host Routing:            Legacy
    

如上所示, 在 Kernel < 5.10 的环境: "3 台 Debian 10 开发板, Kernel 4.19, arm64" 中, Host-Routing 功能为 Legacy.

> 📝**Notes**
> 
> 根据[上一篇文章 - 绕过 IPTables 连接跟踪](https://ewhisper.cn/posts/58823/): 在无法使用 eBPF 主机路由 (Host-Routing) 的情况下，网络数据包仍需在主机命名空间中穿越常规网络堆栈，iptables 会增加大量成本。通过禁用所有 Pod 流量的连接跟踪 (connection tracking) 要求，从而绕过 iptables 连接跟踪器(iptables connection tracker)，可将这种遍历成本降至最低。  
> 所以, 在"3 台 Debian 10 开发板, Kernel 4.19, arm64" 中, 是有必要设置"绕过 IPTables 连接跟踪" 的.

总结
--

本文调优 Cilium, 启用 Host Routing(主机路由) 以完全绕过 iptables 和上层主机堆栈，并实现比常规 veth 设备操作更快的网络命名空间切换。

但是前提条件是 Kernel >= 5.10. (在没有条件启用 Host-Routing 的环境中, 可以设置"绕过 iptables 连接跟踪"以提升性能.)

至此，性能调优已完成：

*   ✔️ 启用本地路由 (Native Routing)
*   ✔️ 完全替换 KubeProxy
*   ✔️ IP 地址伪装 (Masquerading) 切换为基于 eBPF 的模式
*   ✔️ Kubernetes NodePort 实现在 DSR(Direct Server Return) 模式下运行
*   ✔️ 绕过 iptables 连接跟踪 (Bypass iptables Connection Tracking)
*   ✔️ 主机路由 (Host Routing) 切换为基于 BPF 的模式 (需要 Linux Kernel >= 5.10)
*   启用 IPv6 BIG TCP (需要 Linux Kernel >= 5.19)
*   修改 MTU 为巨型帧 (jumbo frames) （需要网络条件允许）
*   启用带宽管理器 (Bandwidth Manager) (需要 Kernel >= 5.1)
*   启用 Pod 的 BBR 拥塞控制 (需要 Kernel >= 5.18)
*   启用 XDP 加速 （需要 支持本地 XDP 驱动程序）

📚️参考文档
-------

*   [Host-Routing - Tuning Guide — Cilium 1.13.4 documentation](https://docs.cilium.io/en/stable/operations/performance/tuning/#id1)
*   [CNI Performance Benchmark — Cilium 1.13.4 documentation](https://docs.cilium.io/en/stable/operations/performance/benchmark/#tcp-throughput-tcp-stream)
*   [Cilium 1.9: Maglev, Deny Policies, VM Support, OpenShift, Hubble mTLS, Bandwidth Manager, eBPF Node-Local Redirect, Datapath Optimizations, and more](https://cilium.io/blog/2020/11/10/cilium-19/#veth)

> _三人行, 必有我师; 知识共享, 天下为公._ 本文由东风微鸣技术博客 [EWhisper.cn](https://EWhisper.cn) 编写.