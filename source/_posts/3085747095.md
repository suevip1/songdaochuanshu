---
layout: post
title: "深度学习（五）——DatadLoader的使用"
date: "2023-07-15T01:24:20.772Z"
---
深度学习（五）——DatadLoader的使用
=======================

![深度学习（五）——DatadLoader的使用](https://img2023.cnblogs.com/blog/2744125/202307/2744125-20230714213343372-286693335.png) 我们在打扑克，一摞的扑克牌就相当于dataset，拿牌的手相当于神经网络。而dataloader相当于抽牌的过程，它可以控制我们抽几张牌，用几只手抽牌。

**摘要：**我们在打扑克，一摞的扑克牌就相当于dataset，拿牌的手相当于神经网络。而dataloader相当于抽牌的过程，它可以控制我们抽几张牌，用几只手抽牌。

一、DataLoader简介
==============

> 官网地址:
> 
> [torch.utils.data — PyTorch 2.0 documentation](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)

1\. DataLoder类
--------------

    class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=None, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None, generator=None, *, prefetch_factor=None, persistent_workers=False, pin_memory_device='')
    

由此可见，DataLoder必须需要输入的参数只有\\(dataset\\)。

2\. 参数说明
--------

*   **dataset**(Dataset): 数据集的储存的路径位置等信息
    
*   **batch\_size**(int): 每次取数据的数量，比如batchi\_size=2，那么每次取2条数据
    
*   **shuffle**(bool): True: 打乱数据(可以理解为打牌中洗牌的过程); False: 不打乱。默认为False
    
*   **num\_workers**(int): 加载数据的进程，多进程会更快。默认为0，即用主进程进行加载。但在windows系统下，num\_workers如果非0，可能会出现 _BrokenPipeError\[Error 32\]_ 错误
    
*   **drop\_last**(bool): 比如我们从100条数据中每次取3条，到最后会余下1条，如果drop\_last=True，那么这条数据会被舍弃（即只要前面99条数据）；如果为False，则保留这条数据
    

二、DataLoader实操
==============

*   数据集仍然采用上一篇的_CIFAR10_数据集

1\. DataLoader取数据的逻辑
--------------------

*   首先import dataset，dataset会返回一个数据的img和target
    
*   然后import dataloder，并设置\\(batch\\\_size\\)，比如\\(batch\\\_size=4\\)，那么dataloder会获取这些数据：dataset\[0\]=img0, target0; dataset\[1\]=img1, target1; dataset\[2\]=img2, target2; dataset\[3\]=img3, target3. 并分别将其中的4个img和4个target进行打包，并返回打包好的imgs和targets
    

比如下面这串代码：

    import torchvision
    from torch.utils.data import DataLoader
    
    #测试集，并将PIL数据转化为tensor类型
    test_data=torchvision.datasets.CIFAR10("./dataset",train=False,transform=torchvision.transforms.ToTensor())
    
    #batch_size=4:每次从test_data中取4个数据集并打包
    test_loader=DataLoader(dataset=test_data, batch_size=4, shuffle=True, num_workers=0, drop_last=False)
    

这里的test\_loader会取出test\_data\[0\]、test\_data\[1\]、test\_data\[2\]、test\_data\[3\]的img和target，并分别打包。返回两个参数：打包好的imgs，打包好的taregts

2\. 如何取出DataLoader中打包好的img、target数据
-----------------------------------

### （1）输出打包好的img、target

代码示例如下：

    import torchvision
    from torch.utils.data import DataLoader
    
    #测试集，并将PIL数据转化为tensor类型
    test_data=torchvision.datasets.CIFAR10("./dataset",train=False,transform=torchvision.transforms.ToTensor())
    
    #batch_size=4:每次从test_data中取4个数据集并打包
    test_loader=DataLoader(dataset=test_data, batch_size=4, shuffle=True, num_workers=0, drop_last=False)
    
    #测试数据集中第一章图片及target
    img, target=test_data[0]  
    print(img.shape)
    print(target)
    
    #取出test_loader中的图片
    for data in test_loader:
        imgs,targets = data
        print(imgs.shape)    #[Run] torch.Size([4, 3, 32, 32])  4张图片打包，3通道，32×32
        print(targets)       #[Run] tensor([3, 5, 2, 7]) 4张图，每张图片对应的标签分别是3，5，2，7（某一次print的举例，每次print结果不太一样）
    

在11行处debug一下可以发现，test\_loader中有个叫sampler的采样器，采取的是随机采样的方式，也就是说这batch\_size=4时，每次抓取的4张图片都是随机抓取的。

### （2）展示图片

用tensorboard就可以可视化了，具体操作改一下上面代码最后的for循环就好了

    from torch.utils.tensorboard import SummaryWriter
    writer=SummaryWriter("dataloder")
    
    step=0  #tensorboard步长参数
    for data in test_loader:
        imgs,targets = data
        # print(imgs.shape)    #[Run] torch.Size([4, 3, 32, 32])  4张图片打包，3通道，32×32
        # print(targets)       #[Run] tensor([3, 5, 2, 7]) 4张图，每张图片对应的标签分别是3，5，2，7（某一次print的举例，每次print结果不太一样）
        writer.add_images("test_data",imgs,step)  #注意这里是add_images,不是add_image。因为这里是加入了64张图
        step=step+1
    writer.close()
    

### （3）关于shuffle的理解

*   可以理解为一个for循环就是打一次牌，打完一轮牌后，若shuffle=False，那么下一轮每一步抓到的牌都会跟上一轮相同；如果shuffle=True，那么就会进行洗牌，打乱牌的顺序后，下一轮每一步跟上一轮的会有不同。

首先将shuffle设置为False：

    test_loader=DataLoader(dataset=test_data, batch_size=64, shuffle=True, num_workers=0, drop_last=False)
    

然后对（2）的代码进行修改，运行代码：

    for epoch in range(2):  #假设打两次牌，我们来观察两次牌中间的洗牌情况
        step = 0  # tensorboard步长参数
        for data in test_loader:
            imgs,targets = data
            # print(imgs.shape)    #[Run] torch.Size([4, 3, 32, 32])  4张图片打包，3通道，32×32
            # print(targets)       #[Run] tensor([3, 5, 2, 7]) 4张图，每张图片对应的标签分别是3，5，2，7（某一次print的举例，每次print结果不太一样）
            writer.add_images("Epoch: {}".format(epoch),imgs,step)  #注意这里是add_images,不是add_image。因为这里是加入了64张图
            step=step+1
    writer.close()
    
    

结果显示，未洗牌时运行的结果是一样的：  
![](https://img2023.cnblogs.com/blog/2744125/202307/2744125-20230714212513430-414399851.png)

*   将shuffle设置为True，再次运行，可以发现两次结果还是不一样的：  
    ![](https://img2023.cnblogs.com/blog/2744125/202307/2744125-20230714212523139-146710443.png)